@InProceedings{10.5555/2999134.2999257,
  author           = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  booktitle        = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1},
  title            = {ImageNet classification with deep convolutional neural networks},
  year             = {2012},
  address          = {Red Hook, NY, USA},
  pages            = {1097–1105},
  publisher        = {Curran Associates Inc.},
  series           = {NIPS'12},
  abstract         = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overriding in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  creationdate     = {2024-11-12T12:58:08},
  location         = {Lake Tahoe, Nevada},
  modificationdate = {2024-11-12T12:58:08},
  numpages         = {9},
}

@InProceedings{10656117,
  author           = {X. Weng and B. Ivanovic and Y. Wang and Y. Wang and M. Pavone},
  booktitle        = {2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title            = {PARA-Drive: Parallelized Architecture for Real-Time Autonomous Driving},
  year             = {2024},
  address          = {Los Alamitos, CA, USA},
  month            = {jun},
  pages            = {15449-15458},
  publisher        = {IEEE Computer Society},
  abstract         = {Recent works have proposed end-to-end autonomous vehicle (AV) architectures comprised of differentiable modules, achieving state-of-the-art driving performance. While they provide advantages over the traditional perception-prediction-planning pipeline (e.g., removing information bottlenecks between components and alleviating integration challenges), they do so using a diverse combination of tasks, modules, and their interconnectivity. As of yet, however, there has been no systematic analysis of the necessity of these modules or the impact of their connectivity, placement, and internal representations on overall driving performance. Addressing this gap, our work conducts a comprehensive exploration of the design space of end-to-end modular AV stacks. Our findings culminate in the development of PARA-Drivel: a fully parallel end-to-end AV architecture. PARA-Drive not only achieves state-of-the-art performance in perception, prediction, and planning, but also significantly enhances runtime speed by nearly 3 x, without compromising on interpretability or safety.},
  doi              = {10.1109/CVPR52733.2024.01463},
  file             = {:10656117 - PARA Drive_ Parallelized Architecture for Real Time Autonomous Driving.pdf:PDF},
  keywords         = {computer vision;systematics;runtime;pipelines;computer architecture;real-time systems;safety},
  modificationdate = {2024-10-15T09:17:56},
}

@Article{6795724,
  author           = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
  journal          = {Neural Computation},
  title            = {Backpropagation Applied to Handwritten Zip Code Recognition},
  year             = {1989},
  number           = {4},
  pages            = {541-551},
  volume           = {1},
  creationdate     = {2024-11-12T12:56:28},
  doi              = {10.1162/neco.1989.1.4.541},
  modificationdate = {2024-11-12T12:56:28},
}

@Article{article,
  author           = {Sarker, Iqbal},
  journal          = {SN Computer Science},
  title            = {Deep Learning: A Comprehensive Overview on Techniques, Taxonomy, Applications and Research Directions},
  year             = {2021},
  month            = {08},
  volume           = {2},
  creationdate     = {2024-09-16T09:32:30},
  doi              = {10.1007/s42979-021-00815-1},
  file             = {:article - Deep Learning_ a Comprehensive Overview on Techniques, Taxonomy, Applications and Research Directions.pdf:PDF},
  modificationdate = {2024-10-15T09:03:24},
  readstatus       = {read},
}

@Article{Carion2020,
  author           = {Nicolas Carion and Francisco Massa and Gabriel Synnaeve and Nicolas Usunier and Alexander Kirillov and Sergey Zagoruyko},
  title            = {End-to-End Object Detection with Transformers},
  year             = {2020},
  month            = may,
  abstract         = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
  archiveprefix    = {arXiv},
  comment          = {DETR},
  eprint           = {2005.12872},
  file             = {:Carion2020 - End to End Object Detection with Transformers.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-01-24T15:03:22},
  primaryclass     = {cs.CV},
  priority         = {prio1},
  readstatus       = {read},
}

@Article{Chen2022,
  author           = {Xuanyao Chen and Tianyuan Zhang and Yue Wang and Yilun Wang and Hang Zhao},
  journal          = {CVPR 2023 workshop on autonomous driving},
  title            = {FUTR3D: A Unified Sensor Fusion Framework for 3D Detection},
  year             = {2022},
  month            = mar,
  abstract         = {Sensor fusion is an essential topic in many perception systems, such as autonomous driving and robotics. Existing multi-modal 3D detection models usually involve customized designs depending on the sensor combinations or setups. In this work, we propose the first unified end-to-end sensor fusion framework for 3D detection, named FUTR3D, which can be used in (almost) any sensor configuration. FUTR3D employs a query-based Modality-Agnostic Feature Sampler (MAFS), together with a transformer decoder with a set-to-set loss for 3D detection, thus avoiding using late fusion heuristics and post-processing tricks. We validate the effectiveness of our framework on various combinations of cameras, low-resolution LiDARs, high-resolution LiDARs, and Radars. On NuScenes dataset, FUTR3D achieves better performance over specifically designed methods across different sensor combinations. Moreover, FUTR3D achieves great flexibility with different sensor configurations and enables low-cost autonomous driving. For example, only using a 4-beam LiDAR with cameras, FUTR3D (58.0 mAP) achieves on par performance with state-of-the-art 3D detection model CenterPoint (56.6 mAP) using a 32-beam LiDAR.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:05:17},
  eprint           = {2203.10642},
  file             = {:Chen2022 - FUTR3D_ a Unified Sensor Fusion Framework for 3D Detection.pdf:PDF},
  groups           = {Reading Group Sparse Query},
  keywords         = {cs.CV},
  modificationdate = {2024-10-16T13:27:07},
  primaryclass     = {cs.CV},
}

@Article{Chen2022a,
  author           = {Shaoyu Chen and Tianheng Cheng and Xinggang Wang and Wenming Meng and Qian Zhang and Wenyu Liu},
  title            = {Efficient and Robust 2D-to-BEV Representation Learning via Geometry-guided Kernel Transformer},
  year             = {2022},
  month            = jun,
  abstract         = {Learning Bird's Eye View (BEV) representation from surrounding-view cameras is of great importance for autonomous driving. In this work, we propose a Geometry-guided Kernel Transformer (GKT), a novel 2D-to-BEV representation learning mechanism. GKT leverages the geometric priors to guide the transformer to focus on discriminative regions and unfolds kernel features to generate BEV representation. For fast inference, we further introduce a look-up table (LUT) indexing method to get rid of the camera's calibrated parameters at runtime. GKT can run at $72.3$ FPS on 3090 GPU / $45.6$ FPS on 2080ti GPU and is robust to the camera deviation and the predefined BEV height. And GKT achieves the state-of-the-art real-time segmentation results, i.e., 38.0 mIoU (100m$\times$100m perception range at a 0.5m resolution) on the nuScenes val set. Given the efficiency, effectiveness, and robustness, GKT has great practical values in autopilot scenarios, especially for real-time running systems. Code and models will be available at \url{https://github.com/hustvl/GKT}.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:07:17},
  eprint           = {2206.04584},
  file             = {:Chen2022a - Efficient and Robust 2D to BEV Representation Learning Via Geometry Guided Kernel Transformer.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2024-10-15T09:07:18},
  primaryclass     = {cs.CV},
}

@Article{Chen2023,
  author           = {Long Chen and Oleg Sinavski and Jan Hünermann and Alice Karnsund and Andrew James Willmott and Danny Birch and Daniel Maund and Jamie Shotton},
  title            = {Driving with LLMs: Fusing Object-Level Vector Modality for Explainable Autonomous Driving},
  year             = {2023},
  month            = oct,
  abstract         = {Large Language Models (LLMs) have shown promise in the autonomous driving sector, particularly in generalization and interpretability. We introduce a unique object-level multimodal LLM architecture that merges vectorized numeric modalities with a pre-trained LLM to improve context understanding in driving situations. We also present a new dataset of 160k QA pairs derived from 10k driving scenarios, paired with high quality control commands collected with RL agent and question answer pairs generated by teacher LLM (GPT-3.5). A distinct pretraining strategy is devised to align numeric vector modalities with static LLM representations using vector captioning language data. We also introduce an evaluation metric for Driving QA and demonstrate our LLM-driver's proficiency in interpreting driving scenarios, answering questions, and decision-making. Our findings highlight the potential of LLM-based driving action generation in comparison to traditional behavioral cloning. We make our benchmark, datasets, and model available for further exploration.},
  archiveprefix    = {arXiv},
  eprint           = {2310.01957},
  file             = {:Chen2023 - Driving with LLMs_ Fusing Object Level Vector Modality for Explainable Autonomous Driving.pdf:PDF},
  keywords         = {cs.RO, cs.AI, cs.CL, cs.CV},
  modificationdate = {2024-10-15T09:03:34},
  primaryclass     = {cs.RO},
}

@Article{Chen2023a,
  author           = {Li Chen and Penghao Wu and Kashyap Chitta and Bernhard Jaeger and Andreas Geiger and Hongyang Li},
  title            = {End-to-end Autonomous Driving: Challenges and Frontiers},
  year             = {2023},
  month            = jun,
  abstract         = {The autonomous driving community has witnessed a rapid growth in approaches that embrace an end-to-end algorithm framework, utilizing raw sensor input to generate vehicle motion plans, instead of concentrating on individual tasks such as detection and motion prediction. End-to-end systems, in comparison to modular pipelines, benefit from joint feature optimization for perception and planning. This field has flourished due to the availability of large-scale datasets, closed-loop evaluation, and the increasing need for autonomous driving algorithms to perform effectively in challenging scenarios. In this survey, we provide a comprehensive analysis of more than 270 papers, covering the motivation, roadmap, methodology, challenges, and future trends in end-to-end autonomous driving. We delve into several critical challenges, including multi-modality, interpretability, causal confusion, robustness, and world models, amongst others. Additionally, we discuss current advancements in foundation models and visual pre-training, as well as how to incorporate these techniques within the end-to-end driving framework. we maintain an active repository that contains up-to-date literature and open-source projects at https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:10:40},
  eprint           = {2306.16927},
  file             = {:Chen2023a - End to End Autonomous Driving_ Challenges and Frontiers.pdf:PDF},
  keywords         = {cs.RO, cs.AI, cs.CV, cs.LG},
  modificationdate = {2024-10-15T09:10:41},
  primaryclass     = {cs.RO},
}

@Article{Chen2024,
  author           = {Shaoyu Chen and Bo Jiang and Hao Gao and Bencheng Liao and Qing Xu and Qian Zhang and Chang Huang and Wenyu Liu and Xinggang Wang},
  title            = {VADv2: End-to-End Vectorized Autonomous Driving via Probabilistic Planning},
  year             = {2024},
  month            = feb,
  abstract         = {Learning a human-like driving policy from large-scale driving demonstrations is promising, but the uncertainty and non-deterministic nature of planning make it challenging. In this work, to cope with the uncertainty problem, we propose VADv2, an end-to-end driving model based on probabilistic planning. VADv2 takes multi-view image sequences as input in a streaming manner, transforms sensor data into environmental token embeddings, outputs the probabilistic distribution of action, and samples one action to control the vehicle. Only with camera sensors, VADv2 achieves state-of-the-art closed-loop performance on the CARLA Town05 benchmark, significantly outperforming all existing methods. It runs stably in a fully end-to-end manner, even without the rule-based wrapper. Closed-loop demos are presented at https://hgao-cv.github.io/VADv2.},
  archiveprefix    = {arXiv},
  eprint           = {2402.13243},
  file             = {:Chen2024 - VADv2_ End to End Vectorized Autonomous Driving Via Probabilistic Planning.pdf:PDF},
  groups           = {Reading Group Sparse Query},
  keywords         = {cs.CV, cs.RO},
  modificationdate = {2024-10-16T13:27:48},
  primaryclass     = {cs.CV},
}

@Article{Chu2024,
  author           = {Xiaomeng Chu and Jiajun Deng and Guoliang You and Yifan Duan and Yao Li and Yanyong Zhang},
  title            = {RayFormer: Improving Query-Based Multi-Camera 3D Object Detection via Ray-Centric Strategies},
  year             = {2024},
  month            = jul,
  abstract         = {The recent advances in query-based multi-camera 3D object detection are featured by initializing object queries in the 3D space, and then sampling features from perspective-view images to perform multi-round query refinement. In such a framework, query points near the same camera ray are likely to sample similar features from very close pixels, resulting in ambiguous query features and degraded detection accuracy. To this end, we introduce RayFormer, a camera-ray-inspired query-based 3D object detector that aligns the initialization and feature extraction of object queries with the optical characteristics of cameras. Specifically, RayFormer transforms perspective-view image features into bird's eye view (BEV) via the lift-splat-shoot method and segments the BEV map to sectors based on the camera rays. Object queries are uniformly and sparsely initialized along each camera ray, facilitating the projection of different queries onto different areas in the image to extract distinct features. Besides, we leverage the instance information of images to supplement the uniformly initialized object queries by further involving additional queries along the ray from 2D object detection boxes. To extract unique object-level features that cater to distinct queries, we design a ray sampling method that suitably organizes the distribution of feature sampling points on both images and bird's eye view. Extensive experiments are conducted on the nuScenes dataset to validate our proposed ray-inspired model design. The proposed RayFormer achieves 55.5% mAP and 63.3% NDS, respectively. Our codes will be made available.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:16:21},
  eprint           = {2407.14923},
  file             = {:Chu2024 - RayFormer_ Improving Query Based Multi Camera 3D Object Detection Via Ray Centric Strategies.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2024-10-15T09:16:24},
  primaryclass     = {cs.CV},
}

@Article{Doll2024,
  author           = {Simon Doll and Niklas Hanselmann and Lukas Schneider and Richard Schulz and Marius Cordts and Markus Enzweiler and Hendrik P. A. Lensch},
  title            = {DualAD: Disentangling the Dynamic and Static World for End-to-End Driving},
  year             = {2024},
  month            = jun,
  abstract         = {State-of-the-art approaches for autonomous driving integrate multiple sub-tasks of the overall driving task into a single pipeline that can be trained in an end-to-end fashion by passing latent representations between the different modules. In contrast to previous approaches that rely on a unified grid to represent the belief state of the scene, we propose dedicated representations to disentangle dynamic agents and static scene elements. This allows us to explicitly compensate for the effect of both ego and object motion between consecutive time steps and to flexibly propagate the belief state through time. Furthermore, dynamic objects can not only attend to the input camera images, but also directly benefit from the inferred static scene structure via a novel dynamic-static cross-attention. Extensive experiments on the challenging nuScenes benchmark demonstrate the benefits of the proposed dual-stream design, especially for modelling highly dynamic agents in the scene, and highlight the improved temporal consistency of our approach. Our method titled DualAD not only outperforms independently trained single-task networks, but also improves over previous state-of-the-art end-to-end models by a large margin on all tasks along the functional chain of driving.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:14:22},
  eprint           = {2406.06264},
  file             = {:Doll2024 - DualAD_ Disentangling the Dynamic and Static World for End to End Driving.pdf:PDF},
  groups           = {Reading Group Sparse Query},
  keywords         = {cs.CV},
  modificationdate = {2024-10-16T13:27:57},
  primaryclass     = {cs.CV},
}

@Article{Dosovitskiy2020,
  author           = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
  title            = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  year             = {2020},
  month            = oct,
  abstract         = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-09-30T15:50:31},
  eprint           = {2010.11929},
  file             = {:Dosovitskiy2020 - An Image Is Worth 16x16 Words_ Transformers for Image Recognition at Scale.pdf:PDF},
  keywords         = {cs.CV, cs.AI, cs.LG},
  modificationdate = {2025-01-24T15:03:24},
  primaryclass     = {cs.CV},
  priority         = {prio1},
  readstatus       = {read},
}

@Article{Drews2022,
  author           = {Florian Drews and Di Feng and Florian Faion and Lars Rosenbaum and Michael Ulrich and Claudius Gläser},
  title            = {DeepFusion: A Robust and Modular 3D Object Detector for Lidars, Cameras and Radars},
  year             = {2022},
  month            = sep,
  abstract         = {We propose DeepFusion, a modular multi-modal architecture to fuse lidars, cameras and radars in different combinations for 3D object detection. Specialized feature extractors take advantage of each modality and can be exchanged easily, making the approach simple and flexible. Extracted features are transformed into bird's-eye-view as a common representation for fusion. Spatial and semantic alignment is performed prior to fusing modalities in the feature space. Finally, a detection head exploits rich multi-modal features for improved 3D detection performance. Experimental results for lidar-camera, lidar-camera-radar and camera-radar fusion show the flexibility and effectiveness of our fusion approach. In the process, we study the largely unexplored task of faraway car detection up to 225 meters, showing the benefits of our lidar-camera fusion. Furthermore, we investigate the required density of lidar points for 3D object detection and illustrate implications at the example of robustness against adverse weather conditions. Moreover, ablation studies on our camera-radar fusion highlight the importance of accurate depth estimation.},
  archiveprefix    = {arXiv},
  comment          = {Bosch paper},
  creationdate     = {2024-10-22T09:57:45},
  eprint           = {2209.12729},
  file             = {:Drews2022 - DeepFusion_ a Robust and Modular 3D Object Detector for Lidars, Cameras and Radars.pdf:PDF},
  keywords         = {cs.CV, cs.AI, cs.LG, cs.RO},
  modificationdate = {2024-10-22T10:00:48},
  primaryclass     = {cs.CV},
  priority         = {prio1},
}

@Article{Fujimoto2025,
  author           = {Scott Fujimoto and Pierluca D'Oro and Amy Zhang and Yuandong Tian and Michael Rabbat},
  title            = {Towards General-Purpose Model-Free Reinforcement Learning},
  year             = {2025},
  month            = jan,
  abstract         = {Reinforcement learning (RL) promises a framework for near-universal problem-solving. In practice however, RL algorithms are often tailored to specific benchmarks, relying on carefully tuned hyperparameters and algorithmic choices. Recently, powerful model-based RL methods have shown impressive general results across benchmarks but come at the cost of increased complexity and slow run times, limiting their broader applicability. In this paper, we attempt to find a unifying model-free deep RL algorithm that can address a diverse class of domains and problem settings. To achieve this, we leverage model-based representations that approximately linearize the value function, taking advantage of the denser task objectives used by model-based RL while avoiding the costs associated with planning or simulated trajectories. We evaluate our algorithm, MR.Q, on a variety of common RL benchmarks with a single set of hyperparameters and show a competitive performance against domain-specific and general baselines, providing a concrete step towards building general-purpose model-free deep RL algorithms.},
  archiveprefix    = {arXiv},
  creationdate     = {2025-01-28T13:55:56},
  eprint           = {2501.16142},
  file             = {:http\://arxiv.org/pdf/2501.16142v1:PDF},
  keywords         = {cs.LG, cs.AI},
  modificationdate = {2025-01-28T13:55:56},
  primaryclass     = {cs.LG},
}

@Article{Harley2022,
  author           = {Adam W. Harley and Zhaoyuan Fang and Jie Li and Rares Ambrus and Katerina Fragkiadaki},
  title            = {Simple-BEV: What Really Matters for Multi-Sensor BEV Perception?},
  year             = {2022},
  month            = jun,
  abstract         = {Building 3D perception systems for autonomous vehicles that do not rely on high-density LiDAR is a critical research problem because of the expense of LiDAR systems compared to cameras and other sensors. Recent research has developed a variety of camera-only methods, where features are differentiably "lifted" from the multi-camera images onto the 2D ground plane, yielding a "bird's eye view" (BEV) feature representation of the 3D space around the vehicle. This line of work has produced a variety of novel "lifting" methods, but we observe that other details in the training setups have shifted at the same time, making it unclear what really matters in top-performing methods. We also observe that using cameras alone is not a real-world constraint, considering that additional sensors like radar have been integrated into real vehicles for years already. In this paper, we first of all attempt to elucidate the high-impact factors in the design and training protocol of BEV perception models. We find that batch size and input resolution greatly affect performance, while lifting strategies have a more modest effect -- even a simple parameter-free lifter works well. Second, we demonstrate that radar data can provide a substantial boost to performance, helping to close the gap between camera-only and LiDAR-enabled systems. We analyze the radar usage details that lead to good performance, and invite the community to re-consider this commonly-neglected part of the sensor platform.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-09-25T13:17:31},
  eprint           = {2206.07959},
  file             = {:Harley2022 - Simple BEV_ What Really Matters for Multi Sensor BEV Perception_.pdf:PDF},
  groups           = {Reading Group Sparse Query},
  keywords         = {cs.CV},
  modificationdate = {2024-10-16T13:26:35},
  primaryclass     = {cs.CV},
}

@Article{Hatamizadeh2023,
  author           = {Ali Hatamizadeh and Greg Heinrich and Hongxu Yin and Andrew Tao and Jose M. Alvarez and Jan Kautz and Pavlo Molchanov},
  title            = {FasterViT: Fast Vision Transformers with Hierarchical Attention},
  year             = {2023},
  month            = jun,
  abstract         = {We design a new family of hybrid CNN-ViT neural networks, named FasterViT, with a focus on high image throughput for computer vision (CV) applications. FasterViT combines the benefits of fast local representation learning in CNNs and global modeling properties in ViT. Our newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs. We benefit from efficient window-based self-attention. Each window has access to dedicated carrier tokens that participate in local and global representation learning. At a high level, global self-attentions enable the efficient cross-window communication at lower costs. FasterViT achieves a SOTA Pareto-front in terms of accuracy and image throughput. We have extensively validated its effectiveness on various CV tasks including classification, object detection and segmentation. We also show that HAT can be used as a plug-and-play module for existing networks and enhance them. We further demonstrate significantly faster and more accurate performance than competitive counterparts for images with high resolution. Code is available at https://github.com/NVlabs/FasterViT.},
  archiveprefix    = {arXiv},
  comment          = {hierarchical attention},
  creationdate     = {2024-10-22T09:06:17},
  eprint           = {2306.06189},
  file             = {:- FasterViT_ Fast Vision Transformers with Hierarchical Attention.pdf:PDF},
  keywords         = {cs.CV, cs.AI, cs.LG},
  modificationdate = {2024-10-22T09:06:30},
  primaryclass     = {cs.CV},
}

@Article{He2015,
  author           = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  title            = {Deep Residual Learning for Image Recognition},
  year             = {2015},
  month            = dec,
  abstract         = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-11-12T12:58:52},
  eprint           = {1512.03385},
  file             = {:He2015 - Deep Residual Learning for Image Recognition.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2024-11-12T12:58:53},
  primaryclass     = {cs.CV},
}

@Article{He2021,
  author           = {Lu He and Qianyu Zhou and Xiangtai Li and Li Niu and Guangliang Cheng and Xiao Li and Wenxuan Liu and Yunhai Tong and Lizhuang Ma and Liqing Zhang},
  title            = {End-to-End Video Object Detection with Spatial-Temporal Transformers},
  year             = {2021},
  month            = may,
  abstract         = {Recently, DETR and Deformable DETR have been proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance as previous complex hand-crafted detectors. However, their performance on Video Object Detection (VOD) has not been well explored. In this paper, we present TransVOD, an end-to-end video object detection model based on a spatial-temporal Transformer architecture. The goal of this paper is to streamline the pipeline of VOD, effectively removing the need for many hand-crafted components for feature aggregation, e.g., optical flow, recurrent neural networks, relation networks. Besides, benefited from the object query design in DETR, our method does not need complicated post-processing methods such as Seq-NMS or Tubelet rescoring, which keeps the pipeline simple and clean. In particular, we present temporal Transformer to aggregate both the spatial object queries and the feature memories of each frame. Our temporal Transformer consists of three components: Temporal Deformable Transformer Encoder (TDTE) to encode the multiple frame spatial details, Temporal Query Encoder (TQE) to fuse object queries, and Temporal Deformable Transformer Decoder to obtain current frame detection results. These designs boost the strong baseline deformable DETR by a significant margin (3%-4% mAP) on the ImageNet VID dataset. TransVOD yields comparable results performance on the benchmark of ImageNet VID. We hope our TransVOD can provide a new perspective for video object detection. Code will be made publicly available at https://github.com/SJTU-LuHe/TransVOD.},
  archiveprefix    = {arXiv},
  comment          = {DETR},
  eprint           = {2105.10920},
  file             = {:He2021 - End to End Video Object Detection with Spatial Temporal Transformers.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2024-10-21T15:55:12},
  primaryclass     = {cs.CV},
  priority         = {prio1},
}

@Article{Hu2022,
  author           = {Yihan Hu and Jiazhi Yang and Li Chen and Keyu Li and Chonghao Sima and Xizhou Zhu and Siqi Chai and Senyao Du and Tianwei Lin and Wenhai Wang and Lewei Lu and Xiaosong Jia and Qiang Liu and Jifeng Dai and Yu Qiao and Hongyang Li},
  title            = {Planning-oriented Autonomous Driving},
  year             = {2022},
  month            = dec,
  abstract         = {Modern autonomous driving system is characterized as modular tasks in sequential order, i.e., perception, prediction, and planning. In order to perform a wide diversity of tasks and achieve advanced-level intelligence, contemporary approaches either deploy standalone models for individual tasks, or design a multi-task paradigm with separate heads. However, they might suffer from accumulative errors or deficient task coordination. Instead, we argue that a favorable framework should be devised and optimized in pursuit of the ultimate goal, i.e., planning of the self-driving car. Oriented at this, we revisit the key components within perception and prediction, and prioritize the tasks such that all these tasks contribute to planning. We introduce Unified Autonomous Driving (UniAD), a comprehensive framework up-to-date that incorporates full-stack driving tasks in one network. It is exquisitely devised to leverage advantages of each module, and provide complementary feature abstractions for agent interaction from a global perspective. Tasks are communicated with unified query interfaces to facilitate each other toward planning. We instantiate UniAD on the challenging nuScenes benchmark. With extensive ablations, the effectiveness of using such a philosophy is proven by substantially outperforming previous state-of-the-arts in all aspects. Code and models are public.},
  archiveprefix    = {arXiv},
  comment          = {end2end MB, uniAD},
  eprint           = {2212.10156},
  file             = {:Hu2022 - Planning Oriented Autonomous Driving.pdf:PDF},
  groups           = {Reading Group Sparse Query},
  keywords         = {cs.CV, cs.RO},
  modificationdate = {2024-10-16T13:27:38},
  primaryclass     = {cs.CV},
}

@Article{Huang2021,
  author           = {Junjie Huang and Guan Huang and Zheng Zhu and Yun Ye and Dalong Du},
  title            = {BEVDet: High-performance Multi-camera 3D Object Detection in Bird-Eye-View},
  year             = {2021},
  month            = dec,
  abstract         = {Autonomous driving perceives its surroundings for decision making, which is one of the most complex scenarios in visual perception. The success of paradigm innovation in solving the 2D object detection task inspires us to seek an elegant, feasible, and scalable paradigm for fundamentally pushing the performance boundary in this area. To this end, we contribute the BEVDet paradigm in this paper. BEVDet performs 3D object detection in Bird-Eye-View (BEV), where most target values are defined and route planning can be handily performed. We merely reuse existing modules to build its framework but substantially develop its performance by constructing an exclusive data augmentation strategy and upgrading the Non-Maximum Suppression strategy. In the experiment, BEVDet offers an excellent trade-off between accuracy and time-efficiency. As a fast version, BEVDet-Tiny scores 31.2% mAP and 39.2% NDS on the nuScenes val set. It is comparable with FCOS3D, but requires just 11% computational budget of 215.3 GFLOPs and runs 9.2 times faster at 15.6 FPS. Another high-precision version dubbed BEVDet-Base scores 39.3% mAP and 47.2% NDS, significantly exceeding all published results. With a comparable inference speed, it surpasses FCOS3D by a large margin of +9.8% mAP and +10.0% NDS. The source code is publicly available for further research at https://github.com/HuangJunJie2017/BEVDet .},
  archiveprefix    = {arXiv},
  creationdate     = {2024-09-30T15:53:29},
  eprint           = {2112.11790},
  file             = {:Huang2021 - BEVDet_ High Performance Multi Camera 3D Object Detection in Bird Eye View.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-01-24T15:03:27},
  primaryclass     = {cs.CV},
}

@Article{Huang2022,
  author           = {Junjie Huang and Guan Huang},
  title            = {BEVDet4D: Exploit Temporal Cues in Multi-camera 3D Object Detection},
  year             = {2022},
  month            = mar,
  abstract         = {Single frame data contains finite information which limits the performance of the existing vision-based multi-camera 3D object detection paradigms. For fundamentally pushing the performance boundary in this area, a novel paradigm dubbed BEVDet4D is proposed to lift the scalable BEVDet paradigm from the spatial-only 3D space to the spatial-temporal 4D space. We upgrade the naive BEVDet framework with a few modifications just for fusing the feature from the previous frame with the corresponding one in the current frame. In this way, with negligible additional computing budget, we enable BEVDet4D to access the temporal cues by querying and comparing the two candidate features. Beyond this, we simplify the task of velocity prediction by removing the factors of ego-motion and time in the learning target. As a result, BEVDet4D with robust generalization performance reduces the velocity error by up to -62.9%. This makes the vision-based methods, for the first time, become comparable with those relied on LiDAR or radar in this aspect. On challenge benchmark nuScenes, we report a new record of 54.5% NDS with the high-performance configuration dubbed BEVDet4D-Base, which surpasses the previous leading method BEVDet-Base by +7.3% NDS. The source code is publicly available for further research at https://github.com/HuangJunJie2017/BEVDet .},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:05:45},
  eprint           = {2203.17054},
  file             = {:Huang2022 - BEVDet4D_ Exploit Temporal Cues in Multi Camera 3D Object Detection.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2024-10-15T09:05:47},
  primaryclass     = {cs.CV},
}

@Article{Huang2024,
  author           = {Zhe Huang and Yizhe Zhao and Hao Xiao and Chenyan Wu and Lingting Ge},
  title            = {DuoSpaceNet: Leveraging Both Bird's-Eye-View and Perspective View Representations for 3D Object Detection},
  year             = {2024},
  month            = may,
  abstract         = {Recent advances in multi-view camera-only 3D object detection either rely on an accurate reconstruction of bird's-eye-view (BEV) 3D features or on traditional 2D perspective view (PV) image features. While both have their own pros and cons, few have found a way to stitch them together in order to benefit from "the best of both worlds". To this end, we explore a duo space (i.e., BEV and PV) 3D perception framework, in conjunction with some useful duo space fusion strategies that allow effective aggregation of the two feature representations. To the best of our knowledge, our proposed method, DuoSpaceNet, is the first to leverage two distinct feature spaces and achieves the state-of-the-art 3D object detection and BEV map segmentation results on nuScenes dataset.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:13:59},
  eprint           = {2405.10577},
  file             = {:Huang2024 - DuoSpaceNet_ Leveraging Both Bird's Eye View and Perspective View Representations for 3D Object Detection.pdf:PDF},
  keywords         = {cs.CV, cs.RO},
  modificationdate = {2024-10-15T09:14:05},
  primaryclass     = {cs.CV},
}

@Article{Ioffe2015,
  author           = {Sergey Ioffe and Christian Szegedy},
  title            = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  year             = {2015},
  month            = feb,
  abstract         = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.},
  archiveprefix    = {arXiv},
  creationdate     = {2025-01-24T12:50:34},
  eprint           = {1502.03167},
  file             = {:- Batch Normalization_ Accelerating Deep Network Training by Reducing Internal Covariate Shift.pdf:PDF},
  keywords         = {cs.LG},
  modificationdate = {2025-01-24T12:50:36},
  primaryclass     = {cs.LG},
}

@Article{Jia2023,
  author           = {Xiaosong Jia and Penghao Wu and Li Chen and Jiangwei Xie and Conghui He and Junchi Yan and Hongyang Li},
  title            = {Think Twice before Driving: Towards Scalable Decoders for End-to-End Autonomous Driving},
  year             = {2023},
  month            = may,
  abstract         = {End-to-end autonomous driving has made impressive progress in recent years. Existing methods usually adopt the decoupled encoder-decoder paradigm, where the encoder extracts hidden features from raw sensor data, and the decoder outputs the ego-vehicle's future trajectories or actions. Under such a paradigm, the encoder does not have access to the intended behavior of the ego agent, leaving the burden of finding out safety-critical regions from the massive receptive field and inferring about future situations to the decoder. Even worse, the decoder is usually composed of several simple multi-layer perceptrons (MLP) or GRUs while the encoder is delicately designed (e.g., a combination of heavy ResNets or Transformer). Such an imbalanced resource-task division hampers the learning process. In this work, we aim to alleviate the aforementioned problem by two principles: (1) fully utilizing the capacity of the encoder; (2) increasing the capacity of the decoder. Concretely, we first predict a coarse-grained future position and action based on the encoder features. Then, conditioned on the position and action, the future scene is imagined to check the ramification if we drive accordingly. We also retrieve the encoder features around the predicted coordinate to obtain fine-grained information about the safety-critical region. Finally, based on the predicted future and the retrieved salient feature, we refine the coarse-grained position and action by predicting its offset from ground-truth. The above refinement module could be stacked in a cascaded fashion, which extends the capacity of the decoder with spatial-temporal prior knowledge about the conditioned future. We conduct experiments on the CARLA simulator and achieve state-of-the-art performance in closed-loop benchmarks. Extensive ablation studies demonstrate the effectiveness of each proposed module.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:10:11},
  eprint           = {2305.06242},
  file             = {:Jia2023 - Think Twice before Driving_ Towards Scalable Decoders for End to End Autonomous Driving.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2024-10-15T09:10:19},
  primaryclass     = {cs.CV},
}

@Article{Jiang2022,
  author           = {Yanqin Jiang and Li Zhang and Zhenwei Miao and Xiatian Zhu and Jin Gao and Weiming Hu and Yu-Gang Jiang},
  title            = {PolarFormer: Multi-camera 3D Object Detection with Polar Transformer},
  year             = {2022},
  month            = jun,
  abstract         = {3D object detection in autonomous driving aims to reason "what" and "where" the objects of interest present in a 3D world. Following the conventional wisdom of previous 2D object detection, existing methods often adopt the canonical Cartesian coordinate system with perpendicular axis. However, we conjugate that this does not fit the nature of the ego car's perspective, as each onboard camera perceives the world in shape of wedge intrinsic to the imaging geometry with radical (non-perpendicular) axis. Hence, in this paper we advocate the exploitation of the Polar coordinate system and propose a new Polar Transformer (PolarFormer) for more accurate 3D object detection in the bird's-eye-view (BEV) taking as input only multi-camera 2D images. Specifically, we design a cross attention based Polar detection head without restriction to the shape of input structure to deal with irregular Polar grids. For tackling the unconstrained object scale variations along Polar's distance dimension, we further introduce a multi-scalePolar representation learning strategy. As a result, our model can make best use of the Polar representation rasterized via attending to the corresponding image observation in a sequence-to-sequence fashion subject to the geometric constraints. Thorough experiments on the nuScenes dataset demonstrate that our PolarFormer outperforms significantly state-of-the-art 3D object detection alternatives.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:06:32},
  eprint           = {2206.15398},
  file             = {:Jiang2022 - PolarFormer_ Multi Camera 3D Object Detection with Polar Transformer.pdf:PDF},
  keywords         = {cs.CV, cs.AI},
  modificationdate = {2024-10-21T14:30:17},
  primaryclass     = {cs.CV},
  priority         = {prio1},
}

@Article{Jiang2023,
  author           = {Bo Jiang and Shaoyu Chen and Qing Xu and Bencheng Liao and Jiajie Chen and Helong Zhou and Qian Zhang and Wenyu Liu and Chang Huang and Xinggang Wang},
  title            = {VAD: Vectorized Scene Representation for Efficient Autonomous Driving},
  year             = {2023},
  month            = mar,
  abstract         = {Autonomous driving requires a comprehensive understanding of the surrounding environment for reliable trajectory planning. Previous works rely on dense rasterized scene representation (e.g., agent occupancy and semantic map) to perform planning, which is computationally intensive and misses the instance-level structure information. In this paper, we propose VAD, an end-to-end vectorized paradigm for autonomous driving, which models the driving scene as a fully vectorized representation. The proposed vectorized paradigm has two significant advantages. On one hand, VAD exploits the vectorized agent motion and map elements as explicit instance-level planning constraints which effectively improves planning safety. On the other hand, VAD runs much faster than previous end-to-end planning methods by getting rid of computation-intensive rasterized representation and hand-designed post-processing steps. VAD achieves state-of-the-art end-to-end planning performance on the nuScenes dataset, outperforming the previous best method by a large margin. Our base model, VAD-Base, greatly reduces the average collision rate by 29.0% and runs 2.5x faster. Besides, a lightweight variant, VAD-Tiny, greatly improves the inference speed (up to 9.3x) while achieving comparable planning performance. We believe the excellent performance and the high efficiency of VAD are critical for the real-world deployment of an autonomous driving system. Code and models are available at https://github.com/hustvl/VAD for facilitating future research.},
  archiveprefix    = {arXiv},
  comment          = {end2end MB},
  eprint           = {2303.12077},
  file             = {:Jiang2023 - VAD_ Vectorized Scene Representation for Efficient Autonomous Driving.pdf:PDF},
  groups           = {Reading Group Sparse Query},
  keywords         = {cs.RO, cs.CV},
  modificationdate = {2024-10-16T13:27:48},
  primaryclass     = {cs.RO},
}

@Article{Jiang2023a,
  author           = {Xiaohui Jiang and Shuailin Li and Yingfei Liu and Shihao Wang and Fan Jia and Tiancai Wang and Lijin Han and Xiangyu Zhang},
  title            = {Far3D: Expanding the Horizon for Surround-view 3D Object Detection},
  year             = {2023},
  month            = aug,
  abstract         = {Recently 3D object detection from surround-view images has made notable advancements with its low deployment cost. However, most works have primarily focused on close perception range while leaving long-range detection less explored. Expanding existing methods directly to cover long distances poses challenges such as heavy computation costs and unstable convergence. To address these limitations, this paper proposes a novel sparse query-based framework, dubbed Far3D. By utilizing high-quality 2D object priors, we generate 3D adaptive queries that complement the 3D global queries. To efficiently capture discriminative features across different views and scales for long-range objects, we introduce a perspective-aware aggregation module. Additionally, we propose a range-modulated 3D denoising approach to address query error propagation and mitigate convergence issues in long-range tasks. Significantly, Far3D demonstrates SoTA performance on the challenging Argoverse 2 dataset, covering a wide range of 150 meters, surpassing several LiDAR-based approaches. Meanwhile, Far3D exhibits superior performance compared to previous methods on the nuScenes dataset. The code is available at https://github.com/megvii-research/Far3D.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-09-30T15:54:22},
  eprint           = {2308.09616},
  file             = {:Jiang2023a - Far3D_ Expanding the Horizon for Surround View 3D Object Detection.pdf:PDF},
  groups           = {Reading Group Sparse Query},
  keywords         = {cs.CV},
  modificationdate = {2025-01-24T15:03:38},
  primaryclass     = {cs.CV},
  priority         = {prio2},
}

@Article{Kingma2014AdamAM,
  author           = {Diederik P. Kingma and Jimmy Ba},
  journal          = {CoRR},
  title            = {Adam: A Method for Stochastic Optimization},
  year             = {2014},
  volume           = {abs/1412.6980},
  creationdate     = {2024-11-12T13:33:04},
  modificationdate = {2024-11-12T13:33:04},
  url              = {https://api.semanticscholar.org/CorpusID:6628106},
}

@Article{Li2022,
  author           = {Zhiqi Li and Wenhai Wang and Hongyang Li and Enze Xie and Chonghao Sima and Tong Lu and Qiao Yu and Jifeng Dai},
  title            = {BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers},
  year             = {2022},
  month            = mar,
  abstract         = {3D visual perception tasks, including 3D detection and map segmentation based on multi-camera images, are essential for autonomous driving systems. In this work, we present a new framework termed BEVFormer, which learns unified BEV representations with spatiotemporal transformers to support multiple autonomous driving perception tasks. In a nutshell, BEVFormer exploits both spatial and temporal information by interacting with spatial and temporal space through predefined grid-shaped BEV queries. To aggregate spatial information, we design spatial cross-attention that each BEV query extracts the spatial features from the regions of interest across camera views. For temporal information, we propose temporal self-attention to recurrently fuse the history BEV information. Our approach achieves the new state-of-the-art 56.9\% in terms of NDS metric on the nuScenes \texttt{test} set, which is 9.0 points higher than previous best arts and on par with the performance of LiDAR-based baselines. We further show that BEVFormer remarkably improves the accuracy of velocity estimation and recall of objects under low visibility conditions. The code is available at \url{https://github.com/zhiqi-li/BEVFormer}.},
  archiveprefix    = {arXiv},
  comment          = {questions:
* does the transformer-based BEV encoder only contribute to the feature generation and not to the BB representation in the feature space as in Deep Fusion 2 "Association and Update" module?
* is the 3D object detection head considered the decoder of the transformer?
* how does the temporal self-attention cope with target vehicle movement after ego motion compensation? --> offsets delta p?},
  creationdate     = {2024-10-15T09:04:35},
  eprint           = {2203.17270},
  file             = {:Li2022 - BEVFormer_ Learning Bird's Eye View Representation from Multi Camera Images Via Spatiotemporal Transformers.pdf:PDF},
  groups           = {Reading Group Sparse Query},
  keywords         = {cs.CV},
  modificationdate = {2024-10-24T10:55:24},
  primaryclass     = {cs.CV},
  readstatus       = {read},
}

@Article{Li2022a,
  author           = {Feng Li and Hao Zhang and Shilong Liu and Jian Guo and Lionel M. Ni and Lei Zhang},
  title            = {DN-DETR: Accelerate DETR Training by Introducing Query DeNoising},
  year             = {2022},
  month            = mar,
  abstract         = {We present in this paper a novel denoising training method to speedup DETR (DEtection TRansformer) training and offer a deepened understanding of the slow convergence issue of DETR-like methods. We show that the slow convergence results from the instability of bipartite graph matching which causes inconsistent optimization goals in early training stages. To address this issue, except for the Hungarian loss, our method additionally feeds ground-truth bounding boxes with noises into Transformer decoder and trains the model to reconstruct the original boxes, which effectively reduces the bipartite graph matching difficulty and leads to a faster convergence. Our method is universal and can be easily plugged into any DETR-like methods by adding dozens of lines of code to achieve a remarkable improvement. As a result, our DN-DETR results in a remarkable improvement ($+1.9$AP) under the same setting and achieves the best result (AP $43.4$ and $48.6$ with $12$ and $50$ epochs of training respectively) among DETR-like methods with ResNet-$50$ backbone. Compared with the baseline under the same setting, DN-DETR achieves comparable performance with $50\%$ training epochs. Code is available at \url{https://github.com/FengLi-ust/DN-DETR}.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-11-13T14:48:48},
  eprint           = {2203.01305},
  file             = {:Li2022a - DN DETR_ Accelerate DETR Training by Introducing Query DeNoising.pdf:PDF},
  keywords         = {cs.CV, cs.AI},
  modificationdate = {2024-11-13T14:48:52},
  primaryclass     = {cs.CV},
}

@Article{Li2023,
  author           = {Tianyu Li and Li Chen and Huijie Wang and Yang Li and Jiazhi Yang and Xiangwei Geng and Shengyin Jiang and Yuting Wang and Hang Xu and Chunjing Xu and Junchi Yan and Ping Luo and Hongyang Li},
  title            = {Graph-based Topology Reasoning for Driving Scenes},
  year             = {2023},
  month            = apr,
  abstract         = {Understanding the road genome is essential to realize autonomous driving. This highly intelligent problem contains two aspects - the connection relationship of lanes, and the assignment relationship between lanes and traffic elements, where a comprehensive topology reasoning method is vacant. On one hand, previous map learning techniques struggle in deriving lane connectivity with segmentation or laneline paradigms; or prior lane topology-oriented approaches focus on centerline detection and neglect the interaction modeling. On the other hand, the traffic element to lane assignment problem is limited in the image domain, leaving how to construct the correspondence from two views an unexplored challenge. To address these issues, we present TopoNet, the first end-to-end framework capable of abstracting traffic knowledge beyond conventional perception tasks. To capture the driving scene topology, we introduce three key designs: (1) an embedding module to incorporate semantic knowledge from 2D elements into a unified feature space; (2) a curated scene graph neural network to model relationships and enable feature interaction inside the network; (3) instead of transmitting messages arbitrarily, a scene knowledge graph is devised to differentiate prior knowledge from various types of the road genome. We evaluate TopoNet on the challenging scene understanding benchmark, OpenLane-V2, where our approach outperforms all previous works by a great margin on all perceptual and topological metrics. The code is released at https://github.com/OpenDriveLab/TopoNet},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:09:16},
  eprint           = {2304.05277},
  file             = {:Li2023 - Graph Based Topology Reasoning for Driving Scenes.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2024-10-15T09:09:25},
  primaryclass     = {cs.CV},
}

@Article{Li2023a,
  author           = {Tianyu Li and Peijin Jia and Bangjun Wang and Li Chen and Kun Jiang and Junchi Yan and Hongyang Li},
  title            = {LaneSegNet: Map Learning with Lane Segment Perception for Autonomous Driving},
  year             = {2023},
  month            = dec,
  abstract         = {A map, as crucial information for downstream applications of an autonomous driving system, is usually represented in lanelines or centerlines. However, existing literature on map learning primarily focuses on either detecting geometry-based lanelines or perceiving topology relationships of centerlines. Both of these methods ignore the intrinsic relationship of lanelines and centerlines, that lanelines bind centerlines. While simply predicting both types of lane in one model is mutually excluded in learning objective, we advocate lane segment as a new representation that seamlessly incorporates both geometry and topology information. Thus, we introduce LaneSegNet, the first end-to-end mapping network generating lane segments to obtain a complete representation of the road structure. Our algorithm features two key modifications. One is a lane attention module to capture pivotal region details within the long-range feature space. Another is an identical initialization strategy for reference points, which enhances the learning of positional priors for lane attention. On the OpenLane-V2 dataset, LaneSegNet outperforms previous counterparts by a substantial gain across three tasks, \textit{i.e.}, map element detection (+4.8 mAP), centerline perception (+6.9 DET$_l$), and the newly defined one, lane segment perception (+5.6 mAP). Furthermore, it obtains a real-time inference speed of 14.7 FPS. Code is accessible at https://github.com/OpenDriveLab/LaneSegNet.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:12:00},
  eprint           = {2312.16108},
  file             = {:Li2023a - LaneSegNet_ Map Learning with Lane Segment Perception for Autonomous Driving.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2024-10-15T09:12:06},
  primaryclass     = {cs.CV},
}

@Article{Liao2022,
  author           = {Bencheng Liao and Shaoyu Chen and Xinggang Wang and Tianheng Cheng and Qian Zhang and Wenyu Liu and Chang Huang},
  title            = {MapTR: Structured Modeling and Learning for Online Vectorized HD Map Construction},
  year             = {2022},
  month            = aug,
  abstract         = {High-definition (HD) map provides abundant and precise environmental information of the driving scene, serving as a fundamental and indispensable component for planning in autonomous driving system. We present MapTR, a structured end-to-end Transformer for efficient online vectorized HD map construction. We propose a unified permutation-equivalent modeling approach, i.e., modeling map element as a point set with a group of equivalent permutations, which accurately describes the shape of map element and stabilizes the learning process. We design a hierarchical query embedding scheme to flexibly encode structured map information and perform hierarchical bipartite matching for map element learning. MapTR achieves the best performance and efficiency with only camera input among existing vectorized map construction approaches on nuScenes dataset. In particular, MapTR-nano runs at real-time inference speed ($25.1$ FPS) on RTX 3090, $8\times$ faster than the existing state-of-the-art camera-based method while achieving $5.0$ higher mAP. Even compared with the existing state-of-the-art multi-modality method, MapTR-nano achieves $0.7$ higher mAP, and MapTR-tiny achieves $13.5$ higher mAP and $3\times$ faster inference speed. Abundant qualitative results show that MapTR maintains stable and robust map construction quality in complex and various driving scenes. MapTR is of great application value in autonomous driving. Code and more demos are available at \url{https://github.com/hustvl/MapTR}.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:07:35},
  eprint           = {2208.14437},
  file             = {:Liao2022 - MapTR_ Structured Modeling and Learning for Online Vectorized HD Map Construction.pdf:PDF},
  keywords         = {cs.CV, cs.RO},
  modificationdate = {2024-10-15T09:07:38},
  primaryclass     = {cs.CV},
}

@Article{Liao2023,
  author           = {Bencheng Liao and Shaoyu Chen and Yunchi Zhang and Bo Jiang and Qian Zhang and Wenyu Liu and Chang Huang and Xinggang Wang},
  title            = {MapTRv2: An End-to-End Framework for Online Vectorized HD Map Construction},
  year             = {2023},
  month            = aug,
  abstract         = {High-definition (HD) map provides abundant and precise static environmental information of the driving scene, serving as a fundamental and indispensable component for planning in autonomous driving system. In this paper, we present \textbf{Map} \textbf{TR}ansformer, an end-to-end framework for online vectorized HD map construction. We propose a unified permutation-equivalent modeling approach, \ie, modeling map element as a point set with a group of equivalent permutations, which accurately describes the shape of map element and stabilizes the learning process. We design a hierarchical query embedding scheme to flexibly encode structured map information and perform hierarchical bipartite matching for map element learning. To speed up convergence, we further introduce auxiliary one-to-many matching and dense supervision. The proposed method well copes with various map elements with arbitrary shapes. It runs at real-time inference speed and achieves state-of-the-art performance on both nuScenes and Argoverse2 datasets. Abundant qualitative results show stable and robust map construction quality in complex and various driving scenes. Code and more demos are available at \url{https://github.com/hustvl/MapTR} for facilitating further studies and applications.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:11:30},
  eprint           = {2308.05736},
  file             = {:Liao2023 - MapTRv2_ an End to End Framework for Online Vectorized HD Map Construction.pdf:PDF},
  keywords         = {cs.CV, cs.RO},
  modificationdate = {2024-10-15T09:11:45},
  primaryclass     = {cs.CV},
}

@Article{Lin2022,
  author           = {Xuewu Lin and Tianwei Lin and Zixiang Pei and Lichao Huang and Zhizhong Su},
  title            = {Sparse4D: Multi-view 3D Object Detection with Sparse Spatial-Temporal Fusion},
  year             = {2022},
  month            = nov,
  abstract         = {Bird-eye-view (BEV) based methods have made great progress recently in multi-view 3D detection task. Comparing with BEV based methods, sparse based methods lag behind in performance, but still have lots of non-negligible merits. To push sparse 3D detection further, in this work, we introduce a novel method, named Sparse4D, which does the iterative refinement of anchor boxes via sparsely sampling and fusing spatial-temporal features. (1) Sparse 4D Sampling: for each 3D anchor, we assign multiple 4D keypoints, which are then projected to multi-view/scale/timestamp image features to sample corresponding features; (2) Hierarchy Feature Fusion: we hierarchically fuse sampled features of different view/scale, different timestamp and different keypoints to generate high-quality instance feature. In this way, Sparse4D can efficiently and effectively achieve 3D detection without relying on dense view transformation nor global attention, and is more friendly to edge devices deployment. Furthermore, we introduce an instance-level depth reweight module to alleviate the ill-posed issue in 3D-to-2D projection. In experiment, our method outperforms all sparse based methods and most BEV based methods on detection task in the nuScenes dataset.},
  archiveprefix    = {arXiv},
  eprint           = {2211.10581},
  file             = {:Lin2022 - Sparse4D_ Multi View 3D Object Detection with Sparse Spatial Temporal Fusion.pdf:PDF},
  groups           = {Reading Group Sparse Query},
  keywords         = {cs.CV},
  modificationdate = {2024-10-21T14:29:24},
  primaryclass     = {cs.CV},
  priority         = {prio1},
}

@Article{Lin2023,
  author           = {Xuewu Lin and Tianwei Lin and Zixiang Pei and Lichao Huang and Zhizhong Su},
  title            = {Sparse4D v2: Recurrent Temporal Fusion with Sparse Model},
  year             = {2023},
  month            = may,
  abstract         = {Sparse algorithms offer great flexibility for multi-view temporal perception tasks. In this paper, we present an enhanced version of Sparse4D, in which we improve the temporal fusion module by implementing a recursive form of multi-frame feature sampling. By effectively decoupling image features and structured anchor features, Sparse4D enables a highly efficient transformation of temporal features, thereby facilitating temporal fusion solely through the frame-by-frame transmission of sparse features. The recurrent temporal fusion approach provides two main benefits. Firstly, it reduces the computational complexity of temporal fusion from $O(T)$ to $O(1)$, resulting in significant improvements in inference speed and memory usage. Secondly, it enables the fusion of long-term information, leading to more pronounced performance improvements due to temporal fusion. Our proposed approach, Sparse4Dv2, further enhances the performance of the sparse perception algorithm and achieves state-of-the-art results on the nuScenes 3D detection benchmark. Code will be available at \url{https://github.com/linxuewu/Sparse4D}.},
  archiveprefix    = {arXiv},
  eprint           = {2305.14018},
  file             = {:Lin2023 - Sparse4D V2_ Recurrent Temporal Fusion with Sparse Model.pdf:PDF},
  groups           = {Reading Group Sparse Query},
  keywords         = {cs.CV},
  modificationdate = {2024-10-21T14:29:24},
  primaryclass     = {cs.CV},
  priority         = {prio1},
}

@Article{Lin2023a,
  author           = {Xuewu Lin and Zixiang Pei and Tianwei Lin and Lichao Huang and Zhizhong Su},
  title            = {Sparse4D v3: Advancing End-to-End 3D Detection and Tracking},
  year             = {2023},
  month            = nov,
  abstract         = {In autonomous driving perception systems, 3D detection and tracking are the two fundamental tasks. This paper delves deeper into this field, building upon the Sparse4D framework. We introduce two auxiliary training tasks (Temporal Instance Denoising and Quality Estimation) and propose decoupled attention to make structural improvements, leading to significant enhancements in detection performance. Additionally, we extend the detector into a tracker using a straightforward approach that assigns instance ID during inference, further highlighting the advantages of query-based algorithms. Extensive experiments conducted on the nuScenes benchmark validate the effectiveness of the proposed improvements. With ResNet50 as the backbone, we witnessed enhancements of 3.0\%, 2.2\%, and 7.6\% in mAP, NDS, and AMOTA, achieving 46.9\%, 56.1\%, and 49.0\%, respectively. Our best model achieved 71.9\% NDS and 67.7\% AMOTA on the nuScenes test set. Code will be released at \url{https://github.com/linxuewu/Sparse4D}.},
  archiveprefix    = {arXiv},
  eprint           = {2311.11722},
  file             = {:Lin2023a - Sparse4D V3_ Advancing End to End 3D Detection and Tracking.pdf:PDF},
  groups           = {Reading Group Sparse Query},
  keywords         = {cs.CV, cs.AI, cs.RO},
  modificationdate = {2024-10-21T14:29:24},
  primaryclass     = {cs.CV},
  priority         = {prio1},
}

@Article{Lin2024,
  author           = {Zhiwei Lin and Zhe Liu and Yongtao Wang and Le Zhang and Ce Zhu},
  title            = {RCBEVDet++: Toward High-accuracy Radar-Camera Fusion 3D Perception Network},
  year             = {2024},
  month            = sep,
  abstract         = {Perceiving the surrounding environment is a fundamental task in autonomous driving. To obtain highly accurate perception results, modern autonomous driving systems typically employ multi-modal sensors to collect comprehensive environmental data. Among these, the radar-camera multi-modal perception system is especially favored for its excellent sensing capabilities and cost-effectiveness. However, the substantial modality differences between radar and camera sensors pose challenges in fusing information. To address this problem, this paper presents RCBEVDet, a radar-camera fusion 3D object detection framework. Specifically, RCBEVDet is developed from an existing camera-based 3D object detector, supplemented by a specially designed radar feature extractor, RadarBEVNet, and a Cross-Attention Multi-layer Fusion (CAMF) module. Firstly, RadarBEVNet encodes sparse radar points into a dense bird's-eye-view (BEV) feature using a dual-stream radar backbone and a Radar Cross Section aware BEV encoder. Secondly, the CAMF module utilizes a deformable attention mechanism to align radar and camera BEV features and adopts channel and spatial fusion layers to fuse them. To further enhance RCBEVDet's capabilities, we introduce RCBEVDet++, which advances the CAMF through sparse fusion, supports query-based multi-view camera perception models, and adapts to a broader range of perception tasks. Extensive experiments on the nuScenes show that our method integrates seamlessly with existing camera-based 3D perception models and improves their performance across various perception tasks. Furthermore, our method achieves state-of-the-art radar-camera fusion results in 3D object detection, BEV semantic segmentation, and 3D multi-object tracking tasks. Notably, with ViT-L as the image backbone, RCBEVDet++ achieves 72.73 NDS and 67.34 mAP in 3D object detection without test-time augmentation or model ensembling.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:15:57},
  eprint           = {2409.04979},
  file             = {:- RCBEVDet++_ toward High Accuracy Radar Camera Fusion 3D Perception Network.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2024-10-15T09:15:58},
  primaryclass     = {cs.CV},
}

@Article{Lin2024a,
  author           = {Zhiwei Lin and Zhe Liu and Zhongyu Xia and Xinhao Wang and Yongtao Wang and Shengxiang Qi and Yang Dong and Nan Dong and Le Zhang and Ce Zhu},
  title            = {RCBEVDet: Radar-camera Fusion in Bird's Eye View for 3D Object Detection},
  year             = {2024},
  month            = mar,
  abstract         = {Three-dimensional object detection is one of the key tasks in autonomous driving. To reduce costs in practice, low-cost multi-view cameras for 3D object detection are proposed to replace the expansive LiDAR sensors. However, relying solely on cameras is difficult to achieve highly accurate and robust 3D object detection. An effective solution to this issue is combining multi-view cameras with the economical millimeter-wave radar sensor to achieve more reliable multi-modal 3D object detection. In this paper, we introduce RCBEVDet, a radar-camera fusion 3D object detection method in the bird's eye view (BEV). Specifically, we first design RadarBEVNet for radar BEV feature extraction. RadarBEVNet consists of a dual-stream radar backbone and a Radar Cross-Section (RCS) aware BEV encoder. In the dual-stream radar backbone, a point-based encoder and a transformer-based encoder are proposed to extract radar features, with an injection and extraction module to facilitate communication between the two encoders. The RCS-aware BEV encoder takes RCS as the object size prior to scattering the point feature in BEV. Besides, we present the Cross-Attention Multi-layer Fusion module to automatically align the multi-modal BEV feature from radar and camera with the deformable attention mechanism, and then fuse the feature with channel and spatial fusion layers. Experimental results show that RCBEVDet achieves new state-of-the-art radar-camera fusion results on nuScenes and view-of-delft (VoD) 3D object detection benchmarks. Furthermore, RCBEVDet achieves better 3D detection results than all real-time camera-only and radar-camera 3D object detectors with a faster inference speed at 21~28 FPS. The source code will be released at https://github.com/VDIGPKU/RCBEVDet.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:15:57},
  eprint           = {2403.16440},
  file             = {:Lin2024a - RCBEVDet_ Radar Camera Fusion in Bird's Eye View for 3D Object Detection.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2024-10-15T09:15:59},
  primaryclass     = {cs.CV},
}

@Article{Liu2022,
  author           = {Yingfei Liu and Tiancai Wang and Xiangyu Zhang and Jian Sun},
  title            = {PETR: Position Embedding Transformation for Multi-View 3D Object Detection},
  year             = {2022},
  month            = mar,
  abstract         = {In this paper, we develop position embedding transformation (PETR) for multi-view 3D object detection. PETR encodes the position information of 3D coordinates into image features, producing the 3D position-aware features. Object query can perceive the 3D position-aware features and perform end-to-end object detection. PETR achieves state-of-the-art performance (50.4% NDS and 44.1% mAP) on standard nuScenes dataset and ranks 1st place on the benchmark. It can serve as a simple yet strong baseline for future research. Code is available at \url{https://github.com/megvii-research/PETR}.},
  archiveprefix    = {arXiv},
  eprint           = {2203.05625},
  file             = {:Liu2022 - PETR_ Position Embedding Transformation for Multi View 3D Object Detection.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-01-22T16:46:00},
  primaryclass     = {cs.CV},
  priority         = {prio1},
}

@Article{Liu2022a,
  author           = {Yingfei Liu and Junjie Yan and Fan Jia and Shuailin Li and Aqi Gao and Tiancai Wang and Xiangyu Zhang and Jian Sun},
  title            = {PETRv2: A Unified Framework for 3D Perception from Multi-Camera Images},
  year             = {2022},
  month            = jun,
  abstract         = {In this paper, we propose PETRv2, a unified framework for 3D perception from multi-view images. Based on PETR, PETRv2 explores the effectiveness of temporal modeling, which utilizes the temporal information of previous frames to boost 3D object detection. More specifically, we extend the 3D position embedding (3D PE) in PETR for temporal modeling. The 3D PE achieves the temporal alignment on object position of different frames. A feature-guided position encoder is further introduced to improve the data adaptability of 3D PE. To support for multi-task learning (e.g., BEV segmentation and 3D lane detection), PETRv2 provides a simple yet effective solution by introducing task-specific queries, which are initialized under different spaces. PETRv2 achieves state-of-the-art performance on 3D object detection, BEV segmentation and 3D lane detection. Detailed robustness analysis is also conducted on PETR framework. We hope PETRv2 can serve as a strong baseline for 3D perception. Code is available at \url{https://github.com/megvii-research/PETR}.},
  archiveprefix    = {arXiv},
  eprint           = {2206.01256},
  file             = {:Liu2022a - PETRv2_ a Unified Framework for 3D Perception from Multi Camera Images.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-01-22T16:46:05},
  primaryclass     = {cs.CV},
  priority         = {prio1},
}

@Article{Liu2022b,
  author           = {Zhijian Liu and Haotian Tang and Alexander Amini and Xinyu Yang and Huizi Mao and Daniela Rus and Song Han},
  title            = {BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird's-Eye View Representation},
  year             = {2022},
  month            = may,
  abstract         = {Multi-sensor fusion is essential for an accurate and reliable autonomous driving system. Recent approaches are based on point-level fusion: augmenting the LiDAR point cloud with camera features. However, the camera-to-LiDAR projection throws away the semantic density of camera features, hindering the effectiveness of such methods, especially for semantic-oriented tasks (such as 3D scene segmentation). In this paper, we break this deeply-rooted convention with BEVFusion, an efficient and generic multi-task multi-sensor fusion framework. It unifies multi-modal features in the shared bird's-eye view (BEV) representation space, which nicely preserves both geometric and semantic information. To achieve this, we diagnose and lift key efficiency bottlenecks in the view transformation with optimized BEV pooling, reducing latency by more than 40x. BEVFusion is fundamentally task-agnostic and seamlessly supports different 3D perception tasks with almost no architectural changes. It establishes the new state of the art on nuScenes, achieving 1.3% higher mAP and NDS on 3D object detection and 13.6% higher mIoU on BEV map segmentation, with 1.9x lower computation cost. Code to reproduce our results is available at https://github.com/mit-han-lab/bevfusion.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:06:03},
  eprint           = {2205.13542},
  file             = {:Liu2022b - BEVFusion_ Multi Task Multi Sensor Fusion with Unified Bird's Eye View Representation.pdf:PDF},
  groups           = {Reading Group Sparse Query},
  keywords         = {cs.CV},
  modificationdate = {2025-01-22T16:52:26},
  primaryclass     = {cs.CV},
  readstatus       = {read},
}

@Article{Liu2023,
  author           = {Haisong Liu and Yao Teng and Tao Lu and Haiguang Wang and Limin Wang},
  title            = {SparseBEV: High-Performance Sparse 3D Object Detection from Multi-Camera Videos},
  year             = {2023},
  month            = aug,
  abstract         = {Camera-based 3D object detection in BEV (Bird's Eye View) space has drawn great attention over the past few years. Dense detectors typically follow a two-stage pipeline by first constructing a dense BEV feature and then performing object detection in BEV space, which suffers from complex view transformations and high computation cost. On the other side, sparse detectors follow a query-based paradigm without explicit dense BEV feature construction, but achieve worse performance than the dense counterparts. In this paper, we find that the key to mitigate this performance gap is the adaptability of the detector in both BEV and image space. To achieve this goal, we propose SparseBEV, a fully sparse 3D object detector that outperforms the dense counterparts. SparseBEV contains three key designs, which are (1) scale-adaptive self attention to aggregate features with adaptive receptive field in BEV space, (2) adaptive spatio-temporal sampling to generate sampling locations under the guidance of queries, and (3) adaptive mixing to decode the sampled features with dynamic weights from the queries. On the test split of nuScenes, SparseBEV achieves the state-of-the-art performance of 67.5 NDS. On the val split, SparseBEV achieves 55.8 NDS while maintaining a real-time inference speed of 23.5 FPS. Code is available at https://github.com/MCG-NJU/SparseBEV.},
  archiveprefix    = {arXiv},
  comment          = {questions:
* pillars instead of reference points as formulation of queries; what are pillars?
* how can I understand sparse pillar queries?
* in 3.1 it says that learnable queries are initialized with ...; is this like a set of trained weights or are these computed during inference and if so, why are these then "learnable"?},
  creationdate     = {2024-09-30T15:55:05},
  eprint           = {2308.09244},
  file             = {:Liu2023 - SparseBEV_ High Performance Sparse 3D Object Detection from Multi Camera Videos.pdf:PDF},
  groups           = {Reading Group Sparse Query},
  keywords         = {cs.CV},
  modificationdate = {2025-01-24T15:03:43},
  primaryclass     = {cs.CV},
  priority         = {prio1},
  readstatus       = {read},
}

@Article{Liu2023a,
  author           = {Haisong Liu and Yang Chen and Haiguang Wang and Zetong Yang and Tianyu Li and Jia Zeng and Li Chen and Hongyang Li and Limin Wang},
  title            = {Fully Sparse 3D Occupancy Prediction},
  year             = {2023},
  month            = dec,
  abstract         = {Occupancy prediction plays a pivotal role in autonomous driving. Previous methods typically construct dense 3D volumes, neglecting the inherent sparsity of the scene and suffering from high computational costs. To bridge the gap, we introduce a novel fully sparse occupancy network, termed SparseOcc. SparseOcc initially reconstructs a sparse 3D representation from camera-only inputs and subsequently predicts semantic/instance occupancy from the 3D sparse representation by sparse queries. A mask-guided sparse sampling is designed to enable sparse queries to interact with 2D features in a fully sparse manner, thereby circumventing costly dense features or global attention. Additionally, we design a thoughtful ray-based evaluation metric, namely RayIoU, to solve the inconsistency penalty along the depth axis raised in traditional voxel-level mIoU criteria. SparseOcc demonstrates its effectiveness by achieving a RayIoU of 34.0, while maintaining a real-time inference speed of 17.3 FPS, with 7 history frames inputs. By incorporating more preceding frames to 15, SparseOcc continuously improves its performance to 35.1 RayIoU without bells and whistles.},
  archiveprefix    = {arXiv},
  comment          = {mentioned by Christoph Schöller "Looking into how well it is possible to represent the static environment via a sparse encoding would be a different or future task."},
  creationdate     = {2024-10-31T09:04:02},
  eprint           = {2312.17118},
  file             = {:Liu2023a - Fully Sparse 3D Occupancy Prediction.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2024-10-31T09:04:37},
  primaryclass     = {cs.CV},
}

@Article{Lv2024,
  author           = {Wenyu Lv and Yian Zhao and Qinyao Chang and Kui Huang and Guanzhong Wang and Yi Liu},
  title            = {RT-DETRv2: Improved Baseline with Bag-of-Freebies for Real-Time Detection Transformer},
  year             = {2024},
  month            = jul,
  abstract         = {In this report, we present RT-DETRv2, an improved Real-Time DEtection TRansformer (RT-DETR). RT-DETRv2 builds upon the previous state-of-the-art real-time detector, RT-DETR, and opens up a set of bag-of-freebies for flexibility and practicality, as well as optimizing the training strategy to achieve enhanced performance. To improve the flexibility, we suggest setting a distinct number of sampling points for features at different scales in the deformable attention to achieve selective multi-scale feature extraction by the decoder. To enhance practicality, we propose an optional discrete sampling operator to replace the grid_sample operator that is specific to RT-DETR compared to YOLOs. This removes the deployment constraints typically associated with DETRs. For the training strategy, we propose dynamic data augmentation and scale-adaptive hyperparameters customization to improve performance without loss of speed. Source code and pre-trained models will be available at https://github.com/lyuwenyu/RT-DETR.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:15:03},
  eprint           = {2407.17140},
  file             = {:- RT DETRv2_ Improved Baseline with Bag of Freebies for Real Time Detection Transformer.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2024-10-15T09:15:03},
  primaryclass     = {cs.CV},
}

@Article{Ma2022,
  author           = {Yuexin Ma and Tai Wang and Xuyang Bai and Huitong Yang and Yuenan Hou and Yaming Wang and Yu Qiao and Ruigang Yang and Dinesh Manocha and Xinge Zhu},
  title            = {Vision-Centric BEV Perception: A Survey},
  year             = {2022},
  month            = aug,
  abstract         = {In recent years, vision-centric Bird's Eye View (BEV) perception has garnered significant interest from both industry and academia due to its inherent advantages, such as providing an intuitive representation of the world and being conducive to data fusion. The rapid advancements in deep learning have led to the proposal of numerous methods for addressing vision-centric BEV perception challenges. However, there has been no recent survey encompassing this novel and burgeoning research field. To catalyze future research, this paper presents a comprehensive survey of the latest developments in vision-centric BEV perception and its extensions. It compiles and organizes up-to-date knowledge, offering a systematic review and summary of prevalent algorithms. Additionally, the paper provides in-depth analyses and comparative results on various BEV perception tasks, facilitating the evaluation of future works and sparking new research directions. Furthermore, the paper discusses and shares valuable empirical implementation details to aid in the advancement of related algorithms.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:08:08},
  eprint           = {2208.02797},
  file             = {:Ma2022 - Vision Centric BEV Perception_ a Survey.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2024-10-15T09:08:09},
  primaryclass     = {cs.CV},
}

@Article{Min2023,
  author           = {Lejun Min and Junyan Jiang and Gus Xia and Jingwei Zhao},
  title            = {Polyffusion: A Diffusion Model for Polyphonic Score Generation with Internal and External Controls},
  year             = {2023},
  month            = jul,
  abstract         = {We propose Polyffusion, a diffusion model that generates polyphonic music scores by regarding music as image-like piano roll representations. The model is capable of controllable music generation with two paradigms: internal control and external control. Internal control refers to the process in which users pre-define a part of the music and then let the model infill the rest, similar to the task of masked music generation (or music inpainting). External control conditions the model with external yet related information, such as chord, texture, or other features, via the cross-attention mechanism. We show that by using internal and external controls, Polyffusion unifies a wide range of music creation tasks, including melody generation given accompaniment, accompaniment generation given melody, arbitrary music segment inpainting, and music arrangement given chords or textures. Experimental results show that our model significantly outperforms existing Transformer and sampling-based baselines, and using pre-trained disentangled representations as external conditions yields more effective controls.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:10:50},
  eprint           = {2307.10304},
  file             = {:Min2023 - Polyffusion_ a Diffusion Model for Polyphonic Score Generation with Internal and External Controls.pdf:PDF},
  keywords         = {cs.SD, cs.LG, eess.AS},
  modificationdate = {2024-10-15T09:10:52},
  primaryclass     = {cs.SD},
}

@Article{Pan2024,
  author           = {Chenbin Pan and Burhaneddin Yaman and Tommaso Nesti and Abhirup Mallik and Alessandro G Allievi and Senem Velipasalar and Liu Ren},
  journal          = {CVPR2024},
  title            = {VLP: Vision Language Planning for Autonomous Driving},
  year             = {2024},
  month            = jan,
  abstract         = {Autonomous driving is a complex and challenging task that aims at safe motion planning through scene understanding and reasoning. While vision-only autonomous driving methods have recently achieved notable performance, through enhanced scene understanding, several key issues, including lack of reasoning, low generalization performance and long-tail scenarios, still need to be addressed. In this paper, we present VLP, a novel Vision-Language-Planning framework that exploits language models to bridge the gap between linguistic understanding and autonomous driving. VLP enhances autonomous driving systems by strengthening both the source memory foundation and the self-driving car's contextual understanding. VLP achieves state-of-the-art end-to-end planning performance on the challenging NuScenes dataset by achieving 35.9\% and 60.5\% reduction in terms of average L2 error and collision rates, respectively, compared to the previous best method. Moreover, VLP shows improved performance in challenging long-tail scenarios and strong generalization capabilities when faced with new urban environments.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:12:56},
  eprint           = {2401.05577},
  file             = {:Pan2024 - VLP_ Vision Language Planning for Autonomous Driving.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2024-10-15T09:12:57},
  primaryclass     = {cs.CV},
}

@Article{Philion2020,
  author           = {Jonah Philion and Sanja Fidler},
  title            = {Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D},
  year             = {2020},
  month            = aug,
  abstract         = {The goal of perception for autonomous vehicles is to extract semantic representations from multiple sensors and fuse these representations into a single "bird's-eye-view" coordinate frame for consumption by motion planning. We propose a new end-to-end architecture that directly extracts a bird's-eye-view representation of a scene given image data from an arbitrary number of cameras. The core idea behind our approach is to "lift" each image individually into a frustum of features for each camera, then "splat" all frustums into a rasterized bird's-eye-view grid. By training on the entire camera rig, we provide evidence that our model is able to learn not only how to represent images but how to fuse predictions from all cameras into a single cohesive representation of the scene while being robust to calibration error. On standard bird's-eye-view tasks such as object segmentation and map segmentation, our model outperforms all baselines and prior work. In pursuit of the goal of learning dense representations for motion planning, we show that the representations inferred by our model enable interpretable end-to-end motion planning by "shooting" template trajectories into a bird's-eye-view cost map output by our network. We benchmark our approach against models that use oracle depth from lidar. Project page with code: https://nv-tlabs.github.io/lift-splat-shoot .},
  archiveprefix    = {arXiv},
  creationdate     = {2024-09-25T13:17:09},
  eprint           = {2008.05711},
  file             = {:Philion2020 - Lift, Splat, Shoot_ Encoding Images from Arbitrary Camera Rigs by Implicitly Unprojecting to 3D.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2024-10-15T09:03:34},
  primaryclass     = {cs.CV},
}

@Article{Pittner2024,
  author           = {Maximilian Pittner and Joel Janai and Alexandru P. Condurache},
  title            = {LaneCPP: Continuous 3D Lane Detection using Physical Priors},
  year             = {2024},
  month            = jun,
  abstract         = {Monocular 3D lane detection has become a fundamental problem in the context of autonomous driving, which comprises the tasks of finding the road surface and locating lane markings. One major challenge lies in a flexible but robust line representation capable of modeling complex lane structures, while still avoiding unpredictable behavior. While previous methods rely on fully data-driven approaches, we instead introduce a novel approach LaneCPP that uses a continuous 3D lane detection model leveraging physical prior knowledge about the lane structure and road geometry. While our sophisticated lane model is capable of modeling complex road structures, it also shows robust behavior since physical constraints are incorporated by means of a regularization scheme that can be analytically applied to our parametric representation. Moreover, we incorporate prior knowledge about the road geometry into the 3D feature space by modeling geometry-aware spatial features, guiding the network to learn an internal road surface representation. In our experiments, we show the benefits of our contributions and prove the meaningfulness of using priors to make 3D lane detection more robust. The results show that LaneCPP achieves state-of-the-art performance in terms of F-Score and geometric errors.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:14:38},
  eprint           = {2406.08381},
  file             = {:Pittner2024 - LaneCPP_ Continuous 3D Lane Detection Using Physical Priors.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2024-10-15T09:14:47},
  primaryclass     = {cs.CV},
}

@Article{Ruppel2022,
  author           = {Felicia Ruppel and Florian Faion and Claudius Gläser and Klaus Dietmayer},
  title            = {Transformers for Object Detection in Large Point Clouds},
  year             = {2022},
  month            = sep,
  abstract         = {We present TransLPC, a novel detection model for large point clouds that is based on a transformer architecture. While object detection with transformers has been an active field of research, it has proved difficult to apply such models to point clouds that span a large area, e.g. those that are common in autonomous driving, with lidar or radar data. TransLPC is able to remedy these issues: The structure of the transformer model is modified to allow for larger input sequence lengths, which are sufficient for large point clouds. Besides this, we propose a novel query refinement technique to improve detection accuracy, while retaining a memory-friendly number of transformer decoder queries. The queries are repositioned between layers, moving them closer to the bounding box they are estimating, in an efficient manner. This simple technique has a significant effect on detection accuracy, which is evaluated on the challenging nuScenes dataset on real-world lidar data. Besides this, the proposed method is compatible with existing transformer-based solutions that require object detection, e.g. for joint multi-object tracking and detection, and enables them to be used in conjunction with large point clouds.},
  archiveprefix    = {arXiv},
  comment          = {Bosch paper},
  creationdate     = {2024-10-22T10:02:53},
  eprint           = {2209.15258},
  file             = {:Ruppel2022 - Transformers for Object Detection in Large Point Clouds.pdf:PDF},
  keywords         = {cs.CV, cs.LG},
  modificationdate = {2024-10-22T10:03:46},
  primaryclass     = {cs.CV},
  priority         = {prio1},
}

@Article{Schmidhuber2014,
  author           = {Juergen Schmidhuber},
  journal          = {Neural Networks, Vol 61, pp 85-117, Jan 2015},
  title            = {Deep Learning in Neural Networks: An Overview},
  year             = {2014},
  month            = apr,
  abstract         = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning & evolutionary computation, and indirect search for short programs encoding deep and large networks.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-09-24T16:29:59},
  doi              = {10.1016/j.neunet.2014.09.003},
  eprint           = {1404.7828},
  file             = {:Schmidhuber2014 - Deep Learning in Neural Networks_ an Overview.pdf:PDF},
  keywords         = {cs.NE, cs.LG},
  modificationdate = {2024-10-15T09:03:24},
  primaryclass     = {cs.NE},
}

@Article{Shao2023,
  author           = {Hao Shao and Letian Wang and Ruobing Chen and Steven L. Waslander and Hongsheng Li and Yu Liu},
  title            = {ReasonNet: End-to-End Driving with Temporal and Global Reasoning},
  year             = {2023},
  month            = may,
  abstract         = {The large-scale deployment of autonomous vehicles is yet to come, and one of the major remaining challenges lies in urban dense traffic scenarios. In such cases, it remains challenging to predict the future evolution of the scene and future behaviors of objects, and to deal with rare adverse events such as the sudden appearance of occluded objects. In this paper, we present ReasonNet, a novel end-to-end driving framework that extensively exploits both temporal and global information of the driving scene. By reasoning on the temporal behavior of objects, our method can effectively process the interactions and relationships among features in different frames. Reasoning about the global information of the scene can also improve overall perception performance and benefit the detection of adverse events, especially the anticipation of potential danger from occluded objects. For comprehensive evaluation on occlusion events, we also release publicly a driving simulation benchmark DriveOcclusionSim consisting of diverse occlusion events. We conduct extensive experiments on multiple CARLA benchmarks, where our model outperforms all prior methods, ranking first on the sensor track of the public CARLA Leaderboard.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:09:45},
  eprint           = {2305.10507},
  file             = {:Shao2023 - ReasonNet_ End to End Driving with Temporal and Global Reasoning.pdf:PDF},
  keywords         = {cs.CV, cs.AI},
  modificationdate = {2024-10-15T09:09:46},
  primaryclass     = {cs.CV},
}

@Article{Sun2020,
  author           = {Peize Sun and Rufeng Zhang and Yi Jiang and Tao Kong and Chenfeng Xu and Wei Zhan and Masayoshi Tomizuka and Lei Li and Zehuan Yuan and Changhu Wang and Ping Luo},
  title            = {Sparse R-CNN: End-to-End Object Detection with Learnable Proposals},
  year             = {2020},
  month            = nov,
  abstract         = {We present Sparse R-CNN, a purely sparse method for object detection in images. Existing works on object detection heavily rely on dense object candidates, such as $k$ anchor boxes pre-defined on all grids of image feature map of size $H\times W$. In our method, however, a fixed sparse set of learned object proposals, total length of $N$, are provided to object recognition head to perform classification and location. By eliminating $HWk$ (up to hundreds of thousands) hand-designed object candidates to $N$ (e.g. 100) learnable proposals, Sparse R-CNN completely avoids all efforts related to object candidates design and many-to-one label assignment. More importantly, final predictions are directly output without non-maximum suppression post-procedure. Sparse R-CNN demonstrates accuracy, run-time and training convergence performance on par with the well-established detector baselines on the challenging COCO dataset, e.g., achieving 45.0 AP in standard $3\times$ training schedule and running at 22 fps using ResNet-50 FPN model. We hope our work could inspire re-thinking the convention of dense prior in object detectors. The code is available at: https://github.com/PeizeSun/SparseR-CNN.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-31T09:06:58},
  eprint           = {2011.12450},
  file             = {:Sun2020 - Sparse R CNN_ End to End Object Detection with Learnable Proposals.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2024-11-20T17:19:15},
  primaryclass     = {cs.CV},
}

@Article{Sun2024,
  author           = {Wenchao Sun and Xuewu Lin and Yining Shi and Chuang Zhang and Haoran Wu and Sifa Zheng},
  title            = {SparseDrive: End-to-End Autonomous Driving via Sparse Scene Representation},
  year             = {2024},
  month            = may,
  abstract         = {The well-established modular autonomous driving system is decoupled into different standalone tasks, e.g. perception, prediction and planning, suffering from information loss and error accumulation across modules. In contrast, end-to-end paradigms unify multi-tasks into a fully differentiable framework, allowing for optimization in a planning-oriented spirit. Despite the great potential of end-to-end paradigms, both the performance and efficiency of existing methods are not satisfactory, particularly in terms of planning safety. We attribute this to the computationally expensive BEV (bird's eye view) features and the straightforward design for prediction and planning. To this end, we explore the sparse representation and review the task design for end-to-end autonomous driving, proposing a new paradigm named SparseDrive. Concretely, SparseDrive consists of a symmetric sparse perception module and a parallel motion planner. The sparse perception module unifies detection, tracking and online mapping with a symmetric model architecture, learning a fully sparse representation of the driving scene. For motion prediction and planning, we review the great similarity between these two tasks, leading to a parallel design for motion planner. Based on this parallel design, which models planning as a multi-modal problem, we propose a hierarchical planning selection strategy , which incorporates a collision-aware rescore module, to select a rational and safe trajectory as the final planning output. With such effective designs, SparseDrive surpasses previous state-of-the-arts by a large margin in performance of all tasks, while achieving much higher training and inference efficiency. Code will be avaliable at https://github.com/swc-17/SparseDrive for facilitating future research.},
  archiveprefix    = {arXiv},
  eprint           = {2405.19620},
  file             = {:Sun2024 - SparseDrive_ End to End Autonomous Driving Via Sparse Scene Representation.pdf:PDF},
  groups           = {Reading Group Sparse Query},
  keywords         = {cs.CV},
  modificationdate = {2024-10-16T13:27:31},
  primaryclass     = {cs.CV},
}

@Article{Vaswani2017,
  author           = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  title            = {Attention Is All You Need},
  year             = {2017},
  month            = jun,
  abstract         = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix    = {arXiv},
  comment          = {basis for transformer models},
  creationdate     = {2024-09-17T11:23:11},
  eprint           = {1706.03762},
  file             = {:Vaswani2017 - Attention Is All You Need.pdf:PDF},
  keywords         = {cs.CL, cs.LG},
  modificationdate = {2025-01-24T15:03:18},
  primaryclass     = {cs.CL},
  priority         = {prio1},
  readstatus       = {read},
}

@Article{Wang2017,
  author           = {Haohan Wang and Bhiksha Raj},
  title            = {On the Origin of Deep Learning},
  year             = {2017},
  month            = feb,
  abstract         = {This paper is a review of the evolutionary history of deep learning models. It covers from the genesis of neural networks when associationism modeling of the brain is studied, to the models that dominate the last decade of research in deep learning like convolutional neural networks, deep belief networks, and recurrent neural networks. In addition to a review of these models, this paper primarily focuses on the precedents of the models above, examining how the initial ideas are assembled to construct the early models and how these preliminary models are developed into their current forms. Many of these evolutionary paths last more than half a century and have a diversity of directions. For example, CNN is built on prior knowledge of biological vision system; DBN is evolved from a trade-off of modeling power and computation complexity of graphical models and many nowadays models are neural counterparts of ancient linear models. This paper reviews these evolutionary paths and offers a concise thought flow of how these models are developed, and aims to provide a thorough background for deep learning. More importantly, along with the path, this paper summarizes the gist behind these milestones and proposes many directions to guide the future research of deep learning.},
  archiveprefix    = {arXiv},
  eprint           = {1702.07800},
  file             = {:Wang2017 - On the Origin of Deep Learning.pdf:PDF;:Wang2017 - On the Origin of Deep Learning.pdf:PDF},
  keywords         = {cs.LG, cs.NE, stat.ML},
  modificationdate = {2024-10-16T13:26:12},
  primaryclass     = {cs.LG},
}

@Article{Wang2019,
  author           = {Zhongdao Wang and Liang Zheng and Yixuan Liu and Yali Li and Shengjin Wang},
  title            = {Towards Real-Time Multi-Object Tracking},
  year             = {2019},
  month            = sep,
  abstract         = {Modern multiple object tracking (MOT) systems usually follow the \emph{tracking-by-detection} paradigm. It has 1) a detection model for target localization and 2) an appearance embedding model for data association. Having the two models separately executed might lead to efficiency problems, as the running time is simply a sum of the two steps without investigating potential structures that can be shared between them. Existing research efforts on real-time MOT usually focus on the association step, so they are essentially real-time association methods but not real-time MOT system. In this paper, we propose an MOT system that allows target detection and appearance embedding to be learned in a shared model. Specifically, we incorporate the appearance embedding model into a single-shot detector, such that the model can simultaneously output detections and the corresponding embeddings. We further propose a simple and fast association method that works in conjunction with the joint model. In both components the computation cost is significantly reduced compared with former MOT systems, resulting in a neat and fast baseline for future follow-ups on real-time MOT algorithm design. To our knowledge, this work reports the first (near) real-time MOT system, with a running speed of 22 to 40 FPS depending on the input resolution. Meanwhile, its tracking accuracy is comparable to the state-of-the-art trackers embodying separate detection and embedding (SDE) learning ($64.4\%$ MOTA \vs $66.1\%$ MOTA on MOT-16 challenge). Code and models are available at \url{https://github.com/Zhongdao/Towards-Realtime-MOT}.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-22T09:29:04},
  eprint           = {1909.12605},
  file             = {:Wang2019 - Towards Real Time Multi Object Tracking.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2024-10-22T09:29:06},
  primaryclass     = {cs.CV},
}

@Article{Wang2021,
  author           = {Yue Wang and Vitor Guizilini and Tianyuan Zhang and Yilun Wang and Hang Zhao and Justin Solomon},
  title            = {DETR3D: 3D Object Detection from Multi-view Images via 3D-to-2D Queries},
  year             = {2021},
  month            = oct,
  abstract         = {We introduce a framework for multi-camera 3D object detection. In contrast to existing works, which estimate 3D bounding boxes directly from monocular images or use depth prediction networks to generate input for 3D object detection from 2D information, our method manipulates predictions directly in 3D space. Our architecture extracts 2D features from multiple camera images and then uses a sparse set of 3D object queries to index into these 2D features, linking 3D positions to multi-view images using camera transformation matrices. Finally, our model makes a bounding box prediction per object query, using a set-to-set loss to measure the discrepancy between the ground-truth and the prediction. This top-down approach outperforms its bottom-up counterpart in which object bounding box prediction follows per-pixel depth estimation, since it does not suffer from the compounding error introduced by a depth prediction model. Moreover, our method does not require post-processing such as non-maximum suppression, dramatically improving inference speed. We achieve state-of-the-art performance on the nuScenes autonomous driving benchmark.},
  archiveprefix    = {arXiv},
  eprint           = {2110.06922},
  file             = {:Wang2021 - DETR3D_ 3D Object Detection from Multi View Images Via 3D to 2D Queries.pdf:PDF},
  keywords         = {cs.CV, cs.AI, cs.LG, cs.RO},
  modificationdate = {2025-01-24T15:03:29},
  primaryclass     = {cs.CV},
  priority         = {prio1},
}

@Article{Wang2022,
  author           = {Shihao Wang and Xiaohui Jiang and Ying Li},
  title            = {Focal-PETR: Embracing Foreground for Efficient Multi-Camera 3D Object Detection},
  year             = {2022},
  month            = dec,
  abstract         = {The dominant multi-camera 3D detection paradigm is based on explicit 3D feature construction, which requires complicated indexing of local image-view features via 3D-to-2D projection. Other methods implicitly introduce geometric positional encoding and perform global attention (e.g., PETR) to build the relationship between image tokens and 3D objects. The 3D-to-2D perspective inconsistency and global attention lead to a weak correlation between foreground tokens and queries, resulting in slow convergence. We propose Focal-PETR with instance-guided supervision and spatial alignment module to adaptively focus object queries on discriminative foreground regions. Focal-PETR additionally introduces a down-sampling strategy to reduce the consumption of global attention. Due to the highly parallelized implementation and down-sampling strategy, our model, without depth supervision, achieves leading performance on the large-scale nuScenes benchmark and a superior speed of 30 FPS on a single RTX3090 GPU. Extensive experiments show that our method outperforms PETR while consuming 3x fewer training hours. The code will be made publicly available.},
  archiveprefix    = {arXiv},
  creationdate     = {2025-01-24T14:19:18},
  eprint           = {2212.05505},
  file             = {:Wang2022 - Focal PETR_ Embracing Foreground for Efficient Multi Camera 3D Object Detection.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-01-24T14:19:35},
  primaryclass     = {cs.CV},
  priority         = {prio2},
}

@Article{Wang2023,
  author           = {Wenhai Wang and Jiangwei Xie and ChuanYang Hu and Haoming Zou and Jianan Fan and Wenwen Tong and Yang Wen and Silei Wu and Hanming Deng and Zhiqi Li and Hao Tian and Lewei Lu and Xizhou Zhu and Xiaogang Wang and Yu Qiao and Jifeng Dai},
  title            = {DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving},
  year             = {2023},
  month            = dec,
  abstract         = {Large language models (LLMs) have opened up new possibilities for intelligent agents, endowing them with human-like thinking and cognitive abilities. In this work, we delve into the potential of large language models (LLMs) in autonomous driving (AD). We introduce DriveMLM, an LLM-based AD framework that can perform close-loop autonomous driving in realistic simulators. To this end, (1) we bridge the gap between the language decisions and the vehicle control commands by standardizing the decision states according to the off-the-shelf motion planning module. (2) We employ a multi-modal LLM (MLLM) to model the behavior planning module of a module AD system, which uses driving rules, user commands, and inputs from various sensors (e.g., camera, lidar) as input and makes driving decisions and provide explanations; This model can plug-and-play in existing AD systems such as Apollo for close-loop driving. (3) We design an effective data engine to collect a dataset that includes decision state and corresponding explanation annotation for model training and evaluation. We conduct extensive experiments and show that our model achieves 76.1 driving score on the CARLA Town05 Long, and surpasses the Apollo baseline by 4.7 points under the same settings, demonstrating the effectiveness of our model. We hope this work can serve as a baseline for autonomous driving with LLMs. Code and models shall be released at https://github.com/OpenGVLab/DriveMLM.},
  archiveprefix    = {arXiv},
  eprint           = {2312.09245},
  file             = {:Wang2023 - DriveMLM_ Aligning Multi Modal Large Language Models with Behavioral Planning States for Autonomous Driving.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2024-10-15T09:03:34},
  primaryclass     = {cs.CV},
}

@Article{Wang2023a,
  author           = {Shihao Wang and Yingfei Liu and Tiancai Wang and Ying Li and Xiangyu Zhang},
  title            = {Exploring Object-Centric Temporal Modeling for Efficient Multi-View 3D Object Detection},
  year             = {2023},
  month            = mar,
  abstract         = {In this paper, we propose a long-sequence modeling framework, named StreamPETR, for multi-view 3D object detection. Built upon the sparse query design in the PETR series, we systematically develop an object-centric temporal mechanism. The model is performed in an online manner and the long-term historical information is propagated through object queries frame by frame. Besides, we introduce a motion-aware layer normalization to model the movement of the objects. StreamPETR achieves significant performance improvements only with negligible computation cost, compared to the single-frame baseline. On the standard nuScenes benchmark, it is the first online multi-view method that achieves comparable performance (67.6% NDS & 65.3% AMOTA) with lidar-based methods. The lightweight version realizes 45.0% mAP and 31.7 FPS, outperforming the state-of-the-art method (SOLOFusion) by 2.3% mAP and 1.8x faster FPS. Code has been available at https://github.com/exiawsh/StreamPETR.git.},
  archiveprefix    = {arXiv},
  comment          = {StreamPETR},
  eprint           = {2303.11926},
  file             = {:Wang2023a - Exploring Object Centric Temporal Modeling for Efficient Multi View 3D Object Detection.pdf:PDF},
  groups           = {Reading Group Sparse Query},
  keywords         = {cs.CV},
  modificationdate = {2024-11-20T18:04:19},
  primaryclass     = {cs.CV},
  priority         = {prio1},
}

@Article{Wang2023b,
  author           = {Haiyang Wang and Hao Tang and Shaoshuai Shi and Aoxue Li and Zhenguo Li and Bernt Schiele and Liwei Wang},
  title            = {UniTR: A Unified and Efficient Multi-Modal Transformer for Bird's-Eye-View Representation},
  year             = {2023},
  month            = aug,
  abstract         = {Jointly processing information from multiple sensors is crucial to achieving accurate and robust perception for reliable autonomous driving systems. However, current 3D perception research follows a modality-specific paradigm, leading to additional computation overheads and inefficient collaboration between different sensor data. In this paper, we present an efficient multi-modal backbone for outdoor 3D perception named UniTR, which processes a variety of modalities with unified modeling and shared parameters. Unlike previous works, UniTR introduces a modality-agnostic transformer encoder to handle these view-discrepant sensor data for parallel modal-wise representation learning and automatic cross-modal interaction without additional fusion steps. More importantly, to make full use of these complementary sensor types, we present a novel multi-modal integration strategy by both considering semantic-abundant 2D perspective and geometry-aware 3D sparse neighborhood relations. UniTR is also a fundamentally task-agnostic backbone that naturally supports different 3D perception tasks. It sets a new state-of-the-art performance on the nuScenes benchmark, achieving +1.1 NDS higher for 3D object detection and +12.0 higher mIoU for BEV map segmentation with lower inference latency. Code will be available at https://github.com/Haiyang-W/UniTR .},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:11:45},
  eprint           = {2308.07732},
  file             = {:Wang2023b - UniTR_ a Unified and Efficient Multi Modal Transformer for Bird's Eye View Representation.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2024-10-15T09:11:45},
  primaryclass     = {cs.CV},
}

@Article{Wang2024,
  author           = {Shuo Wang and Chunlong Xia and Feng Lv and Yifeng Shi},
  title            = {RT-DETRv3: Real-time End-to-End Object Detection with Hierarchical Dense Positive Supervision},
  year             = {2024},
  month            = sep,
  abstract         = {RT-DETR is the first real-time end-to-end transformer-based object detector. Its efficiency comes from the framework design and the Hungarian matching. However, compared to dense supervision detectors like the YOLO series, the Hungarian matching provides much sparser supervision, leading to insufficient model training and difficult to achieve optimal results. To address these issues, we proposed a hierarchical dense positive supervision method based on RT-DETR, named RT-DETRv3. Firstly, we introduce a CNN-based auxiliary branch that provides dense supervision that collaborates with the original decoder to enhance the encoder feature representation. Secondly, to address insufficient decoder training, we propose a novel learning strategy involving self-attention perturbation. This strategy diversifies label assignment for positive samples across multiple query groups, thereby enriching positive supervisions. Additionally, we introduce a shared-weight decoder branch for dense positive supervision to ensure more high-quality queries matching each ground truth. Notably, all aforementioned modules are training-only. We conduct extensive experiments to demonstrate the effectiveness of our approach on COCO val2017. RT-DETRv3 significantly outperforms existing real-time detectors, including the RT-DETR series and the YOLO series. For example, RT-DETRv3-R18 achieves 48.1% AP (+1.6%/+1.4%) compared to RT-DETR-R18/RT-DETRv2-R18 while maintaining the same latency. Meanwhile, it requires only half of epochs to attain a comparable performance. Furthermore, RT-DETRv3-R101 can attain an impressive 54.6% AP outperforming YOLOv10-X. Code will be released soon.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:15:03},
  eprint           = {2409.08475},
  file             = {:Wang2024 - RT DETRv3_ Real Time End to End Object Detection with Hierarchical Dense Positive Supervision.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2024-10-15T09:15:04},
  primaryclass     = {cs.CV},
}

@Article{Wang2024a,
  author           = {Xiyang Wang and Shouzheng Qi and Jieyou Zhao and Hangning Zhou and Siyu Zhang and Guoan Wang and Kai Tu and Songlin Guo and Jianbo Zhao and Jian Li and Mu Yang},
  title            = {MCTrack: A Unified 3D Multi-Object Tracking Framework for Autonomous Driving},
  year             = {2024},
  month            = sep,
  abstract         = {This paper introduces MCTrack, a new 3D multi-object tracking method that achieves state-of-the-art (SOTA) performance across KITTI, nuScenes, and Waymo datasets. Addressing the gap in existing tracking paradigms, which often perform well on specific datasets but lack generalizability, MCTrack offers a unified solution. Additionally, we have standardized the format of perceptual results across various datasets, termed BaseVersion, facilitating researchers in the field of multi-object tracking (MOT) to concentrate on the core algorithmic development without the undue burden of data preprocessing. Finally, recognizing the limitations of current evaluation metrics, we propose a novel set that assesses motion information output, such as velocity and acceleration, crucial for downstream tasks. The source codes of the proposed method are available at this link: https://github.com/megvii-research/MCTrackhttps://github.com/megvii-research/MCTrack},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:20:51},
  eprint           = {2409.16149},
  file             = {:http\://arxiv.org/pdf/2409.16149v2:PDF},
  groups           = {Reading Group Sparse Query},
  keywords         = {cs.CV},
  modificationdate = {2024-10-16T13:27:22},
  primaryclass     = {cs.CV},
}

@Article{Wolters2024,
  author           = {Philipp Wolters and Johannes Gilg and Torben Teepe and Fabian Herzog and Anouar Laouichi and Martin Hofmann and Gerhard Rigoll},
  title            = {Unleashing HyDRa: Hybrid Fusion, Depth Consistency and Radar for Unified 3D Perception},
  year             = {2024},
  month            = mar,
  abstract         = {Low-cost, vision-centric 3D perception systems for autonomous driving have made significant progress in recent years, narrowing the gap to expensive LiDAR-based methods. The primary challenge in becoming a fully reliable alternative lies in robust depth prediction capabilities, as camera-based systems struggle with long detection ranges and adverse lighting and weather conditions. In this work, we introduce HyDRa, a novel camera-radar fusion architecture for diverse 3D perception tasks. Building upon the principles of dense BEV (Bird's Eye View)-based architectures, HyDRa introduces a hybrid fusion approach to combine the strengths of complementary camera and radar features in two distinct representation spaces. Our Height Association Transformer module leverages radar features already in the perspective view to produce more robust and accurate depth predictions. In the BEV, we refine the initial sparse representation by a Radar-weighted Depth Consistency. HyDRa achieves a new state-of-the-art for camera-radar fusion of 64.2 NDS (+1.8) and 58.4 AMOTA (+1.5) on the public nuScenes dataset. Moreover, our new semantically rich and spatially accurate BEV features can be directly converted into a powerful occupancy representation, beating all previous camera-based methods on the Occ3D benchmark by an impressive 3.7 mIoU. Code and models are available at https://github.com/phi-wol/hydra.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:13:23},
  eprint           = {2403.07746},
  file             = {:Wolters2024 - Unleashing HyDRa_ Hybrid Fusion, Depth Consistency and Radar for Unified 3D Perception.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2024-10-15T09:13:28},
  primaryclass     = {cs.CV},
}

@Article{Yan2023,
  author           = {Junjie Yan and Yingfei Liu and Jianjian Sun and Fan Jia and Shuailin Li and Tiancai Wang and Xiangyu Zhang},
  title            = {Cross Modal Transformer: Towards Fast and Robust 3D Object Detection},
  year             = {2023},
  month            = jan,
  abstract         = {In this paper, we propose a robust 3D detector, named Cross Modal Transformer (CMT), for end-to-end 3D multi-modal detection. Without explicit view transformation, CMT takes the image and point clouds tokens as inputs and directly outputs accurate 3D bounding boxes. The spatial alignment of multi-modal tokens is performed by encoding the 3D points into multi-modal features. The core design of CMT is quite simple while its performance is impressive. It achieves 74.1\% NDS (state-of-the-art with single model) on nuScenes test set while maintaining fast inference speed. Moreover, CMT has a strong robustness even if the LiDAR is missing. Code is released at https://github.com/junjie18/CMT.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-11-01T09:18:42},
  eprint           = {2301.01283},
  file             = {:Yan2023 - Cross Modal Transformer_ Towards Fast and Robust 3D Object Detection.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2024-11-01T09:18:45},
  primaryclass     = {cs.CV},
}

@Article{Yu2023,
  author           = {En Yu and Tiancai Wang and Zhuoling Li and Yuang Zhang and Xiangyu Zhang and Wenbing Tao},
  title            = {MOTRv3: Release-Fetch Supervision for End-to-End Multi-Object Tracking},
  year             = {2023},
  month            = may,
  abstract         = {Although end-to-end multi-object trackers like MOTR enjoy the merits of simplicity, they suffer from the conflict between detection and association seriously, resulting in unsatisfactory convergence dynamics. While MOTRv2 partly addresses this problem, it demands an additional detection network for assistance. In this work, we serve as the first to reveal that this conflict arises from the unfair label assignment between detect queries and track queries during training, where these detect queries recognize targets and track queries associate them. Based on this observation, we propose MOTRv3, which balances the label assignment process using the developed release-fetch supervision strategy. In this strategy, labels are first released for detection and gradually fetched back for association. Besides, another two strategies named pseudo label distillation and track group denoising are designed to further improve the supervision for detection and association. Without the assistance of an extra detection network during inference, MOTRv3 achieves impressive performance across diverse benchmarks, e.g., MOT17, DanceTrack.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T13:20:36},
  eprint           = {2305.14298},
  file             = {:Yu2023 - MOTRv3_ Release Fetch Supervision for End to End Multi Object Tracking.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2024-10-15T13:20:36},
  primaryclass     = {cs.CV},
}

@Article{Yu2024,
  author           = {Zichen Yu and Quanli Liu and Wei Wang and Liyong Zhang and Xiaoguang Zhao},
  title            = {PolarBEVDet: Exploring Polar Representation for Multi-View 3D Object Detection in Bird's-Eye-View},
  year             = {2024},
  month            = aug,
  abstract         = {Recently, LSS-based multi-view 3D object detection provides an economical and deployment-friendly solution for autonomous driving. However, all the existing LSS-based methods transform multi-view image features into a Cartesian Bird's-Eye-View(BEV) representation, which does not take into account the non-uniform image information distribution and hardly exploits the view symmetry. In this paper, in order to adapt the image information distribution and preserve the view symmetry by regular convolution, we propose to employ the polar BEV representation to substitute the Cartesian BEV representation. To achieve this, we elaborately tailor three modules: a polar view transformer to generate the polar BEV representation, a polar temporal fusion module for fusing historical polar BEV features and a polar detection head to predict the polar-parameterized representation of the object. In addition, we design a 2D auxiliary detection head and a spatial attention enhancement module to improve the quality of feature extraction in perspective view and BEV, respectively. Finally, we integrate the above improvements into a novel multi-view 3D object detector, PolarBEVDet. Experiments on nuScenes show that PolarBEVDet achieves the superior performance. The code is available at https://github.com/Yzichen/PolarBEVDet.git.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:15:14},
  eprint           = {2408.16200},
  file             = {:Yu2024 - PolarBEVDet_ Exploring Polar Representation for Multi View 3D Object Detection in Bird's Eye View.pdf:PDF},
  keywords         = {cs.CV, cs.AI},
  modificationdate = {2024-10-21T14:29:38},
  primaryclass     = {cs.CV},
  priority         = {prio1},
}

@Article{Zeng2021,
  author           = {Fangao Zeng and Bin Dong and Yuang Zhang and Tiancai Wang and Xiangyu Zhang and Yichen Wei},
  title            = {MOTR: End-to-End Multiple-Object Tracking with Transformer},
  year             = {2021},
  month            = may,
  abstract         = {Temporal modeling of objects is a key challenge in multiple object tracking (MOT). Existing methods track by associating detections through motion-based and appearance-based similarity heuristics. The post-processing nature of association prevents end-to-end exploitation of temporal variations in video sequence. In this paper, we propose MOTR, which extends DETR and introduces track query to model the tracked instances in the entire video. Track query is transferred and updated frame-by-frame to perform iterative prediction over time. We propose tracklet-aware label assignment to train track queries and newborn object queries. We further propose temporal aggregation network and collective average loss to enhance temporal relation modeling. Experimental results on DanceTrack show that MOTR significantly outperforms state-of-the-art method, ByteTrack by 6.5% on HOTA metric. On MOT17, MOTR outperforms our concurrent works, TrackFormer and TransTrack, on association performance. MOTR can serve as a stronger baseline for future research on temporal modeling and Transformer-based trackers. Code is available at https://github.com/megvii-research/MOTR.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T13:20:36},
  eprint           = {2105.03247},
  file             = {:http\://arxiv.org/pdf/2105.03247v4:PDF},
  keywords         = {cs.CV},
  modificationdate = {2024-10-15T13:20:36},
  primaryclass     = {cs.CV},
}

@Article{Zhang2022,
  author           = {Yuang Zhang and Tiancai Wang and Xiangyu Zhang},
  title            = {MOTRv2: Bootstrapping End-to-End Multi-Object Tracking by Pretrained Object Detectors},
  year             = {2022},
  month            = nov,
  abstract         = {In this paper, we propose MOTRv2, a simple yet effective pipeline to bootstrap end-to-end multi-object tracking with a pretrained object detector. Existing end-to-end methods, MOTR and TrackFormer are inferior to their tracking-by-detection counterparts mainly due to their poor detection performance. We aim to improve MOTR by elegantly incorporating an extra object detector. We first adopt the anchor formulation of queries and then use an extra object detector to generate proposals as anchors, providing detection prior to MOTR. The simple modification greatly eases the conflict between joint learning detection and association tasks in MOTR. MOTRv2 keeps the query propogation feature and scales well on large-scale benchmarks. MOTRv2 ranks the 1st place (73.4% HOTA on DanceTrack) in the 1st Multiple People Tracking in Group Dance Challenge. Moreover, MOTRv2 reaches state-of-the-art performance on the BDD100K dataset. We hope this simple and effective pipeline can provide some new insights to the end-to-end MOT community. Code is available at \url{https://github.com/megvii-research/MOTRv2}.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T13:20:36},
  eprint           = {2211.09791},
  file             = {:- MOTRv2_ Bootstrapping End to End Multi Object Tracking by Pretrained Object Detectors.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2024-10-15T13:20:37},
  primaryclass     = {cs.CV},
}

@Article{Zhang2024,
  author           = {Diankun Zhang and Guoan Wang and Runwen Zhu and Jianbo Zhao and Xiwu Chen and Siyu Zhang and Jiahao Gong and Qibin Zhou and Wenyuan Zhang and Ningzi Wang and Feiyang Tan and Hangning Zhou and Ziyao Xu and Haotian Yao and Chi Zhang and Xiaojun Liu and Xiaoguang Di and Bin Li},
  title            = {SparseAD: Sparse Query-Centric Paradigm for Efficient End-to-End Autonomous Driving},
  year             = {2024},
  month            = apr,
  abstract         = {End-to-End paradigms use a unified framework to implement multi-tasks in an autonomous driving system. Despite simplicity and clarity, the performance of end-to-end autonomous driving methods on sub-tasks is still far behind the single-task methods. Meanwhile, the widely used dense BEV features in previous end-to-end methods make it costly to extend to more modalities or tasks. In this paper, we propose a Sparse query-centric paradigm for end-to-end Autonomous Driving (SparseAD), where the sparse queries completely represent the whole driving scenario across space, time and tasks without any dense BEV representation. Concretely, we design a unified sparse architecture for perception tasks including detection, tracking, and online mapping. Moreover, we revisit motion prediction and planning, and devise a more justifiable motion planner framework. On the challenging nuScenes dataset, SparseAD achieves SOTA full-task performance among end-to-end methods and significantly narrows the performance gap between end-to-end paradigms and single-task methods. Codes will be released soon.},
  archiveprefix    = {arXiv},
  comment          = {end2end MB},
  eprint           = {2404.06892},
  file             = {:Zhang2024 - SparseAD_ Sparse Query Centric Paradigm for Efficient End to End Autonomous Driving.pdf:PDF},
  groups           = {Reading Group Sparse Query},
  keywords         = {cs.CV},
  modificationdate = {2024-10-16T13:27:31},
  primaryclass     = {cs.CV},
}

@Article{Zhao2023,
  author           = {Yian Zhao and Wenyu Lv and Shangliang Xu and Jinman Wei and Guanzhong Wang and Qingqing Dang and Yi Liu and Jie Chen},
  title            = {DETRs Beat YOLOs on Real-time Object Detection},
  year             = {2023},
  month            = apr,
  abstract         = {The YOLO series has become the most popular framework for real-time object detection due to its reasonable trade-off between speed and accuracy. However, we observe that the speed and accuracy of YOLOs are negatively affected by the NMS. Recently, end-to-end Transformer-based detectors (DETRs) have provided an alternative to eliminating NMS. Nevertheless, the high computational cost limits their practicality and hinders them from fully exploiting the advantage of excluding NMS. In this paper, we propose the Real-Time DEtection TRansformer (RT-DETR), the first real-time end-to-end object detector to our best knowledge that addresses the above dilemma. We build RT-DETR in two steps, drawing on the advanced DETR: first we focus on maintaining accuracy while improving speed, followed by maintaining speed while improving accuracy. Specifically, we design an efficient hybrid encoder to expeditiously process multi-scale features by decoupling intra-scale interaction and cross-scale fusion to improve speed. Then, we propose the uncertainty-minimal query selection to provide high-quality initial queries to the decoder, thereby improving accuracy. In addition, RT-DETR supports flexible speed tuning by adjusting the number of decoder layers to adapt to various scenarios without retraining. Our RT-DETR-R50 / R101 achieves 53.1% / 54.3% AP on COCO and 108 / 74 FPS on T4 GPU, outperforming previously advanced YOLOs in both speed and accuracy. We also develop scaled RT-DETRs that outperform the lighter YOLO detectors (S and M models). Furthermore, RT-DETR-R50 outperforms DINO-R50 by 2.2% AP in accuracy and about 21 times in FPS. After pre-training with Objects365, RT-DETR-R50 / R101 achieves 55.3% / 56.2% AP. The project page: https://zhao-yian.github.io/RTDETR.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:09:28},
  eprint           = {2304.08069},
  file             = {:Zhao2023 - DETRs Beat YOLOs on Real Time Object Detection.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2024-10-15T09:09:31},
  primaryclass     = {cs.CV},
}

@Article{Zheng2024,
  author           = {Wenzhao Zheng and Ruiqi Song and Xianda Guo and Chenming Zhang and Long Chen},
  title            = {GenAD: Generative End-to-End Autonomous Driving},
  year             = {2024},
  month            = feb,
  abstract         = {Directly producing planning results from raw sensors has been a long-desired solution for autonomous driving and has attracted increasing attention recently. Most existing end-to-end autonomous driving methods factorize this problem into perception, motion prediction, and planning. However, we argue that the conventional progressive pipeline still cannot comprehensively model the entire traffic evolution process, e.g., the future interaction between the ego car and other traffic participants and the structural trajectory prior. In this paper, we explore a new paradigm for end-to-end autonomous driving, where the key is to predict how the ego car and the surroundings evolve given past scenes. We propose GenAD, a generative framework that casts autonomous driving into a generative modeling problem. We propose an instance-centric scene tokenizer that first transforms the surrounding scenes into map-aware instance tokens. We then employ a variational autoencoder to learn the future trajectory distribution in a structural latent space for trajectory prior modeling. We further adopt a temporal model to capture the agent and ego movements in the latent space to generate more effective future trajectories. GenAD finally simultaneously performs motion prediction and planning by sampling distributions in the learned structural latent space conditioned on the instance tokens and using the learned temporal model to generate futures. Extensive experiments on the widely used nuScenes benchmark show that the proposed GenAD achieves state-of-the-art performance on vision-centric end-to-end autonomous driving with high efficiency. Code: https://github.com/wzzheng/GenAD.},
  archiveprefix    = {arXiv},
  eprint           = {2402.11502},
  file             = {:Zheng2024 - GenAD_ Generative End to End Autonomous Driving.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2024-10-15T09:03:34},
  primaryclass     = {cs.CV},
}

@Article{Zhu2020,
  author           = {Xizhou Zhu and Weijie Su and Lewei Lu and Bin Li and Xiaogang Wang and Jifeng Dai},
  title            = {Deformable DETR: Deformable Transformers for End-to-End Object Detection},
  year             = {2020},
  month            = oct,
  abstract         = {DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10 times less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code is released at https://github.com/fundamentalvision/Deformable-DETR.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-21T16:46:37},
  eprint           = {2010.04159},
  file             = {:Zhu2020 - Deformable DETR_ Deformable Transformers for End to End Object Detection.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-01-22T16:45:07},
  primaryclass     = {cs.CV},
  priority         = {prio1},
  readstatus       = {read},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Reading Group Sparse Query\;0\;1\;0x00ffffff\;\;\;;
1 StaticGroup:By Status\;0\;1\;0x8a8a8aff\;\;\;;
2 SearchGroup:read\;0\;readstatus=read\;0\;0\;1\;0x00ff00ff\;READ\;\;;
2 SearchGroup:to be read\;0\;not readstatus=read and not readstatus=skimmed\;0\;0\;1\;0xff0000ff\;EYE\;\;;
2 SearchGroup:skimmed\;0\;readstatus=skimmed\;0\;0\;0\;0xffff4dff\;RUN_FAST\;\;;
}

@Comment{jabref-meta: saveOrderConfig:specified;citationkey;false;author;false;title;true;}

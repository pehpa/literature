@InProceedings{10.5555/2999134.2999257,
  author           = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  booktitle        = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1},
  title            = {ImageNet classification with deep convolutional neural networks},
  year             = {2012},
  address          = {Red Hook, NY, USA},
  pages            = {1097â€“1105},
  publisher        = {Curran Associates Inc.},
  series           = {NIPS'12},
  abstract         = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overriding in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  comment          = {AlexNet},
  creationdate     = {2024-11-12T12:58:08},
  location         = {Lake Tahoe, Nevada},
  modificationdate = {2025-02-28T12:19:40},
  numpages         = {9},
  priority         = {prio1},
}

@InProceedings{10656117,
  author           = {X. Weng and B. Ivanovic and Y. Wang and Y. Wang and M. Pavone},
  booktitle        = {2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title            = {PARA-Drive: Parallelized Architecture for Real-Time Autonomous Driving},
  year             = {2024},
  address          = {Los Alamitos, CA, USA},
  month            = {jun},
  pages            = {15449-15458},
  publisher        = {IEEE Computer Society},
  abstract         = {Recent works have proposed end-to-end autonomous vehicle (AV) architectures comprised of differentiable modules, achieving state-of-the-art driving performance. While they provide advantages over the traditional perception-prediction-planning pipeline (e.g., removing information bottlenecks between components and alleviating integration challenges), they do so using a diverse combination of tasks, modules, and their interconnectivity. As of yet, however, there has been no systematic analysis of the necessity of these modules or the impact of their connectivity, placement, and internal representations on overall driving performance. Addressing this gap, our work conducts a comprehensive exploration of the design space of end-to-end modular AV stacks. Our findings culminate in the development of PARA-Drivel: a fully parallel end-to-end AV architecture. PARA-Drive not only achieves state-of-the-art performance in perception, prediction, and planning, but also significantly enhances runtime speed by nearly 3 x, without compromising on interpretability or safety.},
  doi              = {10.1109/CVPR52733.2024.01463},
  file             = {:10656117 - PARA Drive_ Parallelized Architecture for Real Time Autonomous Driving.pdf:PDF},
  keywords         = {computer vision;systematics;runtime;pipelines;computer architecture;real-time systems;safety},
  modificationdate = {2024-10-15T09:17:56},
}

@Article{6795724,
  author           = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
  journal          = {Neural Computation},
  title            = {Backpropagation Applied to Handwritten Zip Code Recognition},
  year             = {1989},
  number           = {4},
  pages            = {541-551},
  volume           = {1},
  creationdate     = {2024-11-12T12:56:28},
  doi              = {10.1162/neco.1989.1.4.541},
  modificationdate = {2024-11-12T12:56:28},
}

@Article{726791,
  author           = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  journal          = {Proceedings of the IEEE},
  title            = {Gradient-based learning applied to document recognition},
  year             = {1998},
  number           = {11},
  pages            = {2278-2324},
  volume           = {86},
  comment          = {LeNet5},
  doi              = {10.1109/5.726791},
  keywords         = {Neural networks;Pattern recognition;Machine learning;Optical character recognition software;Character recognition;Feature extraction;Multi-layer neural network;Optical computing;Hidden Markov models;Principal component analysis},
  modificationdate = {2025-02-28T12:19:42},
  priority         = {prio1},
}

@Article{article,
  author           = {Sarker, Iqbal},
  journal          = {SN Computer Science},
  title            = {Deep Learning: A Comprehensive Overview on Techniques, Taxonomy, Applications and Research Directions},
  year             = {2021},
  month            = {08},
  volume           = {2},
  creationdate     = {2024-09-16T09:32:30},
  doi              = {10.1007/s42979-021-00815-1},
  file             = {:article - Deep Learning_ a Comprehensive Overview on Techniques, Taxonomy, Applications and Research Directions.pdf:PDF},
  modificationdate = {2024-10-15T09:03:24},
  readstatus       = {read},
}

@Article{Carion2020,
  author           = {Nicolas Carion and Francisco Massa and Gabriel Synnaeve and Nicolas Usunier and Alexander Kirillov and Sergey Zagoruyko},
  title            = {End-to-End Object Detection with Transformers},
  year             = {2020},
  month            = may,
  abstract         = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
  archiveprefix    = {arXiv},
  comment          = {DETR},
  doi              = {10.48550/arxiv.2005.12872},
  eprint           = {2005.12872},
  file             = {:Carion2020 - End to End Object Detection with Transformers.pdf:PDF},
  groups           = {PMO AI},
  keywords         = {cs.CV},
  modificationdate = {2025-04-02T13:24:07},
  primaryclass     = {cs.CV},
  priority         = {prio1},
  readstatus       = {read},
}

@Article{Chen2022,
  author           = {Xuanyao Chen and Tianyuan Zhang and Yue Wang and Yilun Wang and Hang Zhao},
  journal          = {CVPR 2023 workshop on autonomous driving},
  title            = {FUTR3D: A Unified Sensor Fusion Framework for 3D Detection},
  year             = {2022},
  month            = mar,
  abstract         = {Sensor fusion is an essential topic in many perception systems, such as autonomous driving and robotics. Existing multi-modal 3D detection models usually involve customized designs depending on the sensor combinations or setups. In this work, we propose the first unified end-to-end sensor fusion framework for 3D detection, named FUTR3D, which can be used in (almost) any sensor configuration. FUTR3D employs a query-based Modality-Agnostic Feature Sampler (MAFS), together with a transformer decoder with a set-to-set loss for 3D detection, thus avoiding using late fusion heuristics and post-processing tricks. We validate the effectiveness of our framework on various combinations of cameras, low-resolution LiDARs, high-resolution LiDARs, and Radars. On NuScenes dataset, FUTR3D achieves better performance over specifically designed methods across different sensor combinations. Moreover, FUTR3D achieves great flexibility with different sensor configurations and enables low-cost autonomous driving. For example, only using a 4-beam LiDAR with cameras, FUTR3D (58.0 mAP) achieves on par performance with state-of-the-art 3D detection model CenterPoint (56.6 mAP) using a 32-beam LiDAR.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:05:17},
  doi              = {10.48550/arxiv.2203.10642},
  eprint           = {2203.10642},
  file             = {:Chen2022 - FUTR3D_ a Unified Sensor Fusion Framework for 3D Detection.pdf:PDF},
  groups           = {Reading Group Sparse Query},
  keywords         = {cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{Chen2022a,
  author           = {Shaoyu Chen and Tianheng Cheng and Xinggang Wang and Wenming Meng and Qian Zhang and Wenyu Liu},
  title            = {Efficient and Robust 2D-to-BEV Representation Learning via Geometry-guided Kernel Transformer},
  year             = {2022},
  month            = jun,
  abstract         = {Learning Bird's Eye View (BEV) representation from surrounding-view cameras is of great importance for autonomous driving. In this work, we propose a Geometry-guided Kernel Transformer (GKT), a novel 2D-to-BEV representation learning mechanism. GKT leverages the geometric priors to guide the transformer to focus on discriminative regions and unfolds kernel features to generate BEV representation. For fast inference, we further introduce a look-up table (LUT) indexing method to get rid of the camera's calibrated parameters at runtime. GKT can run at $72.3$ FPS on 3090 GPU / $45.6$ FPS on 2080ti GPU and is robust to the camera deviation and the predefined BEV height. And GKT achieves the state-of-the-art real-time segmentation results, i.e., 38.0 mIoU (100m$\times$100m perception range at a 0.5m resolution) on the nuScenes val set. Given the efficiency, effectiveness, and robustness, GKT has great practical values in autopilot scenarios, especially for real-time running systems. Code and models will be available at \url{https://github.com/hustvl/GKT}.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:07:17},
  doi              = {10.48550/arxiv.2206.04584},
  eprint           = {2206.04584},
  file             = {:Chen2022a - Efficient and Robust 2D to BEV Representation Learning Via Geometry Guided Kernel Transformer.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{Chen2023,
  author           = {Long Chen and Oleg Sinavski and Jan HÃ¼nermann and Alice Karnsund and Andrew James Willmott and Danny Birch and Daniel Maund and Jamie Shotton},
  title            = {Driving with LLMs: Fusing Object-Level Vector Modality for Explainable Autonomous Driving},
  year             = {2023},
  month            = oct,
  abstract         = {Large Language Models (LLMs) have shown promise in the autonomous driving sector, particularly in generalization and interpretability. We introduce a unique object-level multimodal LLM architecture that merges vectorized numeric modalities with a pre-trained LLM to improve context understanding in driving situations. We also present a new dataset of 160k QA pairs derived from 10k driving scenarios, paired with high quality control commands collected with RL agent and question answer pairs generated by teacher LLM (GPT-3.5). A distinct pretraining strategy is devised to align numeric vector modalities with static LLM representations using vector captioning language data. We also introduce an evaluation metric for Driving QA and demonstrate our LLM-driver's proficiency in interpreting driving scenarios, answering questions, and decision-making. Our findings highlight the potential of LLM-based driving action generation in comparison to traditional behavioral cloning. We make our benchmark, datasets, and model available for further exploration.},
  archiveprefix    = {arXiv},
  doi              = {10.48550/arxiv.2310.01957},
  eprint           = {2310.01957},
  file             = {:Chen2023 - Driving with LLMs_ Fusing Object Level Vector Modality for Explainable Autonomous Driving.pdf:PDF},
  keywords         = {cs.RO, cs.AI, cs.CL, cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.RO},
}

@Article{Chen2023a,
  author           = {Li Chen and Penghao Wu and Kashyap Chitta and Bernhard Jaeger and Andreas Geiger and Hongyang Li},
  title            = {End-to-end Autonomous Driving: Challenges and Frontiers},
  year             = {2023},
  month            = jun,
  abstract         = {The autonomous driving community has witnessed a rapid growth in approaches that embrace an end-to-end algorithm framework, utilizing raw sensor input to generate vehicle motion plans, instead of concentrating on individual tasks such as detection and motion prediction. End-to-end systems, in comparison to modular pipelines, benefit from joint feature optimization for perception and planning. This field has flourished due to the availability of large-scale datasets, closed-loop evaluation, and the increasing need for autonomous driving algorithms to perform effectively in challenging scenarios. In this survey, we provide a comprehensive analysis of more than 270 papers, covering the motivation, roadmap, methodology, challenges, and future trends in end-to-end autonomous driving. We delve into several critical challenges, including multi-modality, interpretability, causal confusion, robustness, and world models, amongst others. Additionally, we discuss current advancements in foundation models and visual pre-training, as well as how to incorporate these techniques within the end-to-end driving framework. we maintain an active repository that contains up-to-date literature and open-source projects at https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:10:40},
  doi              = {10.48550/arxiv.2306.16927},
  eprint           = {2306.16927},
  file             = {:Chen2023a - End to End Autonomous Driving_ Challenges and Frontiers.pdf:PDF},
  keywords         = {cs.RO, cs.AI, cs.CV, cs.LG},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.RO},
}

@Article{Chen2024,
  author           = {Shaoyu Chen and Bo Jiang and Hao Gao and Bencheng Liao and Qing Xu and Qian Zhang and Chang Huang and Wenyu Liu and Xinggang Wang},
  title            = {VADv2: End-to-End Vectorized Autonomous Driving via Probabilistic Planning},
  year             = {2024},
  month            = feb,
  abstract         = {Learning a human-like driving policy from large-scale driving demonstrations is promising, but the uncertainty and non-deterministic nature of planning make it challenging. In this work, to cope with the uncertainty problem, we propose VADv2, an end-to-end driving model based on probabilistic planning. VADv2 takes multi-view image sequences as input in a streaming manner, transforms sensor data into environmental token embeddings, outputs the probabilistic distribution of action, and samples one action to control the vehicle. Only with camera sensors, VADv2 achieves state-of-the-art closed-loop performance on the CARLA Town05 benchmark, significantly outperforming all existing methods. It runs stably in a fully end-to-end manner, even without the rule-based wrapper. Closed-loop demos are presented at https://hgao-cv.github.io/VADv2.},
  archiveprefix    = {arXiv},
  doi              = {10.48550/arxiv.2402.13243},
  eprint           = {2402.13243},
  file             = {:Chen2024 - VADv2_ End to End Vectorized Autonomous Driving Via Probabilistic Planning.pdf:PDF},
  groups           = {Reading Group Sparse Query},
  keywords         = {cs.CV, cs.RO},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{Chu2024,
  author           = {Xiaomeng Chu and Jiajun Deng and Guoliang You and Yifan Duan and Yao Li and Yanyong Zhang},
  title            = {RayFormer: Improving Query-Based Multi-Camera 3D Object Detection via Ray-Centric Strategies},
  year             = {2024},
  month            = jul,
  abstract         = {The recent advances in query-based multi-camera 3D object detection are featured by initializing object queries in the 3D space, and then sampling features from perspective-view images to perform multi-round query refinement. In such a framework, query points near the same camera ray are likely to sample similar features from very close pixels, resulting in ambiguous query features and degraded detection accuracy. To this end, we introduce RayFormer, a camera-ray-inspired query-based 3D object detector that aligns the initialization and feature extraction of object queries with the optical characteristics of cameras. Specifically, RayFormer transforms perspective-view image features into bird's eye view (BEV) via the lift-splat-shoot method and segments the BEV map to sectors based on the camera rays. Object queries are uniformly and sparsely initialized along each camera ray, facilitating the projection of different queries onto different areas in the image to extract distinct features. Besides, we leverage the instance information of images to supplement the uniformly initialized object queries by further involving additional queries along the ray from 2D object detection boxes. To extract unique object-level features that cater to distinct queries, we design a ray sampling method that suitably organizes the distribution of feature sampling points on both images and bird's eye view. Extensive experiments are conducted on the nuScenes dataset to validate our proposed ray-inspired model design. The proposed RayFormer achieves 55.5% mAP and 63.3% NDS, respectively. Our codes will be made available.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:16:21},
  doi              = {10.48550/arxiv.2407.14923},
  eprint           = {2407.14923},
  file             = {:Chu2024 - RayFormer_ Improving Query Based Multi Camera 3D Object Detection Via Ray Centric Strategies.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{DeepSeekAI2025,
  author           = {{DeepSeek-AI} and Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and Zhang, Xiaokang and Yu, Xingkai and Wu, Yu and Wu, Z. F. and Gou, Zhibin and Shao, Zhihong and Li, Zhuoshu and Gao, Ziyi and Liu, Aixin and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Feng, Bei and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and Dai, Damai and Chen, Deli and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Dai, Fucong and Luo, Fuli and Hao, Guangbo and Chen, Guanting and Li, Guowei and Zhang, H. and Bao, Han and Xu, Hanwei and Wang, Haocheng and Ding, Honghui and Xin, Huajian and Gao, Huazuo and Qu, Hui and Li, Hui and Guo, Jianzhong and Li, Jiashi and Wang, Jiawei and Chen, Jingchang and Yuan, Jingyang and Qiu, Junjie and Li, Junlong and Cai, J. L. and Ni, Jiaqi and Liang, Jian and Chen, Jin and Dong, Kai and Hu, Kai and Gao, Kaige and Guan, Kang and Huang, Kexin and Yu, Kuai and Wang, Lean and Zhang, Lecong and Zhao, Liang and Wang, Litong and Zhang, Liyue and Xu, Lei and Xia, Leyi and Zhang, Mingchuan and Zhang, Minghua and Tang, Minghui and Li, Meng and Wang, Miaojun and Li, Mingming and Tian, Ning and Huang, Panpan and Zhang, Peng and Wang, Qiancheng and Chen, Qinyu and Du, Qiushi and Ge, Ruiqi and Zhang, Ruisong and Pan, Ruizhe and Wang, Runji and Chen, R. J. and Jin, R. L. and Chen, Ruyi and Lu, Shanghao and Zhou, Shangyan and Chen, Shanhuang and Ye, Shengfeng and Wang, Shiyu and Yu, Shuiping and Zhou, Shunfeng and Pan, Shuting and Li, S. S. and Zhou, Shuang and Wu, Shaoqing and Ye, Shengfeng and Yun, Tao and Pei, Tian and Sun, Tianyu and Wang, T. and Zeng, Wangding and Zhao, Wanjia and Liu, Wen and Liang, Wenfeng and Gao, Wenjun and Yu, Wenqin and Zhang, Wentao and Xiao, W. L. and An, Wei and Liu, Xiaodong and Wang, Xiaohan and Chen, Xiaokang and Nie, Xiaotao and Cheng, Xin and Liu, Xin and Xie, Xin and Liu, Xingchao and Yang, Xinyu and Li, Xinyuan and Su, Xuecheng and Lin, Xuheng and Li, X. Q. and Jin, Xiangyue and Shen, Xiaojin and Chen, Xiaosha and Sun, Xiaowen and Wang, Xiaoxiang and Song, Xinnan and Zhou, Xinyi and Wang, Xianzu and Shan, Xinxia and Li, Y. K. and Wang, Y. Q. and Wei, Y. X. and Zhang, Yang and Xu, Yanhong and Li, Yao and Zhao, Yao and Sun, Yaofeng and Wang, Yaohui and Yu, Yi and Zhang, Yichao and Shi, Yifan and Xiong, Yiliang and He, Ying and Piao, Yishi and Wang, Yisong and Tan, Yixuan and Ma, Yiyang and Liu, Yiyuan and Guo, Yongqiang and Ou, Yuan and Wang, Yuduan and Gong, Yue and Zou, Yuheng and He, Yujia and Xiong, Yunfan and Luo, Yuxiang and You, Yuxiang and Liu, Yuxuan and Zhou, Yuyang and Zhu, Y. X. and Xu, Yanhong and Huang, Yanping and Li, Yaohui and Zheng, Yi and Zhu, Yuchen and Ma, Yunxian and Tang, Ying and Zha, Yukun and Yan, Yuting and Ren, Z. Z. and Ren, Zehui and Sha, Zhangli and Fu, Zhe and Xu, Zhean and Xie, Zhenda and Zhang, Zhengyan and Hao, Zhewen and Ma, Zhicheng and Yan, Zhigang and Wu, Zhiyu and Gu, Zihui and Zhu, Zijia and Liu, Zijun and Li, Zilin and Xie, Ziwei and Song, Ziyang and Pan, Zizheng and Huang, Zhen and Xu, Zhipeng and Zhang, Zhongyu and Zhang, Zhen},
  title            = {DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},
  year             = {2025},
  month            = jan,
  abstract         = {We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.},
  archiveprefix    = {arXiv},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2025-01-28T20:01:28},
  doi              = {10.48550/ARXIV.2501.12948},
  eprint           = {2501.12948},
  file             = {:DeepSeekAI2025 - DeepSeek R1_ Incentivizing Reasoning Capability in LLMs Via Reinforcement Learning.pdf:PDF},
  keywords         = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences},
  modificationdate = {2025-01-28T22:54:26},
  primaryclass     = {cs.CL},
  publisher        = {arXiv},
}

@Article{Doll2024,
  author           = {Simon Doll and Niklas Hanselmann and Lukas Schneider and Richard Schulz and Marius Cordts and Markus Enzweiler and Hendrik P. A. Lensch},
  title            = {DualAD: Disentangling the Dynamic and Static World for End-to-End Driving},
  year             = {2024},
  month            = jun,
  abstract         = {State-of-the-art approaches for autonomous driving integrate multiple sub-tasks of the overall driving task into a single pipeline that can be trained in an end-to-end fashion by passing latent representations between the different modules. In contrast to previous approaches that rely on a unified grid to represent the belief state of the scene, we propose dedicated representations to disentangle dynamic agents and static scene elements. This allows us to explicitly compensate for the effect of both ego and object motion between consecutive time steps and to flexibly propagate the belief state through time. Furthermore, dynamic objects can not only attend to the input camera images, but also directly benefit from the inferred static scene structure via a novel dynamic-static cross-attention. Extensive experiments on the challenging nuScenes benchmark demonstrate the benefits of the proposed dual-stream design, especially for modelling highly dynamic agents in the scene, and highlight the improved temporal consistency of our approach. Our method titled DualAD not only outperforms independently trained single-task networks, but also improves over previous state-of-the-art end-to-end models by a large margin on all tasks along the functional chain of driving.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:14:22},
  doi              = {10.48550/arxiv.2406.06264},
  eprint           = {2406.06264},
  file             = {:Doll2024 - DualAD_ Disentangling the Dynamic and Static World for End to End Driving.pdf:PDF},
  groups           = {Reading Group Sparse Query},
  keywords         = {cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{Dosovitskiy2020,
  author           = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
  title            = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  year             = {2020},
  month            = oct,
  abstract         = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-09-30T15:50:31},
  doi              = {10.48550/arxiv.2010.11929},
  eprint           = {2010.11929},
  file             = {:Dosovitskiy2020 - An Image Is Worth 16x16 Words_ Transformers for Image Recognition at Scale.pdf:PDF},
  keywords         = {cs.CV, cs.AI, cs.LG},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
  priority         = {prio1},
  readstatus       = {read},
}

@Article{Drews2022,
  author           = {Florian Drews and Di Feng and Florian Faion and Lars Rosenbaum and Michael Ulrich and Claudius GlÃ¤ser},
  title            = {DeepFusion: A Robust and Modular 3D Object Detector for Lidars, Cameras and Radars},
  year             = {2022},
  month            = sep,
  abstract         = {We propose DeepFusion, a modular multi-modal architecture to fuse lidars, cameras and radars in different combinations for 3D object detection. Specialized feature extractors take advantage of each modality and can be exchanged easily, making the approach simple and flexible. Extracted features are transformed into bird's-eye-view as a common representation for fusion. Spatial and semantic alignment is performed prior to fusing modalities in the feature space. Finally, a detection head exploits rich multi-modal features for improved 3D detection performance. Experimental results for lidar-camera, lidar-camera-radar and camera-radar fusion show the flexibility and effectiveness of our fusion approach. In the process, we study the largely unexplored task of faraway car detection up to 225 meters, showing the benefits of our lidar-camera fusion. Furthermore, we investigate the required density of lidar points for 3D object detection and illustrate implications at the example of robustness against adverse weather conditions. Moreover, ablation studies on our camera-radar fusion highlight the importance of accurate depth estimation.},
  archiveprefix    = {arXiv},
  comment          = {Bosch paper},
  creationdate     = {2024-10-22T09:57:45},
  doi              = {10.48550/arxiv.2209.12729},
  eprint           = {2209.12729},
  file             = {:Drews2022 - DeepFusion_ a Robust and Modular 3D Object Detector for Lidars, Cameras and Radars.pdf:PDF},
  keywords         = {cs.CV, cs.AI, cs.LG, cs.RO},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
  priority         = {prio1},
}

@Article{Fujimoto2025,
  author           = {Scott Fujimoto and Pierluca D'Oro and Amy Zhang and Yuandong Tian and Michael Rabbat},
  title            = {Towards General-Purpose Model-Free Reinforcement Learning},
  year             = {2025},
  month            = jan,
  abstract         = {Reinforcement learning (RL) promises a framework for near-universal problem-solving. In practice however, RL algorithms are often tailored to specific benchmarks, relying on carefully tuned hyperparameters and algorithmic choices. Recently, powerful model-based RL methods have shown impressive general results across benchmarks but come at the cost of increased complexity and slow run times, limiting their broader applicability. In this paper, we attempt to find a unifying model-free deep RL algorithm that can address a diverse class of domains and problem settings. To achieve this, we leverage model-based representations that approximately linearize the value function, taking advantage of the denser task objectives used by model-based RL while avoiding the costs associated with planning or simulated trajectories. We evaluate our algorithm, MR.Q, on a variety of common RL benchmarks with a single set of hyperparameters and show a competitive performance against domain-specific and general baselines, providing a concrete step towards building general-purpose model-free deep RL algorithms.},
  archiveprefix    = {arXiv},
  creationdate     = {2025-01-28T13:55:56},
  doi              = {10.48550/arxiv.2501.16142},
  eprint           = {2501.16142},
  file             = {:Fujimoto2025 - Towards General Purpose Model Free Reinforcement Learning.pdf:PDF},
  keywords         = {cs.LG, cs.AI},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.LG},
}

@Article{Girshick2013,
  author           = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  title            = {Rich feature hierarchies for accurate object detection and semantic segmentation},
  year             = {2013},
  month            = nov,
  abstract         = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012---achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.},
  archiveprefix    = {arXiv},
  comment          = {R-CNN, region proposals},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2025-03-07T13:57:57},
  doi              = {10.48550/ARXIV.1311.2524},
  eprint           = {1311.2524},
  file             = {:Girshick2013 - Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation.pdf:PDF:http\://arxiv.org/pdf/1311.2524v5},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  modificationdate = {2025-03-07T14:14:38},
  primaryclass     = {cs.CV},
  priority         = {prio1},
  publisher        = {arXiv},
}

@Article{Girshick2015,
  author           = {Girshick, Ross},
  title            = {Fast R-CNN},
  year             = {2015},
  month            = apr,
  abstract         = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
  archiveprefix    = {arXiv},
  comment          = {Fast R-CNN},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2025-03-07T14:13:59},
  doi              = {10.48550/ARXIV.1504.08083},
  eprint           = {1504.08083},
  file             = {:Girshick2015 - Fast R CNN.pdf:PDF:http\://arxiv.org/pdf/1504.08083v2},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  modificationdate = {2025-03-07T14:14:30},
  primaryclass     = {cs.CV},
  priority         = {prio1},
  publisher        = {arXiv},
}

@Article{Harley2022,
  author           = {Adam W. Harley and Zhaoyuan Fang and Jie Li and Rares Ambrus and Katerina Fragkiadaki},
  title            = {Simple-BEV: What Really Matters for Multi-Sensor BEV Perception?},
  year             = {2022},
  month            = jun,
  abstract         = {Building 3D perception systems for autonomous vehicles that do not rely on high-density LiDAR is a critical research problem because of the expense of LiDAR systems compared to cameras and other sensors. Recent research has developed a variety of camera-only methods, where features are differentiably "lifted" from the multi-camera images onto the 2D ground plane, yielding a "bird's eye view" (BEV) feature representation of the 3D space around the vehicle. This line of work has produced a variety of novel "lifting" methods, but we observe that other details in the training setups have shifted at the same time, making it unclear what really matters in top-performing methods. We also observe that using cameras alone is not a real-world constraint, considering that additional sensors like radar have been integrated into real vehicles for years already. In this paper, we first of all attempt to elucidate the high-impact factors in the design and training protocol of BEV perception models. We find that batch size and input resolution greatly affect performance, while lifting strategies have a more modest effect -- even a simple parameter-free lifter works well. Second, we demonstrate that radar data can provide a substantial boost to performance, helping to close the gap between camera-only and LiDAR-enabled systems. We analyze the radar usage details that lead to good performance, and invite the community to re-consider this commonly-neglected part of the sensor platform.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-09-25T13:17:31},
  doi              = {10.48550/arxiv.2206.07959},
  eprint           = {2206.07959},
  file             = {:Harley2022 - Simple BEV_ What Really Matters for Multi Sensor BEV Perception_.pdf:PDF},
  groups           = {Reading Group Sparse Query},
  keywords         = {cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{Hatamizadeh2023,
  author           = {Ali Hatamizadeh and Greg Heinrich and Hongxu Yin and Andrew Tao and Jose M. Alvarez and Jan Kautz and Pavlo Molchanov},
  title            = {FasterViT: Fast Vision Transformers with Hierarchical Attention},
  year             = {2023},
  month            = jun,
  abstract         = {We design a new family of hybrid CNN-ViT neural networks, named FasterViT, with a focus on high image throughput for computer vision (CV) applications. FasterViT combines the benefits of fast local representation learning in CNNs and global modeling properties in ViT. Our newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs. We benefit from efficient window-based self-attention. Each window has access to dedicated carrier tokens that participate in local and global representation learning. At a high level, global self-attentions enable the efficient cross-window communication at lower costs. FasterViT achieves a SOTA Pareto-front in terms of accuracy and image throughput. We have extensively validated its effectiveness on various CV tasks including classification, object detection and segmentation. We also show that HAT can be used as a plug-and-play module for existing networks and enhance them. We further demonstrate significantly faster and more accurate performance than competitive counterparts for images with high resolution. Code is available at https://github.com/NVlabs/FasterViT.},
  archiveprefix    = {arXiv},
  comment          = {hierarchical attention},
  creationdate     = {2024-10-22T09:06:17},
  doi              = {10.48550/arxiv.2306.06189},
  eprint           = {2306.06189},
  file             = {:Hatamizadeh2023 - FasterViT_ Fast Vision Transformers with Hierarchical Attention.pdf:PDF},
  keywords         = {cs.CV, cs.AI, cs.LG},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{He2015,
  author           = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  title            = {Deep Residual Learning for Image Recognition},
  year             = {2015},
  month            = dec,
  abstract         = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archiveprefix    = {arXiv},
  comment          = {ResNet},
  creationdate     = {2024-11-12T12:58:52},
  doi              = {10.48550/arxiv.1512.03385},
  eprint           = {1512.03385},
  file             = {:He2015 - Deep Residual Learning for Image Recognition.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-02-28T13:22:41},
  primaryclass     = {cs.CV},
  priority         = {prio1},
}

@Article{He2021,
  author           = {Lu He and Qianyu Zhou and Xiangtai Li and Li Niu and Guangliang Cheng and Xiao Li and Wenxuan Liu and Yunhai Tong and Lizhuang Ma and Liqing Zhang},
  title            = {End-to-End Video Object Detection with Spatial-Temporal Transformers},
  year             = {2021},
  month            = may,
  abstract         = {Recently, DETR and Deformable DETR have been proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance as previous complex hand-crafted detectors. However, their performance on Video Object Detection (VOD) has not been well explored. In this paper, we present TransVOD, an end-to-end video object detection model based on a spatial-temporal Transformer architecture. The goal of this paper is to streamline the pipeline of VOD, effectively removing the need for many hand-crafted components for feature aggregation, e.g., optical flow, recurrent neural networks, relation networks. Besides, benefited from the object query design in DETR, our method does not need complicated post-processing methods such as Seq-NMS or Tubelet rescoring, which keeps the pipeline simple and clean. In particular, we present temporal Transformer to aggregate both the spatial object queries and the feature memories of each frame. Our temporal Transformer consists of three components: Temporal Deformable Transformer Encoder (TDTE) to encode the multiple frame spatial details, Temporal Query Encoder (TQE) to fuse object queries, and Temporal Deformable Transformer Decoder to obtain current frame detection results. These designs boost the strong baseline deformable DETR by a significant margin (3%-4% mAP) on the ImageNet VID dataset. TransVOD yields comparable results performance on the benchmark of ImageNet VID. We hope our TransVOD can provide a new perspective for video object detection. Code will be made publicly available at https://github.com/SJTU-LuHe/TransVOD.},
  archiveprefix    = {arXiv},
  comment          = {DETR},
  doi              = {10.48550/arxiv.2105.10920},
  eprint           = {2105.10920},
  file             = {:He2021 - End to End Video Object Detection with Spatial Temporal Transformers.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
  priority         = {prio1},
}

@Article{Howard2017,
  author           = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  title            = {MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications},
  year             = {2017},
  month            = apr,
  abstract         = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
  archiveprefix    = {arXiv},
  comment          = {MobileNet},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2025-03-02T20:03:19},
  doi              = {10.48550/ARXIV.1704.04861},
  eprint           = {1704.04861},
  file             = {:Howard2017 - MobileNets_ Efficient Convolutional Neural Networks for Mobile Vision Applications.pdf:PDF:http\://arxiv.org/pdf/1704.04861v1},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  modificationdate = {2025-03-02T20:28:41},
  primaryclass     = {cs.CV},
  priority         = {prio1},
  publisher        = {arXiv},
}

@Article{Hu2022,
  author           = {Yihan Hu and Jiazhi Yang and Li Chen and Keyu Li and Chonghao Sima and Xizhou Zhu and Siqi Chai and Senyao Du and Tianwei Lin and Wenhai Wang and Lewei Lu and Xiaosong Jia and Qiang Liu and Jifeng Dai and Yu Qiao and Hongyang Li},
  title            = {Planning-oriented Autonomous Driving},
  year             = {2022},
  month            = dec,
  abstract         = {Modern autonomous driving system is characterized as modular tasks in sequential order, i.e., perception, prediction, and planning. In order to perform a wide diversity of tasks and achieve advanced-level intelligence, contemporary approaches either deploy standalone models for individual tasks, or design a multi-task paradigm with separate heads. However, they might suffer from accumulative errors or deficient task coordination. Instead, we argue that a favorable framework should be devised and optimized in pursuit of the ultimate goal, i.e., planning of the self-driving car. Oriented at this, we revisit the key components within perception and prediction, and prioritize the tasks such that all these tasks contribute to planning. We introduce Unified Autonomous Driving (UniAD), a comprehensive framework up-to-date that incorporates full-stack driving tasks in one network. It is exquisitely devised to leverage advantages of each module, and provide complementary feature abstractions for agent interaction from a global perspective. Tasks are communicated with unified query interfaces to facilitate each other toward planning. We instantiate UniAD on the challenging nuScenes benchmark. With extensive ablations, the effectiveness of using such a philosophy is proven by substantially outperforming previous state-of-the-arts in all aspects. Code and models are public.},
  archiveprefix    = {arXiv},
  comment          = {end2end MB, uniAD},
  doi              = {10.48550/arxiv.2212.10156},
  eprint           = {2212.10156},
  file             = {:Hu2022 - Planning Oriented Autonomous Driving.pdf:PDF},
  groups           = {Reading Group Sparse Query},
  keywords         = {cs.CV, cs.RO},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{Huang2021,
  author           = {Junjie Huang and Guan Huang and Zheng Zhu and Yun Ye and Dalong Du},
  title            = {BEVDet: High-performance Multi-camera 3D Object Detection in Bird-Eye-View},
  year             = {2021},
  month            = dec,
  abstract         = {Autonomous driving perceives its surroundings for decision making, which is one of the most complex scenarios in visual perception. The success of paradigm innovation in solving the 2D object detection task inspires us to seek an elegant, feasible, and scalable paradigm for fundamentally pushing the performance boundary in this area. To this end, we contribute the BEVDet paradigm in this paper. BEVDet performs 3D object detection in Bird-Eye-View (BEV), where most target values are defined and route planning can be handily performed. We merely reuse existing modules to build its framework but substantially develop its performance by constructing an exclusive data augmentation strategy and upgrading the Non-Maximum Suppression strategy. In the experiment, BEVDet offers an excellent trade-off between accuracy and time-efficiency. As a fast version, BEVDet-Tiny scores 31.2% mAP and 39.2% NDS on the nuScenes val set. It is comparable with FCOS3D, but requires just 11% computational budget of 215.3 GFLOPs and runs 9.2 times faster at 15.6 FPS. Another high-precision version dubbed BEVDet-Base scores 39.3% mAP and 47.2% NDS, significantly exceeding all published results. With a comparable inference speed, it surpasses FCOS3D by a large margin of +9.8% mAP and +10.0% NDS. The source code is publicly available for further research at https://github.com/HuangJunJie2017/BEVDet .},
  archiveprefix    = {arXiv},
  creationdate     = {2024-09-30T15:53:29},
  doi              = {10.48550/arxiv.2112.11790},
  eprint           = {2112.11790},
  file             = {:Huang2021 - BEVDet_ High Performance Multi Camera 3D Object Detection in Bird Eye View.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{Huang2022,
  author           = {Junjie Huang and Guan Huang},
  title            = {BEVDet4D: Exploit Temporal Cues in Multi-camera 3D Object Detection},
  year             = {2022},
  month            = mar,
  abstract         = {Single frame data contains finite information which limits the performance of the existing vision-based multi-camera 3D object detection paradigms. For fundamentally pushing the performance boundary in this area, a novel paradigm dubbed BEVDet4D is proposed to lift the scalable BEVDet paradigm from the spatial-only 3D space to the spatial-temporal 4D space. We upgrade the naive BEVDet framework with a few modifications just for fusing the feature from the previous frame with the corresponding one in the current frame. In this way, with negligible additional computing budget, we enable BEVDet4D to access the temporal cues by querying and comparing the two candidate features. Beyond this, we simplify the task of velocity prediction by removing the factors of ego-motion and time in the learning target. As a result, BEVDet4D with robust generalization performance reduces the velocity error by up to -62.9%. This makes the vision-based methods, for the first time, become comparable with those relied on LiDAR or radar in this aspect. On challenge benchmark nuScenes, we report a new record of 54.5% NDS with the high-performance configuration dubbed BEVDet4D-Base, which surpasses the previous leading method BEVDet-Base by +7.3% NDS. The source code is publicly available for further research at https://github.com/HuangJunJie2017/BEVDet .},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:05:45},
  doi              = {10.48550/arxiv.2203.17054},
  eprint           = {2203.17054},
  file             = {:Huang2022 - BEVDet4D_ Exploit Temporal Cues in Multi Camera 3D Object Detection.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{Huang2024,
  author           = {Zhe Huang and Yizhe Zhao and Hao Xiao and Chenyan Wu and Lingting Ge},
  title            = {DuoSpaceNet: Leveraging Both Bird's-Eye-View and Perspective View Representations for 3D Object Detection},
  year             = {2024},
  month            = may,
  abstract         = {Recent advances in multi-view camera-only 3D object detection either rely on an accurate reconstruction of bird's-eye-view (BEV) 3D features or on traditional 2D perspective view (PV) image features. While both have their own pros and cons, few have found a way to stitch them together in order to benefit from "the best of both worlds". To this end, we explore a duo space (i.e., BEV and PV) 3D perception framework, in conjunction with some useful duo space fusion strategies that allow effective aggregation of the two feature representations. To the best of our knowledge, our proposed method, DuoSpaceNet, is the first to leverage two distinct feature spaces and achieves the state-of-the-art 3D object detection and BEV map segmentation results on nuScenes dataset.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:13:59},
  doi              = {10.48550/arxiv.2405.10577},
  eprint           = {2405.10577},
  file             = {:Huang2024 - DuoSpaceNet_ Leveraging Both Bird's Eye View and Perspective View Representations for 3D Object Detection.pdf:PDF},
  keywords         = {cs.CV, cs.RO},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{Ioffe2015,
  author           = {Sergey Ioffe and Christian Szegedy},
  title            = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  year             = {2015},
  month            = feb,
  abstract         = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.},
  archiveprefix    = {arXiv},
  creationdate     = {2025-01-24T12:50:34},
  doi              = {10.48550/arxiv.1502.03167},
  eprint           = {1502.03167},
  file             = {:Ioffe2015 - Batch Normalization_ Accelerating Deep Network Training by Reducing Internal Covariate Shift.pdf:PDF},
  keywords         = {cs.LG},
  modificationdate = {2025-03-05T21:47:59},
  primaryclass     = {cs.LG},
}

@Article{Jia2023,
  author           = {Xiaosong Jia and Penghao Wu and Li Chen and Jiangwei Xie and Conghui He and Junchi Yan and Hongyang Li},
  title            = {Think Twice before Driving: Towards Scalable Decoders for End-to-End Autonomous Driving},
  year             = {2023},
  month            = may,
  abstract         = {End-to-end autonomous driving has made impressive progress in recent years. Existing methods usually adopt the decoupled encoder-decoder paradigm, where the encoder extracts hidden features from raw sensor data, and the decoder outputs the ego-vehicle's future trajectories or actions. Under such a paradigm, the encoder does not have access to the intended behavior of the ego agent, leaving the burden of finding out safety-critical regions from the massive receptive field and inferring about future situations to the decoder. Even worse, the decoder is usually composed of several simple multi-layer perceptrons (MLP) or GRUs while the encoder is delicately designed (e.g., a combination of heavy ResNets or Transformer). Such an imbalanced resource-task division hampers the learning process. In this work, we aim to alleviate the aforementioned problem by two principles: (1) fully utilizing the capacity of the encoder; (2) increasing the capacity of the decoder. Concretely, we first predict a coarse-grained future position and action based on the encoder features. Then, conditioned on the position and action, the future scene is imagined to check the ramification if we drive accordingly. We also retrieve the encoder features around the predicted coordinate to obtain fine-grained information about the safety-critical region. Finally, based on the predicted future and the retrieved salient feature, we refine the coarse-grained position and action by predicting its offset from ground-truth. The above refinement module could be stacked in a cascaded fashion, which extends the capacity of the decoder with spatial-temporal prior knowledge about the conditioned future. We conduct experiments on the CARLA simulator and achieve state-of-the-art performance in closed-loop benchmarks. Extensive ablation studies demonstrate the effectiveness of each proposed module.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:10:11},
  doi              = {10.48550/arxiv.2305.06242},
  eprint           = {2305.06242},
  file             = {:Jia2023 - Think Twice before Driving_ Towards Scalable Decoders for End to End Autonomous Driving.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{Jiang2022,
  author           = {Yanqin Jiang and Li Zhang and Zhenwei Miao and Xiatian Zhu and Jin Gao and Weiming Hu and Yu-Gang Jiang},
  title            = {PolarFormer: Multi-camera 3D Object Detection with Polar Transformer},
  year             = {2022},
  month            = jun,
  abstract         = {3D object detection in autonomous driving aims to reason "what" and "where" the objects of interest present in a 3D world. Following the conventional wisdom of previous 2D object detection, existing methods often adopt the canonical Cartesian coordinate system with perpendicular axis. However, we conjugate that this does not fit the nature of the ego car's perspective, as each onboard camera perceives the world in shape of wedge intrinsic to the imaging geometry with radical (non-perpendicular) axis. Hence, in this paper we advocate the exploitation of the Polar coordinate system and propose a new Polar Transformer (PolarFormer) for more accurate 3D object detection in the bird's-eye-view (BEV) taking as input only multi-camera 2D images. Specifically, we design a cross attention based Polar detection head without restriction to the shape of input structure to deal with irregular Polar grids. For tackling the unconstrained object scale variations along Polar's distance dimension, we further introduce a multi-scalePolar representation learning strategy. As a result, our model can make best use of the Polar representation rasterized via attending to the corresponding image observation in a sequence-to-sequence fashion subject to the geometric constraints. Thorough experiments on the nuScenes dataset demonstrate that our PolarFormer outperforms significantly state-of-the-art 3D object detection alternatives.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:06:32},
  doi              = {10.48550/arxiv.2206.15398},
  eprint           = {2206.15398},
  file             = {:Jiang2022 - PolarFormer_ Multi Camera 3D Object Detection with Polar Transformer.pdf:PDF},
  keywords         = {cs.CV, cs.AI},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
  priority         = {prio1},
}

@Article{Jiang2023,
  author           = {Bo Jiang and Shaoyu Chen and Qing Xu and Bencheng Liao and Jiajie Chen and Helong Zhou and Qian Zhang and Wenyu Liu and Chang Huang and Xinggang Wang},
  title            = {VAD: Vectorized Scene Representation for Efficient Autonomous Driving},
  year             = {2023},
  month            = mar,
  abstract         = {Autonomous driving requires a comprehensive understanding of the surrounding environment for reliable trajectory planning. Previous works rely on dense rasterized scene representation (e.g., agent occupancy and semantic map) to perform planning, which is computationally intensive and misses the instance-level structure information. In this paper, we propose VAD, an end-to-end vectorized paradigm for autonomous driving, which models the driving scene as a fully vectorized representation. The proposed vectorized paradigm has two significant advantages. On one hand, VAD exploits the vectorized agent motion and map elements as explicit instance-level planning constraints which effectively improves planning safety. On the other hand, VAD runs much faster than previous end-to-end planning methods by getting rid of computation-intensive rasterized representation and hand-designed post-processing steps. VAD achieves state-of-the-art end-to-end planning performance on the nuScenes dataset, outperforming the previous best method by a large margin. Our base model, VAD-Base, greatly reduces the average collision rate by 29.0% and runs 2.5x faster. Besides, a lightweight variant, VAD-Tiny, greatly improves the inference speed (up to 9.3x) while achieving comparable planning performance. We believe the excellent performance and the high efficiency of VAD are critical for the real-world deployment of an autonomous driving system. Code and models are available at https://github.com/hustvl/VAD for facilitating future research.},
  archiveprefix    = {arXiv},
  comment          = {end2end MB},
  doi              = {10.48550/arxiv.2303.12077},
  eprint           = {2303.12077},
  file             = {:Jiang2023 - VAD_ Vectorized Scene Representation for Efficient Autonomous Driving.pdf:PDF},
  groups           = {Reading Group Sparse Query},
  keywords         = {cs.RO, cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.RO},
}

@Article{Jiang2023a,
  author           = {Xiaohui Jiang and Shuailin Li and Yingfei Liu and Shihao Wang and Fan Jia and Tiancai Wang and Lijin Han and Xiangyu Zhang},
  title            = {Far3D: Expanding the Horizon for Surround-view 3D Object Detection},
  year             = {2023},
  month            = aug,
  abstract         = {Recently 3D object detection from surround-view images has made notable advancements with its low deployment cost. However, most works have primarily focused on close perception range while leaving long-range detection less explored. Expanding existing methods directly to cover long distances poses challenges such as heavy computation costs and unstable convergence. To address these limitations, this paper proposes a novel sparse query-based framework, dubbed Far3D. By utilizing high-quality 2D object priors, we generate 3D adaptive queries that complement the 3D global queries. To efficiently capture discriminative features across different views and scales for long-range objects, we introduce a perspective-aware aggregation module. Additionally, we propose a range-modulated 3D denoising approach to address query error propagation and mitigate convergence issues in long-range tasks. Significantly, Far3D demonstrates SoTA performance on the challenging Argoverse 2 dataset, covering a wide range of 150 meters, surpassing several LiDAR-based approaches. Meanwhile, Far3D exhibits superior performance compared to previous methods on the nuScenes dataset. The code is available at https://github.com/megvii-research/Far3D.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-09-30T15:54:22},
  doi              = {10.48550/arxiv.2308.09616},
  eprint           = {2308.09616},
  file             = {:Jiang2023a - Far3D_ Expanding the Horizon for Surround View 3D Object Detection.pdf:PDF},
  groups           = {Reading Group Sparse Query},
  keywords         = {cs.CV},
  modificationdate = {2025-04-02T13:22:50},
  primaryclass     = {cs.CV},
  priority         = {prio1},
}

@Article{Kingma2014AdamAM,
  author           = {Diederik P. Kingma and Jimmy Ba},
  journal          = {CoRR},
  title            = {Adam: A Method for Stochastic Optimization},
  year             = {2014},
  volume           = {abs/1412.6980},
  creationdate     = {2024-11-12T13:33:04},
  modificationdate = {2024-11-12T13:33:04},
  url              = {https://api.semanticscholar.org/CorpusID:6628106},
}

@Article{Li2022,
  author           = {Zhiqi Li and Wenhai Wang and Hongyang Li and Enze Xie and Chonghao Sima and Tong Lu and Qiao Yu and Jifeng Dai},
  title            = {BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers},
  year             = {2022},
  month            = mar,
  abstract         = {3D visual perception tasks, including 3D detection and map segmentation based on multi-camera images, are essential for autonomous driving systems. In this work, we present a new framework termed BEVFormer, which learns unified BEV representations with spatiotemporal transformers to support multiple autonomous driving perception tasks. In a nutshell, BEVFormer exploits both spatial and temporal information by interacting with spatial and temporal space through predefined grid-shaped BEV queries. To aggregate spatial information, we design spatial cross-attention that each BEV query extracts the spatial features from the regions of interest across camera views. For temporal information, we propose temporal self-attention to recurrently fuse the history BEV information. Our approach achieves the new state-of-the-art 56.9\% in terms of NDS metric on the nuScenes \texttt{test} set, which is 9.0 points higher than previous best arts and on par with the performance of LiDAR-based baselines. We further show that BEVFormer remarkably improves the accuracy of velocity estimation and recall of objects under low visibility conditions. The code is available at \url{https://github.com/zhiqi-li/BEVFormer}.},
  archiveprefix    = {arXiv},
  comment          = {questions:
* does the transformer-based BEV encoder only contribute to the feature generation and not to the BB representation in the feature space as in Deep Fusion 2 "Association and Update" module?
* is the 3D object detection head considered the decoder of the transformer?
* how does the temporal self-attention cope with target vehicle movement after ego motion compensation? --> offsets delta p?},
  creationdate     = {2024-10-15T09:04:35},
  doi              = {10.48550/arxiv.2203.17270},
  eprint           = {2203.17270},
  file             = {:Li2022 - BEVFormer_ Learning Bird's Eye View Representation from Multi Camera Images Via Spatiotemporal Transformers.pdf:PDF},
  groups           = {Reading Group Sparse Query},
  keywords         = {cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
  readstatus       = {read},
}

@Article{Li2022a,
  author           = {Feng Li and Hao Zhang and Shilong Liu and Jian Guo and Lionel M. Ni and Lei Zhang},
  title            = {DN-DETR: Accelerate DETR Training by Introducing Query DeNoising},
  year             = {2022},
  month            = mar,
  abstract         = {We present in this paper a novel denoising training method to speedup DETR (DEtection TRansformer) training and offer a deepened understanding of the slow convergence issue of DETR-like methods. We show that the slow convergence results from the instability of bipartite graph matching which causes inconsistent optimization goals in early training stages. To address this issue, except for the Hungarian loss, our method additionally feeds ground-truth bounding boxes with noises into Transformer decoder and trains the model to reconstruct the original boxes, which effectively reduces the bipartite graph matching difficulty and leads to a faster convergence. Our method is universal and can be easily plugged into any DETR-like methods by adding dozens of lines of code to achieve a remarkable improvement. As a result, our DN-DETR results in a remarkable improvement ($+1.9$AP) under the same setting and achieves the best result (AP $43.4$ and $48.6$ with $12$ and $50$ epochs of training respectively) among DETR-like methods with ResNet-$50$ backbone. Compared with the baseline under the same setting, DN-DETR achieves comparable performance with $50\%$ training epochs. Code is available at \url{https://github.com/FengLi-ust/DN-DETR}.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-11-13T14:48:48},
  doi              = {10.48550/arxiv.2203.01305},
  eprint           = {2203.01305},
  file             = {:Li2022a - DN DETR_ Accelerate DETR Training by Introducing Query DeNoising.pdf:PDF},
  keywords         = {cs.CV, cs.AI},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{Li2023,
  author           = {Tianyu Li and Li Chen and Huijie Wang and Yang Li and Jiazhi Yang and Xiangwei Geng and Shengyin Jiang and Yuting Wang and Hang Xu and Chunjing Xu and Junchi Yan and Ping Luo and Hongyang Li},
  title            = {Graph-based Topology Reasoning for Driving Scenes},
  year             = {2023},
  month            = apr,
  abstract         = {Understanding the road genome is essential to realize autonomous driving. This highly intelligent problem contains two aspects - the connection relationship of lanes, and the assignment relationship between lanes and traffic elements, where a comprehensive topology reasoning method is vacant. On one hand, previous map learning techniques struggle in deriving lane connectivity with segmentation or laneline paradigms; or prior lane topology-oriented approaches focus on centerline detection and neglect the interaction modeling. On the other hand, the traffic element to lane assignment problem is limited in the image domain, leaving how to construct the correspondence from two views an unexplored challenge. To address these issues, we present TopoNet, the first end-to-end framework capable of abstracting traffic knowledge beyond conventional perception tasks. To capture the driving scene topology, we introduce three key designs: (1) an embedding module to incorporate semantic knowledge from 2D elements into a unified feature space; (2) a curated scene graph neural network to model relationships and enable feature interaction inside the network; (3) instead of transmitting messages arbitrarily, a scene knowledge graph is devised to differentiate prior knowledge from various types of the road genome. We evaluate TopoNet on the challenging scene understanding benchmark, OpenLane-V2, where our approach outperforms all previous works by a great margin on all perceptual and topological metrics. The code is released at https://github.com/OpenDriveLab/TopoNet},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:09:16},
  doi              = {10.48550/arxiv.2304.05277},
  eprint           = {2304.05277},
  file             = {:Li2023 - Graph Based Topology Reasoning for Driving Scenes.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{Li2023a,
  author           = {Tianyu Li and Peijin Jia and Bangjun Wang and Li Chen and Kun Jiang and Junchi Yan and Hongyang Li},
  title            = {LaneSegNet: Map Learning with Lane Segment Perception for Autonomous Driving},
  year             = {2023},
  month            = dec,
  abstract         = {A map, as crucial information for downstream applications of an autonomous driving system, is usually represented in lanelines or centerlines. However, existing literature on map learning primarily focuses on either detecting geometry-based lanelines or perceiving topology relationships of centerlines. Both of these methods ignore the intrinsic relationship of lanelines and centerlines, that lanelines bind centerlines. While simply predicting both types of lane in one model is mutually excluded in learning objective, we advocate lane segment as a new representation that seamlessly incorporates both geometry and topology information. Thus, we introduce LaneSegNet, the first end-to-end mapping network generating lane segments to obtain a complete representation of the road structure. Our algorithm features two key modifications. One is a lane attention module to capture pivotal region details within the long-range feature space. Another is an identical initialization strategy for reference points, which enhances the learning of positional priors for lane attention. On the OpenLane-V2 dataset, LaneSegNet outperforms previous counterparts by a substantial gain across three tasks, \textit{i.e.}, map element detection (+4.8 mAP), centerline perception (+6.9 DET$_l$), and the newly defined one, lane segment perception (+5.6 mAP). Furthermore, it obtains a real-time inference speed of 14.7 FPS. Code is accessible at https://github.com/OpenDriveLab/LaneSegNet.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:12:00},
  doi              = {10.48550/arxiv.2312.16108},
  eprint           = {2312.16108},
  file             = {:Li2023a - LaneSegNet_ Map Learning with Lane Segment Perception for Autonomous Driving.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{Liao2022,
  author           = {Bencheng Liao and Shaoyu Chen and Xinggang Wang and Tianheng Cheng and Qian Zhang and Wenyu Liu and Chang Huang},
  title            = {MapTR: Structured Modeling and Learning for Online Vectorized HD Map Construction},
  year             = {2022},
  month            = aug,
  abstract         = {High-definition (HD) map provides abundant and precise environmental information of the driving scene, serving as a fundamental and indispensable component for planning in autonomous driving system. We present MapTR, a structured end-to-end Transformer for efficient online vectorized HD map construction. We propose a unified permutation-equivalent modeling approach, i.e., modeling map element as a point set with a group of equivalent permutations, which accurately describes the shape of map element and stabilizes the learning process. We design a hierarchical query embedding scheme to flexibly encode structured map information and perform hierarchical bipartite matching for map element learning. MapTR achieves the best performance and efficiency with only camera input among existing vectorized map construction approaches on nuScenes dataset. In particular, MapTR-nano runs at real-time inference speed ($25.1$ FPS) on RTX 3090, $8\times$ faster than the existing state-of-the-art camera-based method while achieving $5.0$ higher mAP. Even compared with the existing state-of-the-art multi-modality method, MapTR-nano achieves $0.7$ higher mAP, and MapTR-tiny achieves $13.5$ higher mAP and $3\times$ faster inference speed. Abundant qualitative results show that MapTR maintains stable and robust map construction quality in complex and various driving scenes. MapTR is of great application value in autonomous driving. Code and more demos are available at \url{https://github.com/hustvl/MapTR}.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:07:35},
  doi              = {10.48550/arxiv.2208.14437},
  eprint           = {2208.14437},
  file             = {:Liao2022 - MapTR_ Structured Modeling and Learning for Online Vectorized HD Map Construction.pdf:PDF},
  keywords         = {cs.CV, cs.RO},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{Liao2023,
  author           = {Bencheng Liao and Shaoyu Chen and Yunchi Zhang and Bo Jiang and Qian Zhang and Wenyu Liu and Chang Huang and Xinggang Wang},
  title            = {MapTRv2: An End-to-End Framework for Online Vectorized HD Map Construction},
  year             = {2023},
  month            = aug,
  abstract         = {High-definition (HD) map provides abundant and precise static environmental information of the driving scene, serving as a fundamental and indispensable component for planning in autonomous driving system. In this paper, we present \textbf{Map} \textbf{TR}ansformer, an end-to-end framework for online vectorized HD map construction. We propose a unified permutation-equivalent modeling approach, \ie, modeling map element as a point set with a group of equivalent permutations, which accurately describes the shape of map element and stabilizes the learning process. We design a hierarchical query embedding scheme to flexibly encode structured map information and perform hierarchical bipartite matching for map element learning. To speed up convergence, we further introduce auxiliary one-to-many matching and dense supervision. The proposed method well copes with various map elements with arbitrary shapes. It runs at real-time inference speed and achieves state-of-the-art performance on both nuScenes and Argoverse2 datasets. Abundant qualitative results show stable and robust map construction quality in complex and various driving scenes. Code and more demos are available at \url{https://github.com/hustvl/MapTR} for facilitating further studies and applications.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:11:30},
  doi              = {10.48550/arxiv.2308.05736},
  eprint           = {2308.05736},
  file             = {:Liao2023 - MapTRv2_ an End to End Framework for Online Vectorized HD Map Construction.pdf:PDF},
  keywords         = {cs.CV, cs.RO},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{Lin2013,
  author           = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
  title            = {Network In Network},
  year             = {2013},
  month            = dec,
  abstract         = {We propose a novel deep network structure called "Network In Network" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
  archiveprefix    = {arXiv},
  comment          = {1x1 convolutions},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2025-02-28T13:32:50},
  doi              = {10.48550/ARXIV.1312.4400},
  eprint           = {1312.4400},
  file             = {:Lin2013 - Network in Network.pdf:PDF:http\://arxiv.org/pdf/1312.4400v3},
  keywords         = {Neural and Evolutionary Computing (cs.NE), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences},
  modificationdate = {2025-02-28T13:33:05},
  primaryclass     = {cs.NE},
  publisher        = {arXiv},
}

@Article{Lin2016,
  author           = {Tsung-Yi Lin and Piotr DollÃ¡r and Ross Girshick and Kaiming He and Bharath Hariharan and Serge Belongie},
  title            = {Feature Pyramid Networks for Object Detection},
  year             = {2016},
  month            = dec,
  abstract         = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
  archiveprefix    = {arXiv},
  creationdate     = {2025-02-10T11:26:42},
  eprint           = {1612.03144},
  file             = {:Lin2016 - Feature Pyramid Networks for Object Detection.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-02-10T11:26:43},
  primaryclass     = {cs.CV},
}

@Article{Lin2022,
  author           = {Xuewu Lin and Tianwei Lin and Zixiang Pei and Lichao Huang and Zhizhong Su},
  title            = {Sparse4D: Multi-view 3D Object Detection with Sparse Spatial-Temporal Fusion},
  year             = {2022},
  month            = nov,
  abstract         = {Bird-eye-view (BEV) based methods have made great progress recently in multi-view 3D detection task. Comparing with BEV based methods, sparse based methods lag behind in performance, but still have lots of non-negligible merits. To push sparse 3D detection further, in this work, we introduce a novel method, named Sparse4D, which does the iterative refinement of anchor boxes via sparsely sampling and fusing spatial-temporal features. (1) Sparse 4D Sampling: for each 3D anchor, we assign multiple 4D keypoints, which are then projected to multi-view/scale/timestamp image features to sample corresponding features; (2) Hierarchy Feature Fusion: we hierarchically fuse sampled features of different view/scale, different timestamp and different keypoints to generate high-quality instance feature. In this way, Sparse4D can efficiently and effectively achieve 3D detection without relying on dense view transformation nor global attention, and is more friendly to edge devices deployment. Furthermore, we introduce an instance-level depth reweight module to alleviate the ill-posed issue in 3D-to-2D projection. In experiment, our method outperforms all sparse based methods and most BEV based methods on detection task in the nuScenes dataset.},
  archiveprefix    = {arXiv},
  doi              = {10.48550/arxiv.2211.10581},
  eprint           = {2211.10581},
  file             = {:Lin2022 - Sparse4D_ Multi View 3D Object Detection with Sparse Spatial Temporal Fusion.pdf:PDF},
  groups           = {Reading Group Sparse Query},
  keywords         = {cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
  priority         = {prio1},
}

@Article{Lin2023,
  author           = {Xuewu Lin and Tianwei Lin and Zixiang Pei and Lichao Huang and Zhizhong Su},
  title            = {Sparse4D v2: Recurrent Temporal Fusion with Sparse Model},
  year             = {2023},
  month            = may,
  abstract         = {Sparse algorithms offer great flexibility for multi-view temporal perception tasks. In this paper, we present an enhanced version of Sparse4D, in which we improve the temporal fusion module by implementing a recursive form of multi-frame feature sampling. By effectively decoupling image features and structured anchor features, Sparse4D enables a highly efficient transformation of temporal features, thereby facilitating temporal fusion solely through the frame-by-frame transmission of sparse features. The recurrent temporal fusion approach provides two main benefits. Firstly, it reduces the computational complexity of temporal fusion from $O(T)$ to $O(1)$, resulting in significant improvements in inference speed and memory usage. Secondly, it enables the fusion of long-term information, leading to more pronounced performance improvements due to temporal fusion. Our proposed approach, Sparse4Dv2, further enhances the performance of the sparse perception algorithm and achieves state-of-the-art results on the nuScenes 3D detection benchmark. Code will be available at \url{https://github.com/linxuewu/Sparse4D}.},
  archiveprefix    = {arXiv},
  doi              = {10.48550/arxiv.2305.14018},
  eprint           = {2305.14018},
  file             = {:Lin2023 - Sparse4D V2_ Recurrent Temporal Fusion with Sparse Model.pdf:PDF},
  groups           = {Reading Group Sparse Query},
  keywords         = {cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
  priority         = {prio1},
}

@Article{Lin2023a,
  author           = {Xuewu Lin and Zixiang Pei and Tianwei Lin and Lichao Huang and Zhizhong Su},
  title            = {Sparse4D v3: Advancing End-to-End 3D Detection and Tracking},
  year             = {2023},
  month            = nov,
  abstract         = {In autonomous driving perception systems, 3D detection and tracking are the two fundamental tasks. This paper delves deeper into this field, building upon the Sparse4D framework. We introduce two auxiliary training tasks (Temporal Instance Denoising and Quality Estimation) and propose decoupled attention to make structural improvements, leading to significant enhancements in detection performance. Additionally, we extend the detector into a tracker using a straightforward approach that assigns instance ID during inference, further highlighting the advantages of query-based algorithms. Extensive experiments conducted on the nuScenes benchmark validate the effectiveness of the proposed improvements. With ResNet50 as the backbone, we witnessed enhancements of 3.0\%, 2.2\%, and 7.6\% in mAP, NDS, and AMOTA, achieving 46.9\%, 56.1\%, and 49.0\%, respectively. Our best model achieved 71.9\% NDS and 67.7\% AMOTA on the nuScenes test set. Code will be released at \url{https://github.com/linxuewu/Sparse4D}.},
  archiveprefix    = {arXiv},
  doi              = {10.48550/arxiv.2311.11722},
  eprint           = {2311.11722},
  file             = {:Lin2023a - Sparse4D V3_ Advancing End to End 3D Detection and Tracking.pdf:PDF},
  groups           = {Reading Group Sparse Query},
  keywords         = {cs.CV, cs.AI, cs.RO},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
  priority         = {prio1},
}

@Article{Lin2024,
  author           = {Zhiwei Lin and Zhe Liu and Yongtao Wang and Le Zhang and Ce Zhu},
  title            = {RCBEVDet++: Toward High-accuracy Radar-Camera Fusion 3D Perception Network},
  year             = {2024},
  month            = sep,
  abstract         = {Perceiving the surrounding environment is a fundamental task in autonomous driving. To obtain highly accurate perception results, modern autonomous driving systems typically employ multi-modal sensors to collect comprehensive environmental data. Among these, the radar-camera multi-modal perception system is especially favored for its excellent sensing capabilities and cost-effectiveness. However, the substantial modality differences between radar and camera sensors pose challenges in fusing information. To address this problem, this paper presents RCBEVDet, a radar-camera fusion 3D object detection framework. Specifically, RCBEVDet is developed from an existing camera-based 3D object detector, supplemented by a specially designed radar feature extractor, RadarBEVNet, and a Cross-Attention Multi-layer Fusion (CAMF) module. Firstly, RadarBEVNet encodes sparse radar points into a dense bird's-eye-view (BEV) feature using a dual-stream radar backbone and a Radar Cross Section aware BEV encoder. Secondly, the CAMF module utilizes a deformable attention mechanism to align radar and camera BEV features and adopts channel and spatial fusion layers to fuse them. To further enhance RCBEVDet's capabilities, we introduce RCBEVDet++, which advances the CAMF through sparse fusion, supports query-based multi-view camera perception models, and adapts to a broader range of perception tasks. Extensive experiments on the nuScenes show that our method integrates seamlessly with existing camera-based 3D perception models and improves their performance across various perception tasks. Furthermore, our method achieves state-of-the-art radar-camera fusion results in 3D object detection, BEV semantic segmentation, and 3D multi-object tracking tasks. Notably, with ViT-L as the image backbone, RCBEVDet++ achieves 72.73 NDS and 67.34 mAP in 3D object detection without test-time augmentation or model ensembling.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:15:57},
  doi              = {10.48550/arxiv.2409.04979},
  eprint           = {2409.04979},
  file             = {:Lin2024 - RCBEVDet++_ toward High Accuracy Radar Camera Fusion 3D Perception Network.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{Lin2024a,
  author           = {Zhiwei Lin and Zhe Liu and Zhongyu Xia and Xinhao Wang and Yongtao Wang and Shengxiang Qi and Yang Dong and Nan Dong and Le Zhang and Ce Zhu},
  title            = {RCBEVDet: Radar-camera Fusion in Bird's Eye View for 3D Object Detection},
  year             = {2024},
  month            = mar,
  abstract         = {Three-dimensional object detection is one of the key tasks in autonomous driving. To reduce costs in practice, low-cost multi-view cameras for 3D object detection are proposed to replace the expansive LiDAR sensors. However, relying solely on cameras is difficult to achieve highly accurate and robust 3D object detection. An effective solution to this issue is combining multi-view cameras with the economical millimeter-wave radar sensor to achieve more reliable multi-modal 3D object detection. In this paper, we introduce RCBEVDet, a radar-camera fusion 3D object detection method in the bird's eye view (BEV). Specifically, we first design RadarBEVNet for radar BEV feature extraction. RadarBEVNet consists of a dual-stream radar backbone and a Radar Cross-Section (RCS) aware BEV encoder. In the dual-stream radar backbone, a point-based encoder and a transformer-based encoder are proposed to extract radar features, with an injection and extraction module to facilitate communication between the two encoders. The RCS-aware BEV encoder takes RCS as the object size prior to scattering the point feature in BEV. Besides, we present the Cross-Attention Multi-layer Fusion module to automatically align the multi-modal BEV feature from radar and camera with the deformable attention mechanism, and then fuse the feature with channel and spatial fusion layers. Experimental results show that RCBEVDet achieves new state-of-the-art radar-camera fusion results on nuScenes and view-of-delft (VoD) 3D object detection benchmarks. Furthermore, RCBEVDet achieves better 3D detection results than all real-time camera-only and radar-camera 3D object detectors with a faster inference speed at 21~28 FPS. The source code will be released at https://github.com/VDIGPKU/RCBEVDet.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:15:57},
  doi              = {10.48550/arxiv.2403.16440},
  eprint           = {2403.16440},
  file             = {:Lin2024a - RCBEVDet_ Radar Camera Fusion in Bird's Eye View for 3D Object Detection.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{Liu2022,
  author           = {Yingfei Liu and Tiancai Wang and Xiangyu Zhang and Jian Sun},
  title            = {PETR: Position Embedding Transformation for Multi-View 3D Object Detection},
  year             = {2022},
  month            = mar,
  abstract         = {In this paper, we develop position embedding transformation (PETR) for multi-view 3D object detection. PETR encodes the position information of 3D coordinates into image features, producing the 3D position-aware features. Object query can perceive the 3D position-aware features and perform end-to-end object detection. PETR achieves state-of-the-art performance (50.4% NDS and 44.1% mAP) on standard nuScenes dataset and ranks 1st place on the benchmark. It can serve as a simple yet strong baseline for future research. Code is available at \url{https://github.com/megvii-research/PETR}.},
  archiveprefix    = {arXiv},
  comment          = {PETR, further info medium article: https://medium.com/@jiangmen28/petr-position-embedding-transformation-for-multi-view-3d-object-detection-70cbeb5c3701},
  doi              = {10.48550/arxiv.2203.05625},
  eprint           = {2203.05625},
  file             = {:Liu2022 - PETR_ Position Embedding Transformation for Multi View 3D Object Detection.pdf:PDF},
  groups           = {PMO AI},
  keywords         = {cs.CV},
  modificationdate = {2025-04-03T08:50:23},
  primaryclass     = {cs.CV},
  priority         = {prio1},
  readstatus       = {read},
}

@Article{Liu2022a,
  author           = {Yingfei Liu and Junjie Yan and Fan Jia and Shuailin Li and Aqi Gao and Tiancai Wang and Xiangyu Zhang and Jian Sun},
  title            = {PETRv2: A Unified Framework for 3D Perception from Multi-Camera Images},
  year             = {2022},
  month            = jun,
  abstract         = {In this paper, we propose PETRv2, a unified framework for 3D perception from multi-view images. Based on PETR, PETRv2 explores the effectiveness of temporal modeling, which utilizes the temporal information of previous frames to boost 3D object detection. More specifically, we extend the 3D position embedding (3D PE) in PETR for temporal modeling. The 3D PE achieves the temporal alignment on object position of different frames. A feature-guided position encoder is further introduced to improve the data adaptability of 3D PE. To support for multi-task learning (e.g., BEV segmentation and 3D lane detection), PETRv2 provides a simple yet effective solution by introducing task-specific queries, which are initialized under different spaces. PETRv2 achieves state-of-the-art performance on 3D object detection, BEV segmentation and 3D lane detection. Detailed robustness analysis is also conducted on PETR framework. We hope PETRv2 can serve as a strong baseline for 3D perception. Code is available at \url{https://github.com/megvii-research/PETR}.},
  archiveprefix    = {arXiv},
  comment          = {PETRv2},
  doi              = {10.48550/arxiv.2206.01256},
  eprint           = {2206.01256},
  file             = {:Liu2022a - PETRv2_ a Unified Framework for 3D Perception from Multi Camera Images.pdf:PDF},
  groups           = {PMO AI},
  keywords         = {cs.CV},
  modificationdate = {2025-04-02T13:24:37},
  primaryclass     = {cs.CV},
  priority         = {prio1},
  readstatus       = {read},
}

@Article{Liu2022b,
  author           = {Zhijian Liu and Haotian Tang and Alexander Amini and Xinyu Yang and Huizi Mao and Daniela Rus and Song Han},
  title            = {BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird's-Eye View Representation},
  year             = {2022},
  month            = may,
  abstract         = {Multi-sensor fusion is essential for an accurate and reliable autonomous driving system. Recent approaches are based on point-level fusion: augmenting the LiDAR point cloud with camera features. However, the camera-to-LiDAR projection throws away the semantic density of camera features, hindering the effectiveness of such methods, especially for semantic-oriented tasks (such as 3D scene segmentation). In this paper, we break this deeply-rooted convention with BEVFusion, an efficient and generic multi-task multi-sensor fusion framework. It unifies multi-modal features in the shared bird's-eye view (BEV) representation space, which nicely preserves both geometric and semantic information. To achieve this, we diagnose and lift key efficiency bottlenecks in the view transformation with optimized BEV pooling, reducing latency by more than 40x. BEVFusion is fundamentally task-agnostic and seamlessly supports different 3D perception tasks with almost no architectural changes. It establishes the new state of the art on nuScenes, achieving 1.3% higher mAP and NDS on 3D object detection and 13.6% higher mIoU on BEV map segmentation, with 1.9x lower computation cost. Code to reproduce our results is available at https://github.com/mit-han-lab/bevfusion.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:06:03},
  doi              = {10.48550/arxiv.2205.13542},
  eprint           = {2205.13542},
  file             = {:Liu2022b - BEVFusion_ Multi Task Multi Sensor Fusion with Unified Bird's Eye View Representation.pdf:PDF},
  groups           = {Reading Group Sparse Query},
  keywords         = {cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
  readstatus       = {read},
}

@Article{Liu2023,
  author           = {Haisong Liu and Yao Teng and Tao Lu and Haiguang Wang and Limin Wang},
  title            = {SparseBEV: High-Performance Sparse 3D Object Detection from Multi-Camera Videos},
  year             = {2023},
  month            = aug,
  abstract         = {Camera-based 3D object detection in BEV (Bird's Eye View) space has drawn great attention over the past few years. Dense detectors typically follow a two-stage pipeline by first constructing a dense BEV feature and then performing object detection in BEV space, which suffers from complex view transformations and high computation cost. On the other side, sparse detectors follow a query-based paradigm without explicit dense BEV feature construction, but achieve worse performance than the dense counterparts. In this paper, we find that the key to mitigate this performance gap is the adaptability of the detector in both BEV and image space. To achieve this goal, we propose SparseBEV, a fully sparse 3D object detector that outperforms the dense counterparts. SparseBEV contains three key designs, which are (1) scale-adaptive self attention to aggregate features with adaptive receptive field in BEV space, (2) adaptive spatio-temporal sampling to generate sampling locations under the guidance of queries, and (3) adaptive mixing to decode the sampled features with dynamic weights from the queries. On the test split of nuScenes, SparseBEV achieves the state-of-the-art performance of 67.5 NDS. On the val split, SparseBEV achieves 55.8 NDS while maintaining a real-time inference speed of 23.5 FPS. Code is available at https://github.com/MCG-NJU/SparseBEV.},
  archiveprefix    = {arXiv},
  comment          = {questions:
* pillars instead of reference points as formulation of queries; what are pillars?
* how can I understand sparse pillar queries?
* in 3.1 it says that learnable queries are initialized with ...; is this like a set of trained weights or are these computed during inference and if so, why are these then "learnable"?},
  creationdate     = {2024-09-30T15:55:05},
  doi              = {10.48550/arxiv.2308.09244},
  eprint           = {2308.09244},
  file             = {:Liu2023 - SparseBEV_ High Performance Sparse 3D Object Detection from Multi Camera Videos.pdf:PDF},
  groups           = {Reading Group Sparse Query},
  keywords         = {cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
  priority         = {prio1},
  readstatus       = {read},
}

@Article{Liu2023a,
  author           = {Haisong Liu and Yang Chen and Haiguang Wang and Zetong Yang and Tianyu Li and Jia Zeng and Li Chen and Hongyang Li and Limin Wang},
  title            = {Fully Sparse 3D Occupancy Prediction},
  year             = {2023},
  month            = dec,
  abstract         = {Occupancy prediction plays a pivotal role in autonomous driving. Previous methods typically construct dense 3D volumes, neglecting the inherent sparsity of the scene and suffering from high computational costs. To bridge the gap, we introduce a novel fully sparse occupancy network, termed SparseOcc. SparseOcc initially reconstructs a sparse 3D representation from camera-only inputs and subsequently predicts semantic/instance occupancy from the 3D sparse representation by sparse queries. A mask-guided sparse sampling is designed to enable sparse queries to interact with 2D features in a fully sparse manner, thereby circumventing costly dense features or global attention. Additionally, we design a thoughtful ray-based evaluation metric, namely RayIoU, to solve the inconsistency penalty along the depth axis raised in traditional voxel-level mIoU criteria. SparseOcc demonstrates its effectiveness by achieving a RayIoU of 34.0, while maintaining a real-time inference speed of 17.3 FPS, with 7 history frames inputs. By incorporating more preceding frames to 15, SparseOcc continuously improves its performance to 35.1 RayIoU without bells and whistles.},
  archiveprefix    = {arXiv},
  comment          = {mentioned by Christoph SchÃ¶ller "Looking into how well it is possible to represent the static environment via a sparse encoding would be a different or future task."},
  creationdate     = {2024-10-31T09:04:02},
  doi              = {10.48550/arxiv.2312.17118},
  eprint           = {2312.17118},
  file             = {:Liu2023a - Fully Sparse 3D Occupancy Prediction.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{Luiten2020,
  author           = {Jonathon Luiten and Aljosa Osep and Patrick Dendorfer and Philip Torr and Andreas Geiger and Laura Leal-Taixe and Bastian Leibe},
  journal          = {International Journal of Computer Vision (2020)},
  title            = {HOTA: A Higher Order Metric for Evaluating Multi-Object Tracking},
  year             = {2020},
  month            = sep,
  abstract         = {Multi-Object Tracking (MOT) has been notoriously difficult to evaluate. Previous metrics overemphasize the importance of either detection or association. To address this, we present a novel MOT evaluation metric, HOTA (Higher Order Tracking Accuracy), which explicitly balances the effect of performing accurate detection, association and localization into a single unified metric for comparing trackers. HOTA decomposes into a family of sub-metrics which are able to evaluate each of five basic error types separately, which enables clear analysis of tracking performance. We evaluate the effectiveness of HOTA on the MOTChallenge benchmark, and show that it is able to capture important aspects of MOT performance not previously taken into account by established metrics. Furthermore, we show HOTA scores better align with human visual evaluation of tracking performance.},
  archiveprefix    = {arXiv},
  creationdate     = {2025-07-16T08:45:17},
  doi              = {10.1007/s11263-020-01375-2},
  eprint           = {2009.07736},
  file             = {:- HOTA_ a Higher Order Metric for Evaluating Multi Object Tracking.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-07-16T08:45:28},
  primaryclass     = {cs.CV},
  priority         = {prio1},
}

@Article{Lv2024,
  author           = {Wenyu Lv and Yian Zhao and Qinyao Chang and Kui Huang and Guanzhong Wang and Yi Liu},
  title            = {RT-DETRv2: Improved Baseline with Bag-of-Freebies for Real-Time Detection Transformer},
  year             = {2024},
  month            = jul,
  abstract         = {In this report, we present RT-DETRv2, an improved Real-Time DEtection TRansformer (RT-DETR). RT-DETRv2 builds upon the previous state-of-the-art real-time detector, RT-DETR, and opens up a set of bag-of-freebies for flexibility and practicality, as well as optimizing the training strategy to achieve enhanced performance. To improve the flexibility, we suggest setting a distinct number of sampling points for features at different scales in the deformable attention to achieve selective multi-scale feature extraction by the decoder. To enhance practicality, we propose an optional discrete sampling operator to replace the grid_sample operator that is specific to RT-DETR compared to YOLOs. This removes the deployment constraints typically associated with DETRs. For the training strategy, we propose dynamic data augmentation and scale-adaptive hyperparameters customization to improve performance without loss of speed. Source code and pre-trained models will be available at https://github.com/lyuwenyu/RT-DETR.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:15:03},
  doi              = {10.48550/arxiv.2407.17140},
  eprint           = {2407.17140},
  file             = {:Lv2024 - RT DETRv2_ Improved Baseline with Bag of Freebies for Real Time Detection Transformer.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{Ma2022,
  author           = {Yuexin Ma and Tai Wang and Xuyang Bai and Huitong Yang and Yuenan Hou and Yaming Wang and Yu Qiao and Ruigang Yang and Dinesh Manocha and Xinge Zhu},
  title            = {Vision-Centric BEV Perception: A Survey},
  year             = {2022},
  month            = aug,
  abstract         = {In recent years, vision-centric Bird's Eye View (BEV) perception has garnered significant interest from both industry and academia due to its inherent advantages, such as providing an intuitive representation of the world and being conducive to data fusion. The rapid advancements in deep learning have led to the proposal of numerous methods for addressing vision-centric BEV perception challenges. However, there has been no recent survey encompassing this novel and burgeoning research field. To catalyze future research, this paper presents a comprehensive survey of the latest developments in vision-centric BEV perception and its extensions. It compiles and organizes up-to-date knowledge, offering a systematic review and summary of prevalent algorithms. Additionally, the paper provides in-depth analyses and comparative results on various BEV perception tasks, facilitating the evaluation of future works and sparking new research directions. Furthermore, the paper discusses and shares valuable empirical implementation details to aid in the advancement of related algorithms.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:08:08},
  doi              = {10.48550/arxiv.2208.02797},
  eprint           = {2208.02797},
  file             = {:Ma2022 - Vision Centric BEV Perception_ a Survey.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{Milan2016,
  author           = {Anton Milan and Laura Leal-Taixe and Ian Reid and Stefan Roth and Konrad Schindler},
  title            = {MOT16: A Benchmark for Multi-Object Tracking},
  year             = {2016},
  month            = mar,
  abstract         = {Standardized benchmarks are crucial for the majority of computer vision applications. Although leaderboards and ranking tables should not be over-claimed, benchmarks often provide the most objective measure of performance and are therefore important guides for reseach. Recently, a new benchmark for Multiple Object Tracking, MOTChallenge, was launched with the goal of collecting existing and new data and creating a framework for the standardized evaluation of multiple object tracking methods. The first release of the benchmark focuses on multiple people tracking, since pedestrians are by far the most studied object in the tracking community. This paper accompanies a new release of the MOTChallenge benchmark. Unlike the initial release, all videos of MOT16 have been carefully annotated following a consistent protocol. Moreover, it not only offers a significant increase in the number of labeled boxes, but also provides multiple object classes beside pedestrians and the level of visibility for every single object of interest.},
  archiveprefix    = {arXiv},
  comment          = {MOT},
  creationdate     = {2025-07-21T09:47:04},
  eprint           = {1603.00831},
  file             = {:Milan2016 - MOT16_ a Benchmark for Multi Object Tracking.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-07-21T09:47:12},
  primaryclass     = {cs.CV},
}

@Article{Min2023,
  author           = {Lejun Min and Junyan Jiang and Gus Xia and Jingwei Zhao},
  title            = {Polyffusion: A Diffusion Model for Polyphonic Score Generation with Internal and External Controls},
  year             = {2023},
  month            = jul,
  abstract         = {We propose Polyffusion, a diffusion model that generates polyphonic music scores by regarding music as image-like piano roll representations. The model is capable of controllable music generation with two paradigms: internal control and external control. Internal control refers to the process in which users pre-define a part of the music and then let the model infill the rest, similar to the task of masked music generation (or music inpainting). External control conditions the model with external yet related information, such as chord, texture, or other features, via the cross-attention mechanism. We show that by using internal and external controls, Polyffusion unifies a wide range of music creation tasks, including melody generation given accompaniment, accompaniment generation given melody, arbitrary music segment inpainting, and music arrangement given chords or textures. Experimental results show that our model significantly outperforms existing Transformer and sampling-based baselines, and using pre-trained disentangled representations as external conditions yields more effective controls.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:10:50},
  doi              = {10.48550/arxiv.2307.10304},
  eprint           = {2307.10304},
  file             = {:Min2023 - Polyffusion_ a Diffusion Model for Polyphonic Score Generation with Internal and External Controls.pdf:PDF},
  keywords         = {cs.SD, cs.LG, eess.AS},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.SD},
}

@Article{Pan2024,
  author           = {Chenbin Pan and Burhaneddin Yaman and Tommaso Nesti and Abhirup Mallik and Alessandro G Allievi and Senem Velipasalar and Liu Ren},
  journal          = {CVPR2024},
  title            = {VLP: Vision Language Planning for Autonomous Driving},
  year             = {2024},
  month            = jan,
  abstract         = {Autonomous driving is a complex and challenging task that aims at safe motion planning through scene understanding and reasoning. While vision-only autonomous driving methods have recently achieved notable performance, through enhanced scene understanding, several key issues, including lack of reasoning, low generalization performance and long-tail scenarios, still need to be addressed. In this paper, we present VLP, a novel Vision-Language-Planning framework that exploits language models to bridge the gap between linguistic understanding and autonomous driving. VLP enhances autonomous driving systems by strengthening both the source memory foundation and the self-driving car's contextual understanding. VLP achieves state-of-the-art end-to-end planning performance on the challenging NuScenes dataset by achieving 35.9\% and 60.5\% reduction in terms of average L2 error and collision rates, respectively, compared to the previous best method. Moreover, VLP shows improved performance in challenging long-tail scenarios and strong generalization capabilities when faced with new urban environments.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:12:56},
  doi              = {10.48550/arxiv.2401.05577},
  eprint           = {2401.05577},
  file             = {:Pan2024 - VLP_ Vision Language Planning for Autonomous Driving.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{Philion2020,
  author           = {Jonah Philion and Sanja Fidler},
  title            = {Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D},
  year             = {2020},
  month            = aug,
  abstract         = {The goal of perception for autonomous vehicles is to extract semantic representations from multiple sensors and fuse these representations into a single "bird's-eye-view" coordinate frame for consumption by motion planning. We propose a new end-to-end architecture that directly extracts a bird's-eye-view representation of a scene given image data from an arbitrary number of cameras. The core idea behind our approach is to "lift" each image individually into a frustum of features for each camera, then "splat" all frustums into a rasterized bird's-eye-view grid. By training on the entire camera rig, we provide evidence that our model is able to learn not only how to represent images but how to fuse predictions from all cameras into a single cohesive representation of the scene while being robust to calibration error. On standard bird's-eye-view tasks such as object segmentation and map segmentation, our model outperforms all baselines and prior work. In pursuit of the goal of learning dense representations for motion planning, we show that the representations inferred by our model enable interpretable end-to-end motion planning by "shooting" template trajectories into a bird's-eye-view cost map output by our network. We benchmark our approach against models that use oracle depth from lidar. Project page with code: https://nv-tlabs.github.io/lift-splat-shoot .},
  archiveprefix    = {arXiv},
  creationdate     = {2024-09-25T13:17:09},
  doi              = {10.48550/arxiv.2008.05711},
  eprint           = {2008.05711},
  file             = {:Philion2020 - Lift, Splat, Shoot_ Encoding Images from Arbitrary Camera Rigs by Implicitly Unprojecting to 3D.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{Pittner2024,
  author           = {Maximilian Pittner and Joel Janai and Alexandru P. Condurache},
  title            = {LaneCPP: Continuous 3D Lane Detection using Physical Priors},
  year             = {2024},
  month            = jun,
  abstract         = {Monocular 3D lane detection has become a fundamental problem in the context of autonomous driving, which comprises the tasks of finding the road surface and locating lane markings. One major challenge lies in a flexible but robust line representation capable of modeling complex lane structures, while still avoiding unpredictable behavior. While previous methods rely on fully data-driven approaches, we instead introduce a novel approach LaneCPP that uses a continuous 3D lane detection model leveraging physical prior knowledge about the lane structure and road geometry. While our sophisticated lane model is capable of modeling complex road structures, it also shows robust behavior since physical constraints are incorporated by means of a regularization scheme that can be analytically applied to our parametric representation. Moreover, we incorporate prior knowledge about the road geometry into the 3D feature space by modeling geometry-aware spatial features, guiding the network to learn an internal road surface representation. In our experiments, we show the benefits of our contributions and prove the meaningfulness of using priors to make 3D lane detection more robust. The results show that LaneCPP achieves state-of-the-art performance in terms of F-Score and geometric errors.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:14:38},
  doi              = {10.48550/arxiv.2406.08381},
  eprint           = {2406.08381},
  file             = {:Pittner2024 - LaneCPP_ Continuous 3D Lane Detection Using Physical Priors.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{Redmon2015,
  author           = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  title            = {You Only Look Once: Unified, Real-Time Object Detection},
  year             = {2015},
  month            = jun,
  abstract         = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  archiveprefix    = {arXiv},
  comment          = {YOLO},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2025-03-05T21:33:17},
  doi              = {10.48550/ARXIV.1506.02640},
  eprint           = {1506.02640},
  file             = {:Redmon2015 - You Only Look Once_ Unified, Real Time Object Detection.pdf:PDF:http\://arxiv.org/pdf/1506.02640v5},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  modificationdate = {2025-03-05T21:48:21},
  primaryclass     = {cs.CV},
  priority         = {prio1},
  publisher        = {arXiv},
}

@Article{Redmon2016,
  author           = {Redmon, Joseph and Farhadi, Ali},
  title            = {YOLO9000: Better, Faster, Stronger},
  year             = {2016},
  month            = dec,
  abstract         = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
  archiveprefix    = {arXiv},
  comment          = {YOLO9000},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2025-03-07T20:31:09},
  doi              = {10.48550/ARXIV.1612.08242},
  eprint           = {1612.08242},
  file             = {:Redmon2016 - YOLO9000_ Better, Faster, Stronger.pdf:PDF:http\://arxiv.org/pdf/1612.08242v1},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  modificationdate = {2025-03-07T20:31:17},
  primaryclass     = {cs.CV},
  publisher        = {arXiv},
}

@Article{Ren2015,
  author           = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  title            = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
  year             = {2015},
  month            = jun,
  abstract         = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  archiveprefix    = {arXiv},
  comment          = {Faster R-CNN},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2025-03-07T14:14:22},
  doi              = {10.48550/ARXIV.1506.01497},
  eprint           = {1506.01497},
  file             = {:Ren2015 - Faster R CNN_ Towards Real Time Object Detection with Region Proposal Networks.pdf:PDF:http\://arxiv.org/pdf/1506.01497v3},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  modificationdate = {2025-03-07T14:14:32},
  primaryclass     = {cs.CV},
  priority         = {prio1},
  publisher        = {arXiv},
}

@Article{Ristani2016,
  author           = {Ergys Ristani and Francesco Solera and Roger S. Zou and Rita Cucchiara and Carlo Tomasi},
  title            = {Performance Measures and a Data Set for Multi-Target, Multi-Camera Tracking},
  year             = {2016},
  month            = sep,
  abstract         = {To help accelerate progress in multi-target, multi-camera tracking systems, we present (i) a new pair of precision-recall measures of performance that treats errors of all types uniformly and emphasizes correct identification over sources of error; (ii) the largest fully-annotated and calibrated data set to date with more than 2 million frames of 1080p, 60fps video taken by 8 cameras observing more than 2,700 identities over 85 minutes; and (iii) a reference software system as a comparison baseline. We show that (i) our measures properly account for bottom-line identity match performance in the multi-camera setting; (ii) our data set poses realistic challenges to current trackers; and (iii) the performance of our system is comparable to the state of the art.},
  archiveprefix    = {arXiv},
  comment          = {IDF1},
  creationdate     = {2025-07-21T09:46:03},
  eprint           = {1609.01775},
  file             = {:- Performance Measures and a Data Set for Multi Target, Multi Camera Tracking.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-07-21T09:46:20},
  primaryclass     = {cs.CV},
}

@Article{Ronneberger2015,
  author           = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  title            = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
  year             = {2015},
  month            = may,
  abstract         = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  archiveprefix    = {arXiv},
  comment          = {U-Net},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2025-03-07T16:55:22},
  doi              = {10.48550/ARXIV.1505.04597},
  eprint           = {1505.04597},
  file             = {:Ronneberger2015 - U Net_ Convolutional Networks for Biomedical Image Segmentation.pdf:PDF:http\://arxiv.org/pdf/1505.04597v1},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  modificationdate = {2025-03-07T16:55:32},
  primaryclass     = {cs.CV},
  priority         = {prio1},
  publisher        = {arXiv},
}

@Article{Ruppel2022,
  author           = {Felicia Ruppel and Florian Faion and Claudius GlÃ¤ser and Klaus Dietmayer},
  title            = {Transformers for Object Detection in Large Point Clouds},
  year             = {2022},
  month            = sep,
  abstract         = {We present TransLPC, a novel detection model for large point clouds that is based on a transformer architecture. While object detection with transformers has been an active field of research, it has proved difficult to apply such models to point clouds that span a large area, e.g. those that are common in autonomous driving, with lidar or radar data. TransLPC is able to remedy these issues: The structure of the transformer model is modified to allow for larger input sequence lengths, which are sufficient for large point clouds. Besides this, we propose a novel query refinement technique to improve detection accuracy, while retaining a memory-friendly number of transformer decoder queries. The queries are repositioned between layers, moving them closer to the bounding box they are estimating, in an efficient manner. This simple technique has a significant effect on detection accuracy, which is evaluated on the challenging nuScenes dataset on real-world lidar data. Besides this, the proposed method is compatible with existing transformer-based solutions that require object detection, e.g. for joint multi-object tracking and detection, and enables them to be used in conjunction with large point clouds.},
  archiveprefix    = {arXiv},
  comment          = {Bosch paper},
  creationdate     = {2024-10-22T10:02:53},
  doi              = {10.48550/arxiv.2209.15258},
  eprint           = {2209.15258},
  file             = {:Ruppel2022 - Transformers for Object Detection in Large Point Clouds.pdf:PDF},
  keywords         = {cs.CV, cs.LG},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
  priority         = {prio1},
}

@Article{Russakovsky2014,
  author           = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
  title            = {ImageNet Large Scale Visual Recognition Challenge},
  year             = {2014},
  month            = sep,
  abstract         = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
  archiveprefix    = {arXiv},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2025-01-28T20:01:43},
  doi              = {10.48550/ARXIV.1409.0575},
  eprint           = {1409.0575},
  file             = {:Russakovsky2014 - ImageNet Large Scale Visual Recognition Challenge.pdf:PDF},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, I.4.8; I.5.2},
  modificationdate = {2025-01-28T22:54:48},
  primaryclass     = {cs.CV},
  publisher        = {arXiv},
}

@Article{Sandler2018,
  author           = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  journal          = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 4510-4520},
  title            = {MobileNetV2: Inverted Residuals and Linear Bottlenecks},
  year             = {2018},
  month            = jan,
  abstract         = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters},
  archiveprefix    = {arXiv},
  comment          = {MobileNet v2},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2025-03-02T20:28:26},
  doi              = {10.48550/ARXIV.1801.04381},
  eprint           = {1801.04381},
  file             = {:Sandler2018 - MobileNetV2_ Inverted Residuals and Linear Bottlenecks.pdf:PDF:http\://arxiv.org/pdf/1801.04381v4},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  modificationdate = {2025-03-02T20:28:50},
  primaryclass     = {cs.CV},
  priority         = {prio1},
  publisher        = {arXiv},
}

@Article{Schmidhuber2014,
  author           = {Juergen Schmidhuber},
  journal          = {Neural Networks, Vol 61, pp 85-117, Jan 2015},
  title            = {Deep Learning in Neural Networks: An Overview},
  year             = {2014},
  month            = apr,
  abstract         = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning & evolutionary computation, and indirect search for short programs encoding deep and large networks.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-09-24T16:29:59},
  doi              = {10.1016/j.neunet.2014.09.003},
  eprint           = {1404.7828},
  file             = {:Schmidhuber2014 - Deep Learning in Neural Networks_ an Overview.pdf:PDF},
  keywords         = {cs.NE, cs.LG},
  modificationdate = {2024-10-15T09:03:24},
  primaryclass     = {cs.NE},
}

@Article{Shao2023,
  author           = {Hao Shao and Letian Wang and Ruobing Chen and Steven L. Waslander and Hongsheng Li and Yu Liu},
  title            = {ReasonNet: End-to-End Driving with Temporal and Global Reasoning},
  year             = {2023},
  month            = may,
  abstract         = {The large-scale deployment of autonomous vehicles is yet to come, and one of the major remaining challenges lies in urban dense traffic scenarios. In such cases, it remains challenging to predict the future evolution of the scene and future behaviors of objects, and to deal with rare adverse events such as the sudden appearance of occluded objects. In this paper, we present ReasonNet, a novel end-to-end driving framework that extensively exploits both temporal and global information of the driving scene. By reasoning on the temporal behavior of objects, our method can effectively process the interactions and relationships among features in different frames. Reasoning about the global information of the scene can also improve overall perception performance and benefit the detection of adverse events, especially the anticipation of potential danger from occluded objects. For comprehensive evaluation on occlusion events, we also release publicly a driving simulation benchmark DriveOcclusionSim consisting of diverse occlusion events. We conduct extensive experiments on multiple CARLA benchmarks, where our model outperforms all prior methods, ranking first on the sensor track of the public CARLA Leaderboard.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:09:45},
  doi              = {10.48550/arxiv.2305.10507},
  eprint           = {2305.10507},
  file             = {:Shao2023 - ReasonNet_ End to End Driving with Temporal and Global Reasoning.pdf:PDF},
  keywords         = {cs.CV, cs.AI},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{Simonyan2014,
  author           = {Simonyan, Karen and Zisserman, Andrew},
  title            = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  year             = {2014},
  month            = sep,
  abstract         = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  archiveprefix    = {arXiv},
  comment          = {VGG-16},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2025-02-28T12:06:04},
  doi              = {10.48550/ARXIV.1409.1556},
  eprint           = {1409.1556},
  file             = {:Simonyan2014 - Very Deep Convolutional Networks for Large Scale Image Recognition.pdf:PDF:http\://arxiv.org/pdf/1409.1556v6},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  modificationdate = {2025-02-28T12:19:39},
  primaryclass     = {cs.CV},
  priority         = {prio1},
  publisher        = {arXiv},
}

@Article{Sun2020,
  author           = {Peize Sun and Rufeng Zhang and Yi Jiang and Tao Kong and Chenfeng Xu and Wei Zhan and Masayoshi Tomizuka and Lei Li and Zehuan Yuan and Changhu Wang and Ping Luo},
  title            = {Sparse R-CNN: End-to-End Object Detection with Learnable Proposals},
  year             = {2020},
  month            = nov,
  abstract         = {We present Sparse R-CNN, a purely sparse method for object detection in images. Existing works on object detection heavily rely on dense object candidates, such as $k$ anchor boxes pre-defined on all grids of image feature map of size $H\times W$. In our method, however, a fixed sparse set of learned object proposals, total length of $N$, are provided to object recognition head to perform classification and location. By eliminating $HWk$ (up to hundreds of thousands) hand-designed object candidates to $N$ (e.g. 100) learnable proposals, Sparse R-CNN completely avoids all efforts related to object candidates design and many-to-one label assignment. More importantly, final predictions are directly output without non-maximum suppression post-procedure. Sparse R-CNN demonstrates accuracy, run-time and training convergence performance on par with the well-established detector baselines on the challenging COCO dataset, e.g., achieving 45.0 AP in standard $3\times$ training schedule and running at 22 fps using ResNet-50 FPN model. We hope our work could inspire re-thinking the convention of dense prior in object detectors. The code is available at: https://github.com/PeizeSun/SparseR-CNN.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-31T09:06:58},
  doi              = {10.48550/arxiv.2011.12450},
  eprint           = {2011.12450},
  file             = {:Sun2020 - Sparse R CNN_ End to End Object Detection with Learnable Proposals.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{Sun2024,
  author           = {Wenchao Sun and Xuewu Lin and Yining Shi and Chuang Zhang and Haoran Wu and Sifa Zheng},
  title            = {SparseDrive: End-to-End Autonomous Driving via Sparse Scene Representation},
  year             = {2024},
  month            = may,
  abstract         = {The well-established modular autonomous driving system is decoupled into different standalone tasks, e.g. perception, prediction and planning, suffering from information loss and error accumulation across modules. In contrast, end-to-end paradigms unify multi-tasks into a fully differentiable framework, allowing for optimization in a planning-oriented spirit. Despite the great potential of end-to-end paradigms, both the performance and efficiency of existing methods are not satisfactory, particularly in terms of planning safety. We attribute this to the computationally expensive BEV (bird's eye view) features and the straightforward design for prediction and planning. To this end, we explore the sparse representation and review the task design for end-to-end autonomous driving, proposing a new paradigm named SparseDrive. Concretely, SparseDrive consists of a symmetric sparse perception module and a parallel motion planner. The sparse perception module unifies detection, tracking and online mapping with a symmetric model architecture, learning a fully sparse representation of the driving scene. For motion prediction and planning, we review the great similarity between these two tasks, leading to a parallel design for motion planner. Based on this parallel design, which models planning as a multi-modal problem, we propose a hierarchical planning selection strategy , which incorporates a collision-aware rescore module, to select a rational and safe trajectory as the final planning output. With such effective designs, SparseDrive surpasses previous state-of-the-arts by a large margin in performance of all tasks, while achieving much higher training and inference efficiency. Code will be avaliable at https://github.com/swc-17/SparseDrive for facilitating future research.},
  archiveprefix    = {arXiv},
  doi              = {10.48550/arxiv.2405.19620},
  eprint           = {2405.19620},
  file             = {:Sun2024 - SparseDrive_ End to End Autonomous Driving Via Sparse Scene Representation.pdf:PDF},
  groups           = {Reading Group Sparse Query},
  keywords         = {cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{Szegedy2014,
  author           = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  title            = {Going Deeper with Convolutions},
  year             = {2014},
  month            = sep,
  abstract         = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
  archiveprefix    = {arXiv},
  comment          = {Inception, GoogLeNet},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2025-02-28T13:46:30},
  doi              = {10.48550/ARXIV.1409.4842},
  eprint           = {1409.4842},
  file             = {:Szegedy2014 - Going Deeper with Convolutions.pdf:PDF:http\://arxiv.org/pdf/1409.4842v1},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  modificationdate = {2025-02-28T15:01:41},
  primaryclass     = {cs.CV},
  priority         = {prio1},
  publisher        = {arXiv},
}

@Article{Tahiraj2025,
  author           = {Ilir Tahiraj and Jeremialie Swadiryus and Felix Fent and Markus Lienkamp},
  title            = {Cal or No Cal? -- Real-Time Miscalibration Detection of LiDAR and Camera Sensors},
  year             = {2025},
  month            = mar,
  abstract         = {The goal of extrinsic calibration is the alignment of sensor data to ensure an accurate representation of the surroundings and enable sensor fusion applications. From a safety perspective, sensor calibration is a key enabler of autonomous driving. In the current state of the art, a trend from target-based offline calibration towards targetless online calibration can be observed. However, online calibration is subject to strict real-time and resource constraints which are not met by state-of-the-art methods. This is mainly due to the high number of parameters to estimate, the reliance on geometric features, or the dependence on specific vehicle maneuvers. To meet these requirements and ensure the vehicle's safety at any time, we propose a miscalibration detection framework that shifts the focus from the direct regression of calibration parameters to a binary classification of the calibration state, i.e., calibrated or miscalibrated. Therefore, we propose a contrastive learning approach that compares embedded features in a latent space to classify the calibration state of two different sensor modalities. Moreover, we provide a comprehensive analysis of the feature embeddings and challenging calibration errors that highlight the performance of our approach. As a result, our method outperforms the current state-of-the-art in terms of detection performance, inference time, and resource demand. The code is open source and available on https://github.com/TUMFTM/MiscalibrationDetection.},
  archiveprefix    = {arXiv},
  creationdate     = {2025-04-08T10:44:37},
  eprint           = {2504.01040},
  file             = {:Tahiraj2025 - Cal or No Cal_ Real Time Miscalibration Detection of LiDAR and Camera Sensors.pdf:PDF},
  groups           = {check next},
  keywords         = {cs.CV, cs.RO},
  modificationdate = {2025-04-08T10:44:59},
  primaryclass     = {cs.CV},
}

@Article{Tan2019,
  author           = {Tan, Mingxing and Le, Quoc V.},
  journal          = {International Conference on Machine Learning, 2019},
  title            = {EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},
  year             = {2019},
  month            = may,
  abstract         = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
  archiveprefix    = {arXiv},
  comment          = {EfficientNet},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2025-03-02T20:39:29},
  doi              = {10.48550/ARXIV.1905.11946},
  eprint           = {1905.11946},
  file             = {:Tan2019 - EfficientNet_ Rethinking Model Scaling for Convolutional Neural Networks.pdf:PDF:http\://arxiv.org/pdf/1905.11946v5},
  keywords         = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences},
  modificationdate = {2025-03-02T20:39:38},
  primaryclass     = {cs.LG},
  priority         = {prio1},
  publisher        = {arXiv},
}

@Article{Vaswani2017,
  author           = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  title            = {Attention Is All You Need},
  year             = {2017},
  month            = jun,
  abstract         = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix    = {arXiv},
  comment          = {basis for transformer models},
  creationdate     = {2024-09-17T11:23:11},
  doi              = {10.48550/arxiv.1706.03762},
  eprint           = {1706.03762},
  file             = {:Vaswani2017 - Attention Is All You Need.pdf:PDF;:files/Vaswani2017 - Attention Is All You Need.pdf:PDF},
  keywords         = {cs.CL, cs.LG},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CL},
  priority         = {prio1},
  readstatus       = {read},
}

@Article{Wang2017,
  author           = {Haohan Wang and Bhiksha Raj},
  title            = {On the Origin of Deep Learning},
  year             = {2017},
  month            = feb,
  abstract         = {This paper is a review of the evolutionary history of deep learning models. It covers from the genesis of neural networks when associationism modeling of the brain is studied, to the models that dominate the last decade of research in deep learning like convolutional neural networks, deep belief networks, and recurrent neural networks. In addition to a review of these models, this paper primarily focuses on the precedents of the models above, examining how the initial ideas are assembled to construct the early models and how these preliminary models are developed into their current forms. Many of these evolutionary paths last more than half a century and have a diversity of directions. For example, CNN is built on prior knowledge of biological vision system; DBN is evolved from a trade-off of modeling power and computation complexity of graphical models and many nowadays models are neural counterparts of ancient linear models. This paper reviews these evolutionary paths and offers a concise thought flow of how these models are developed, and aims to provide a thorough background for deep learning. More importantly, along with the path, this paper summarizes the gist behind these milestones and proposes many directions to guide the future research of deep learning.},
  archiveprefix    = {arXiv},
  doi              = {10.48550/arxiv.1702.07800},
  eprint           = {1702.07800},
  file             = {:Wang2017 - On the Origin of Deep Learning.pdf:PDF;:Wang2017 - On the Origin of Deep Learning.pdf:PDF},
  keywords         = {cs.LG, cs.NE, stat.ML},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.LG},
}

@Article{Wang2019,
  author           = {Zhongdao Wang and Liang Zheng and Yixuan Liu and Yali Li and Shengjin Wang},
  title            = {Towards Real-Time Multi-Object Tracking},
  year             = {2019},
  month            = sep,
  abstract         = {Modern multiple object tracking (MOT) systems usually follow the \emph{tracking-by-detection} paradigm. It has 1) a detection model for target localization and 2) an appearance embedding model for data association. Having the two models separately executed might lead to efficiency problems, as the running time is simply a sum of the two steps without investigating potential structures that can be shared between them. Existing research efforts on real-time MOT usually focus on the association step, so they are essentially real-time association methods but not real-time MOT system. In this paper, we propose an MOT system that allows target detection and appearance embedding to be learned in a shared model. Specifically, we incorporate the appearance embedding model into a single-shot detector, such that the model can simultaneously output detections and the corresponding embeddings. We further propose a simple and fast association method that works in conjunction with the joint model. In both components the computation cost is significantly reduced compared with former MOT systems, resulting in a neat and fast baseline for future follow-ups on real-time MOT algorithm design. To our knowledge, this work reports the first (near) real-time MOT system, with a running speed of 22 to 40 FPS depending on the input resolution. Meanwhile, its tracking accuracy is comparable to the state-of-the-art trackers embodying separate detection and embedding (SDE) learning ($64.4\%$ MOTA \vs $66.1\%$ MOTA on MOT-16 challenge). Code and models are available at \url{https://github.com/Zhongdao/Towards-Realtime-MOT}.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-22T09:29:04},
  doi              = {10.48550/arxiv.1909.12605},
  eprint           = {1909.12605},
  file             = {:Wang2019 - Towards Real Time Multi Object Tracking.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{Wang2021,
  author           = {Yue Wang and Vitor Guizilini and Tianyuan Zhang and Yilun Wang and Hang Zhao and Justin Solomon},
  title            = {DETR3D: 3D Object Detection from Multi-view Images via 3D-to-2D Queries},
  year             = {2021},
  month            = oct,
  abstract         = {We introduce a framework for multi-camera 3D object detection. In contrast to existing works, which estimate 3D bounding boxes directly from monocular images or use depth prediction networks to generate input for 3D object detection from 2D information, our method manipulates predictions directly in 3D space. Our architecture extracts 2D features from multiple camera images and then uses a sparse set of 3D object queries to index into these 2D features, linking 3D positions to multi-view images using camera transformation matrices. Finally, our model makes a bounding box prediction per object query, using a set-to-set loss to measure the discrepancy between the ground-truth and the prediction. This top-down approach outperforms its bottom-up counterpart in which object bounding box prediction follows per-pixel depth estimation, since it does not suffer from the compounding error introduced by a depth prediction model. Moreover, our method does not require post-processing such as non-maximum suppression, dramatically improving inference speed. We achieve state-of-the-art performance on the nuScenes autonomous driving benchmark.},
  archiveprefix    = {arXiv},
  comment          = {DETR3D},
  doi              = {10.48550/arxiv.2110.06922},
  eprint           = {2110.06922},
  file             = {:Wang2021 - DETR3D_ 3D Object Detection from Multi View Images Via 3D to 2D Queries.pdf:PDF},
  groups           = {PMO AI},
  keywords         = {cs.CV, cs.AI, cs.LG, cs.RO},
  modificationdate = {2025-04-02T13:24:46},
  primaryclass     = {cs.CV},
  priority         = {prio1},
  readstatus       = {read},
}

@Article{Wang2022,
  author           = {Shihao Wang and Xiaohui Jiang and Ying Li},
  title            = {Focal-PETR: Embracing Foreground for Efficient Multi-Camera 3D Object Detection},
  year             = {2022},
  month            = dec,
  abstract         = {The dominant multi-camera 3D detection paradigm is based on explicit 3D feature construction, which requires complicated indexing of local image-view features via 3D-to-2D projection. Other methods implicitly introduce geometric positional encoding and perform global attention (e.g., PETR) to build the relationship between image tokens and 3D objects. The 3D-to-2D perspective inconsistency and global attention lead to a weak correlation between foreground tokens and queries, resulting in slow convergence. We propose Focal-PETR with instance-guided supervision and spatial alignment module to adaptively focus object queries on discriminative foreground regions. Focal-PETR additionally introduces a down-sampling strategy to reduce the consumption of global attention. Due to the highly parallelized implementation and down-sampling strategy, our model, without depth supervision, achieves leading performance on the large-scale nuScenes benchmark and a superior speed of 30 FPS on a single RTX3090 GPU. Extensive experiments show that our method outperforms PETR while consuming 3x fewer training hours. The code will be made publicly available.},
  archiveprefix    = {arXiv},
  creationdate     = {2025-01-24T14:19:18},
  doi              = {10.48550/arxiv.2212.05505},
  eprint           = {2212.05505},
  file             = {:Wang2022 - Focal PETR_ Embracing Foreground for Efficient Multi Camera 3D Object Detection.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-04-02T13:22:46},
  primaryclass     = {cs.CV},
  priority         = {prio1},
}

@Article{Wang2023,
  author           = {Wenhai Wang and Jiangwei Xie and ChuanYang Hu and Haoming Zou and Jianan Fan and Wenwen Tong and Yang Wen and Silei Wu and Hanming Deng and Zhiqi Li and Hao Tian and Lewei Lu and Xizhou Zhu and Xiaogang Wang and Yu Qiao and Jifeng Dai},
  title            = {DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving},
  year             = {2023},
  month            = dec,
  abstract         = {Large language models (LLMs) have opened up new possibilities for intelligent agents, endowing them with human-like thinking and cognitive abilities. In this work, we delve into the potential of large language models (LLMs) in autonomous driving (AD). We introduce DriveMLM, an LLM-based AD framework that can perform close-loop autonomous driving in realistic simulators. To this end, (1) we bridge the gap between the language decisions and the vehicle control commands by standardizing the decision states according to the off-the-shelf motion planning module. (2) We employ a multi-modal LLM (MLLM) to model the behavior planning module of a module AD system, which uses driving rules, user commands, and inputs from various sensors (e.g., camera, lidar) as input and makes driving decisions and provide explanations; This model can plug-and-play in existing AD systems such as Apollo for close-loop driving. (3) We design an effective data engine to collect a dataset that includes decision state and corresponding explanation annotation for model training and evaluation. We conduct extensive experiments and show that our model achieves 76.1 driving score on the CARLA Town05 Long, and surpasses the Apollo baseline by 4.7 points under the same settings, demonstrating the effectiveness of our model. We hope this work can serve as a baseline for autonomous driving with LLMs. Code and models shall be released at https://github.com/OpenGVLab/DriveMLM.},
  archiveprefix    = {arXiv},
  doi              = {10.48550/arxiv.2312.09245},
  eprint           = {2312.09245},
  file             = {:Wang2023 - DriveMLM_ Aligning Multi Modal Large Language Models with Behavioral Planning States for Autonomous Driving.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{Wang2023a,
  author           = {Shihao Wang and Yingfei Liu and Tiancai Wang and Ying Li and Xiangyu Zhang},
  title            = {Exploring Object-Centric Temporal Modeling for Efficient Multi-View 3D Object Detection},
  year             = {2023},
  month            = mar,
  abstract         = {In this paper, we propose a long-sequence modeling framework, named StreamPETR, for multi-view 3D object detection. Built upon the sparse query design in the PETR series, we systematically develop an object-centric temporal mechanism. The model is performed in an online manner and the long-term historical information is propagated through object queries frame by frame. Besides, we introduce a motion-aware layer normalization to model the movement of the objects. StreamPETR achieves significant performance improvements only with negligible computation cost, compared to the single-frame baseline. On the standard nuScenes benchmark, it is the first online multi-view method that achieves comparable performance (67.6% NDS & 65.3% AMOTA) with lidar-based methods. The lightweight version realizes 45.0% mAP and 31.7 FPS, outperforming the state-of-the-art method (SOLOFusion) by 2.3% mAP and 1.8x faster FPS. Code has been available at https://github.com/exiawsh/StreamPETR.git.},
  archiveprefix    = {arXiv},
  comment          = {StreamPETR},
  doi              = {10.48550/arxiv.2303.11926},
  eprint           = {2303.11926},
  file             = {:Wang2023a - Exploring Object Centric Temporal Modeling for Efficient Multi View 3D Object Detection.pdf:PDF},
  groups           = {Reading Group Sparse Query},
  keywords         = {cs.CV},
  modificationdate = {2025-07-29T14:38:58},
  primaryclass     = {cs.CV},
  priority         = {prio1},
  readstatus       = {read},
}

@Article{Wang2023b,
  author           = {Haiyang Wang and Hao Tang and Shaoshuai Shi and Aoxue Li and Zhenguo Li and Bernt Schiele and Liwei Wang},
  title            = {UniTR: A Unified and Efficient Multi-Modal Transformer for Bird's-Eye-View Representation},
  year             = {2023},
  month            = aug,
  abstract         = {Jointly processing information from multiple sensors is crucial to achieving accurate and robust perception for reliable autonomous driving systems. However, current 3D perception research follows a modality-specific paradigm, leading to additional computation overheads and inefficient collaboration between different sensor data. In this paper, we present an efficient multi-modal backbone for outdoor 3D perception named UniTR, which processes a variety of modalities with unified modeling and shared parameters. Unlike previous works, UniTR introduces a modality-agnostic transformer encoder to handle these view-discrepant sensor data for parallel modal-wise representation learning and automatic cross-modal interaction without additional fusion steps. More importantly, to make full use of these complementary sensor types, we present a novel multi-modal integration strategy by both considering semantic-abundant 2D perspective and geometry-aware 3D sparse neighborhood relations. UniTR is also a fundamentally task-agnostic backbone that naturally supports different 3D perception tasks. It sets a new state-of-the-art performance on the nuScenes benchmark, achieving +1.1 NDS higher for 3D object detection and +12.0 higher mIoU for BEV map segmentation with lower inference latency. Code will be available at https://github.com/Haiyang-W/UniTR .},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:11:45},
  doi              = {10.48550/arxiv.2308.07732},
  eprint           = {2308.07732},
  file             = {:Wang2023b - UniTR_ a Unified and Efficient Multi Modal Transformer for Bird's Eye View Representation.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{Wang2024,
  author           = {Shuo Wang and Chunlong Xia and Feng Lv and Yifeng Shi},
  title            = {RT-DETRv3: Real-time End-to-End Object Detection with Hierarchical Dense Positive Supervision},
  year             = {2024},
  month            = sep,
  abstract         = {RT-DETR is the first real-time end-to-end transformer-based object detector. Its efficiency comes from the framework design and the Hungarian matching. However, compared to dense supervision detectors like the YOLO series, the Hungarian matching provides much sparser supervision, leading to insufficient model training and difficult to achieve optimal results. To address these issues, we proposed a hierarchical dense positive supervision method based on RT-DETR, named RT-DETRv3. Firstly, we introduce a CNN-based auxiliary branch that provides dense supervision that collaborates with the original decoder to enhance the encoder feature representation. Secondly, to address insufficient decoder training, we propose a novel learning strategy involving self-attention perturbation. This strategy diversifies label assignment for positive samples across multiple query groups, thereby enriching positive supervisions. Additionally, we introduce a shared-weight decoder branch for dense positive supervision to ensure more high-quality queries matching each ground truth. Notably, all aforementioned modules are training-only. We conduct extensive experiments to demonstrate the effectiveness of our approach on COCO val2017. RT-DETRv3 significantly outperforms existing real-time detectors, including the RT-DETR series and the YOLO series. For example, RT-DETRv3-R18 achieves 48.1% AP (+1.6%/+1.4%) compared to RT-DETR-R18/RT-DETRv2-R18 while maintaining the same latency. Meanwhile, it requires only half of epochs to attain a comparable performance. Furthermore, RT-DETRv3-R101 can attain an impressive 54.6% AP outperforming YOLOv10-X. Code will be released soon.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:15:03},
  doi              = {10.48550/arxiv.2409.08475},
  eprint           = {2409.08475},
  file             = {:Wang2024 - RT DETRv3_ Real Time End to End Object Detection with Hierarchical Dense Positive Supervision.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{Wang2024a,
  author           = {Xiyang Wang and Shouzheng Qi and Jieyou Zhao and Hangning Zhou and Siyu Zhang and Guoan Wang and Kai Tu and Songlin Guo and Jianbo Zhao and Jian Li and Mu Yang},
  title            = {MCTrack: A Unified 3D Multi-Object Tracking Framework for Autonomous Driving},
  year             = {2024},
  month            = sep,
  abstract         = {This paper introduces MCTrack, a new 3D multi-object tracking method that achieves state-of-the-art (SOTA) performance across KITTI, nuScenes, and Waymo datasets. Addressing the gap in existing tracking paradigms, which often perform well on specific datasets but lack generalizability, MCTrack offers a unified solution. Additionally, we have standardized the format of perceptual results across various datasets, termed BaseVersion, facilitating researchers in the field of multi-object tracking (MOT) to concentrate on the core algorithmic development without the undue burden of data preprocessing. Finally, recognizing the limitations of current evaluation metrics, we propose a novel set that assesses motion information output, such as velocity and acceleration, crucial for downstream tasks. The source codes of the proposed method are available at this link: https://github.com/megvii-research/MCTrackhttps://github.com/megvii-research/MCTrack},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:20:51},
  doi              = {10.48550/arxiv.2409.16149},
  eprint           = {2409.16149},
  file             = {:http\://arxiv.org/pdf/2409.16149v2:PDF},
  groups           = {Reading Group Sparse Query},
  keywords         = {cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{Wolters2024,
  author           = {Philipp Wolters and Johannes Gilg and Torben Teepe and Fabian Herzog and Anouar Laouichi and Martin Hofmann and Gerhard Rigoll},
  title            = {Unleashing HyDRa: Hybrid Fusion, Depth Consistency and Radar for Unified 3D Perception},
  year             = {2024},
  month            = mar,
  abstract         = {Low-cost, vision-centric 3D perception systems for autonomous driving have made significant progress in recent years, narrowing the gap to expensive LiDAR-based methods. The primary challenge in becoming a fully reliable alternative lies in robust depth prediction capabilities, as camera-based systems struggle with long detection ranges and adverse lighting and weather conditions. In this work, we introduce HyDRa, a novel camera-radar fusion architecture for diverse 3D perception tasks. Building upon the principles of dense BEV (Bird's Eye View)-based architectures, HyDRa introduces a hybrid fusion approach to combine the strengths of complementary camera and radar features in two distinct representation spaces. Our Height Association Transformer module leverages radar features already in the perspective view to produce more robust and accurate depth predictions. In the BEV, we refine the initial sparse representation by a Radar-weighted Depth Consistency. HyDRa achieves a new state-of-the-art for camera-radar fusion of 64.2 NDS (+1.8) and 58.4 AMOTA (+1.5) on the public nuScenes dataset. Moreover, our new semantically rich and spatially accurate BEV features can be directly converted into a powerful occupancy representation, beating all previous camera-based methods on the Occ3D benchmark by an impressive 3.7 mIoU. Code and models are available at https://github.com/phi-wol/hydra.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:13:23},
  doi              = {10.48550/arxiv.2403.07746},
  eprint           = {2403.07746},
  file             = {:Wolters2024 - Unleashing HyDRa_ Hybrid Fusion, Depth Consistency and Radar for Unified 3D Perception.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{Xu2023,
  author           = {Sheng Xu and Yanjing Li and Mingbao Lin and Peng Gao and Guodong Guo and Jinhu Lu and Baochang Zhang},
  title            = {Q-DETR: An Efficient Low-Bit Quantized Detection Transformer},
  year             = {2023},
  month            = apr,
  abstract         = {The recent detection transformer (DETR) has advanced object detection, but its application on resource-constrained devices requires massive computation and memory resources. Quantization stands out as a solution by representing the network in low-bit parameters and operations. However, there is a significant performance drop when performing low-bit quantized DETR (Q-DETR) with existing quantization methods. We find that the bottlenecks of Q-DETR come from the query information distortion through our empirical analyses. This paper addresses this problem based on a distribution rectification distillation (DRD). We formulate our DRD as a bi-level optimization problem, which can be derived by generalizing the information bottleneck (IB) principle to the learning of Q-DETR. At the inner level, we conduct a distribution alignment for the queries to maximize the self-information entropy. At the upper level, we introduce a new foreground-aware query matching scheme to effectively transfer the teacher information to distillation-desired features to minimize the conditional information entropy. Extensive experimental results show that our method performs much better than prior arts. For example, the 4-bit Q-DETR can theoretically accelerate DETR with ResNet-50 backbone by 6.6x and achieve 39.4% AP, with only 2.6% performance gaps than its real-valued counterpart on the COCO dataset.},
  archiveprefix    = {arXiv},
  creationdate     = {2025-04-24T12:03:28},
  eprint           = {2304.00253},
  file             = {:- Q DETR_ an Efficient Low Bit Quantized Detection Transformer.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-04-24T12:03:56},
  primaryclass     = {cs.CV},
  priority         = {prio1},
}

@Article{Yan2023,
  author           = {Junjie Yan and Yingfei Liu and Jianjian Sun and Fan Jia and Shuailin Li and Tiancai Wang and Xiangyu Zhang},
  title            = {Cross Modal Transformer: Towards Fast and Robust 3D Object Detection},
  year             = {2023},
  month            = jan,
  abstract         = {In this paper, we propose a robust 3D detector, named Cross Modal Transformer (CMT), for end-to-end 3D multi-modal detection. Without explicit view transformation, CMT takes the image and point clouds tokens as inputs and directly outputs accurate 3D bounding boxes. The spatial alignment of multi-modal tokens is performed by encoding the 3D points into multi-modal features. The core design of CMT is quite simple while its performance is impressive. It achieves 74.1\% NDS (state-of-the-art with single model) on nuScenes test set while maintaining fast inference speed. Moreover, CMT has a strong robustness even if the LiDAR is missing. Code is released at https://github.com/junjie18/CMT.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-11-01T09:18:42},
  doi              = {10.48550/arxiv.2301.01283},
  eprint           = {2301.01283},
  file             = {:Yan2023 - Cross Modal Transformer_ Towards Fast and Robust 3D Object Detection.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{Yu2023,
  author           = {En Yu and Tiancai Wang and Zhuoling Li and Yuang Zhang and Xiangyu Zhang and Wenbing Tao},
  title            = {MOTRv3: Release-Fetch Supervision for End-to-End Multi-Object Tracking},
  year             = {2023},
  month            = may,
  abstract         = {Although end-to-end multi-object trackers like MOTR enjoy the merits of simplicity, they suffer from the conflict between detection and association seriously, resulting in unsatisfactory convergence dynamics. While MOTRv2 partly addresses this problem, it demands an additional detection network for assistance. In this work, we serve as the first to reveal that this conflict arises from the unfair label assignment between detect queries and track queries during training, where these detect queries recognize targets and track queries associate them. Based on this observation, we propose MOTRv3, which balances the label assignment process using the developed release-fetch supervision strategy. In this strategy, labels are first released for detection and gradually fetched back for association. Besides, another two strategies named pseudo label distillation and track group denoising are designed to further improve the supervision for detection and association. Without the assistance of an extra detection network during inference, MOTRv3 achieves impressive performance across diverse benchmarks, e.g., MOT17, DanceTrack.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T13:20:36},
  doi              = {10.48550/arxiv.2305.14298},
  eprint           = {2305.14298},
  file             = {:Yu2023 - MOTRv3_ Release Fetch Supervision for End to End Multi Object Tracking.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{Yu2024,
  author           = {Zichen Yu and Quanli Liu and Wei Wang and Liyong Zhang and Xiaoguang Zhao},
  title            = {PolarBEVDet: Exploring Polar Representation for Multi-View 3D Object Detection in Bird's-Eye-View},
  year             = {2024},
  month            = aug,
  abstract         = {Recently, LSS-based multi-view 3D object detection provides an economical and deployment-friendly solution for autonomous driving. However, all the existing LSS-based methods transform multi-view image features into a Cartesian Bird's-Eye-View(BEV) representation, which does not take into account the non-uniform image information distribution and hardly exploits the view symmetry. In this paper, in order to adapt the image information distribution and preserve the view symmetry by regular convolution, we propose to employ the polar BEV representation to substitute the Cartesian BEV representation. To achieve this, we elaborately tailor three modules: a polar view transformer to generate the polar BEV representation, a polar temporal fusion module for fusing historical polar BEV features and a polar detection head to predict the polar-parameterized representation of the object. In addition, we design a 2D auxiliary detection head and a spatial attention enhancement module to improve the quality of feature extraction in perspective view and BEV, respectively. Finally, we integrate the above improvements into a novel multi-view 3D object detector, PolarBEVDet. Experiments on nuScenes show that PolarBEVDet achieves the superior performance. The code is available at https://github.com/Yzichen/PolarBEVDet.git.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:15:14},
  doi              = {10.48550/arxiv.2408.16200},
  eprint           = {2408.16200},
  file             = {:Yu2024 - PolarBEVDet_ Exploring Polar Representation for Multi View 3D Object Detection in Bird's Eye View.pdf:PDF},
  keywords         = {cs.CV, cs.AI},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
  priority         = {prio1},
}

@Article{Yu2025,
  author           = {Jiangyong Yu and Changyong Shu and Dawei Yang and Sifan Zhou and Zichen Yu and Xing Hu and Yan Chen},
  title            = {Q-PETR: Quant-aware Position Embedding Transformation for Multi-View 3D Object Detection},
  year             = {2025},
  month            = feb,
  abstract         = {Camera-based multi-view 3D detection has emerged as an attractive solution for autonomous driving due to its low cost and broad applicability. However, despite the strong performance of PETR-based methods in 3D perception benchmarks, their direct INT8 quantization for onboard deployment leads to drastic accuracy drops-up to 58.2% in mAP and 36.9% in NDS on the NuScenes dataset. In this work, we propose Q-PETR, a quantization-aware position embedding transformation that re-engineers key components of the PETR framework to reconcile the discrepancy between the dynamic ranges of positional encodings and image features, and to adapt the cross-attention mechanism for low-bit inference. By redesigning the positional encoding module and introducing an adaptive quantization strategy, Q-PETR maintains floating-point performance with a performance degradation of less than 1% under standard 8-bit per-tensor post-training quantization. Moreover, compared to its FP32 counterpart, Q-PETR achieves a two-fold speedup and reduces memory usage by three times, thereby offering a deployment-friendly solution for resource-constrained onboard devices. Extensive experiments across various PETR-series models validate the strong generalization and practical benefits of our approach.},
  archiveprefix    = {arXiv},
  creationdate     = {2025-04-24T12:03:46},
  eprint           = {2502.15488},
  file             = {:- Q PETR_ Quant Aware Position Embedding Transformation for Multi View 3D Object Detection.pdf:PDF},
  keywords         = {cs.CV, cs.AI},
  modificationdate = {2025-04-24T12:03:50},
  primaryclass     = {cs.CV},
  priority         = {prio1},
}

@Article{Zeng2021,
  author           = {Fangao Zeng and Bin Dong and Yuang Zhang and Tiancai Wang and Xiangyu Zhang and Yichen Wei},
  title            = {MOTR: End-to-End Multiple-Object Tracking with Transformer},
  year             = {2021},
  month            = may,
  abstract         = {Temporal modeling of objects is a key challenge in multiple object tracking (MOT). Existing methods track by associating detections through motion-based and appearance-based similarity heuristics. The post-processing nature of association prevents end-to-end exploitation of temporal variations in video sequence. In this paper, we propose MOTR, which extends DETR and introduces track query to model the tracked instances in the entire video. Track query is transferred and updated frame-by-frame to perform iterative prediction over time. We propose tracklet-aware label assignment to train track queries and newborn object queries. We further propose temporal aggregation network and collective average loss to enhance temporal relation modeling. Experimental results on DanceTrack show that MOTR significantly outperforms state-of-the-art method, ByteTrack by 6.5% on HOTA metric. On MOT17, MOTR outperforms our concurrent works, TrackFormer and TransTrack, on association performance. MOTR can serve as a stronger baseline for future research on temporal modeling and Transformer-based trackers. Code is available at https://github.com/megvii-research/MOTR.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T13:20:36},
  doi              = {10.48550/arxiv.2105.03247},
  eprint           = {2105.03247},
  file             = {:http\://arxiv.org/pdf/2105.03247v4:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{Zhang2022,
  author           = {Yuang Zhang and Tiancai Wang and Xiangyu Zhang},
  title            = {MOTRv2: Bootstrapping End-to-End Multi-Object Tracking by Pretrained Object Detectors},
  year             = {2022},
  month            = nov,
  abstract         = {In this paper, we propose MOTRv2, a simple yet effective pipeline to bootstrap end-to-end multi-object tracking with a pretrained object detector. Existing end-to-end methods, MOTR and TrackFormer are inferior to their tracking-by-detection counterparts mainly due to their poor detection performance. We aim to improve MOTR by elegantly incorporating an extra object detector. We first adopt the anchor formulation of queries and then use an extra object detector to generate proposals as anchors, providing detection prior to MOTR. The simple modification greatly eases the conflict between joint learning detection and association tasks in MOTR. MOTRv2 keeps the query propogation feature and scales well on large-scale benchmarks. MOTRv2 ranks the 1st place (73.4% HOTA on DanceTrack) in the 1st Multiple People Tracking in Group Dance Challenge. Moreover, MOTRv2 reaches state-of-the-art performance on the BDD100K dataset. We hope this simple and effective pipeline can provide some new insights to the end-to-end MOT community. Code is available at \url{https://github.com/megvii-research/MOTRv2}.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T13:20:36},
  doi              = {10.48550/arxiv.2211.09791},
  eprint           = {2211.09791},
  file             = {:Zhang2022 - MOTRv2_ Bootstrapping End to End Multi Object Tracking by Pretrained Object Detectors.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{Zhang2024,
  author           = {Diankun Zhang and Guoan Wang and Runwen Zhu and Jianbo Zhao and Xiwu Chen and Siyu Zhang and Jiahao Gong and Qibin Zhou and Wenyuan Zhang and Ningzi Wang and Feiyang Tan and Hangning Zhou and Ziyao Xu and Haotian Yao and Chi Zhang and Xiaojun Liu and Xiaoguang Di and Bin Li},
  title            = {SparseAD: Sparse Query-Centric Paradigm for Efficient End-to-End Autonomous Driving},
  year             = {2024},
  month            = apr,
  abstract         = {End-to-End paradigms use a unified framework to implement multi-tasks in an autonomous driving system. Despite simplicity and clarity, the performance of end-to-end autonomous driving methods on sub-tasks is still far behind the single-task methods. Meanwhile, the widely used dense BEV features in previous end-to-end methods make it costly to extend to more modalities or tasks. In this paper, we propose a Sparse query-centric paradigm for end-to-end Autonomous Driving (SparseAD), where the sparse queries completely represent the whole driving scenario across space, time and tasks without any dense BEV representation. Concretely, we design a unified sparse architecture for perception tasks including detection, tracking, and online mapping. Moreover, we revisit motion prediction and planning, and devise a more justifiable motion planner framework. On the challenging nuScenes dataset, SparseAD achieves SOTA full-task performance among end-to-end methods and significantly narrows the performance gap between end-to-end paradigms and single-task methods. Codes will be released soon.},
  archiveprefix    = {arXiv},
  comment          = {end2end MB},
  doi              = {10.48550/arxiv.2404.06892},
  eprint           = {2404.06892},
  file             = {:Zhang2024 - SparseAD_ Sparse Query Centric Paradigm for Efficient End to End Autonomous Driving.pdf:PDF},
  groups           = {Reading Group Sparse Query},
  keywords         = {cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{Zhao2023,
  author           = {Yian Zhao and Wenyu Lv and Shangliang Xu and Jinman Wei and Guanzhong Wang and Qingqing Dang and Yi Liu and Jie Chen},
  title            = {DETRs Beat YOLOs on Real-time Object Detection},
  year             = {2023},
  month            = apr,
  abstract         = {The YOLO series has become the most popular framework for real-time object detection due to its reasonable trade-off between speed and accuracy. However, we observe that the speed and accuracy of YOLOs are negatively affected by the NMS. Recently, end-to-end Transformer-based detectors (DETRs) have provided an alternative to eliminating NMS. Nevertheless, the high computational cost limits their practicality and hinders them from fully exploiting the advantage of excluding NMS. In this paper, we propose the Real-Time DEtection TRansformer (RT-DETR), the first real-time end-to-end object detector to our best knowledge that addresses the above dilemma. We build RT-DETR in two steps, drawing on the advanced DETR: first we focus on maintaining accuracy while improving speed, followed by maintaining speed while improving accuracy. Specifically, we design an efficient hybrid encoder to expeditiously process multi-scale features by decoupling intra-scale interaction and cross-scale fusion to improve speed. Then, we propose the uncertainty-minimal query selection to provide high-quality initial queries to the decoder, thereby improving accuracy. In addition, RT-DETR supports flexible speed tuning by adjusting the number of decoder layers to adapt to various scenarios without retraining. Our RT-DETR-R50 / R101 achieves 53.1% / 54.3% AP on COCO and 108 / 74 FPS on T4 GPU, outperforming previously advanced YOLOs in both speed and accuracy. We also develop scaled RT-DETRs that outperform the lighter YOLO detectors (S and M models). Furthermore, RT-DETR-R50 outperforms DINO-R50 by 2.2% AP in accuracy and about 21 times in FPS. After pre-training with Objects365, RT-DETR-R50 / R101 achieves 55.3% / 56.2% AP. The project page: https://zhao-yian.github.io/RTDETR.},
  archiveprefix    = {arXiv},
  creationdate     = {2024-10-15T09:09:28},
  doi              = {10.48550/arxiv.2304.08069},
  eprint           = {2304.08069},
  file             = {:Zhao2023 - DETRs Beat YOLOs on Real Time Object Detection.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{Zheng2024,
  author           = {Wenzhao Zheng and Ruiqi Song and Xianda Guo and Chenming Zhang and Long Chen},
  title            = {GenAD: Generative End-to-End Autonomous Driving},
  year             = {2024},
  month            = feb,
  abstract         = {Directly producing planning results from raw sensors has been a long-desired solution for autonomous driving and has attracted increasing attention recently. Most existing end-to-end autonomous driving methods factorize this problem into perception, motion prediction, and planning. However, we argue that the conventional progressive pipeline still cannot comprehensively model the entire traffic evolution process, e.g., the future interaction between the ego car and other traffic participants and the structural trajectory prior. In this paper, we explore a new paradigm for end-to-end autonomous driving, where the key is to predict how the ego car and the surroundings evolve given past scenes. We propose GenAD, a generative framework that casts autonomous driving into a generative modeling problem. We propose an instance-centric scene tokenizer that first transforms the surrounding scenes into map-aware instance tokens. We then employ a variational autoencoder to learn the future trajectory distribution in a structural latent space for trajectory prior modeling. We further adopt a temporal model to capture the agent and ego movements in the latent space to generate more effective future trajectories. GenAD finally simultaneously performs motion prediction and planning by sampling distributions in the learned structural latent space conditioned on the instance tokens and using the learned temporal model to generate futures. Extensive experiments on the widely used nuScenes benchmark show that the proposed GenAD achieves state-of-the-art performance on vision-centric end-to-end autonomous driving with high efficiency. Code: https://github.com/wzzheng/GenAD.},
  archiveprefix    = {arXiv},
  doi              = {10.48550/arxiv.2402.11502},
  eprint           = {2402.11502},
  file             = {:Zheng2024 - GenAD_ Generative End to End Autonomous Driving.pdf:PDF},
  keywords         = {cs.CV},
  modificationdate = {2025-01-28T22:53:52},
  primaryclass     = {cs.CV},
}

@Article{Zhu2020,
  author           = {Xizhou Zhu and Weijie Su and Lewei Lu and Bin Li and Xiaogang Wang and Jifeng Dai},
  title            = {Deformable DETR: Deformable Transformers for End-to-End Object Detection},
  year             = {2020},
  month            = oct,
  abstract         = {DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10 times less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code is released at https://github.com/fundamentalvision/Deformable-DETR.},
  archiveprefix    = {arXiv},
  comment          = {Deformable DETR},
  creationdate     = {2024-10-21T16:46:37},
  doi              = {10.48550/arxiv.2010.04159},
  eprint           = {2010.04159},
  file             = {:Zhu2020 - Deformable DETR_ Deformable Transformers for End to End Object Detection.pdf:PDF},
  groups           = {PMO AI},
  keywords         = {cs.CV},
  modificationdate = {2025-04-02T13:24:51},
  primaryclass     = {cs.CV},
  priority         = {prio1},
  readstatus       = {read},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: fileDirectory:files;}

@Comment{jabref-meta: fileDirectory-glantz-MacBook-Air-von-Alexander.fritz.box:files;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Reading Group Sparse Query\;0\;1\;0x00ffffff\;\;\;;
1 StaticGroup:By Status\;0\;1\;0x8a8a8aff\;\;\;;
2 SearchGroup:read\;0\;readstatus=read\;0\;0\;1\;0x00ff00ff\;READ\;\;;
2 SearchGroup:to be read\;0\;not readstatus=read and not readstatus=skimmed\;0\;0\;1\;0xff0000ff\;EYE\;\;;
2 SearchGroup:skimmed\;0\;readstatus=skimmed\;0\;0\;0\;0xffff4dff\;RUN_FAST\;\;;
1 StaticGroup:PMO AI\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:check next\;0\;1\;0x8a8a8aff\;\;\;;
}

@Comment{jabref-meta: saveOrderConfig:specified;citationkey;false;author;false;title;true;}
